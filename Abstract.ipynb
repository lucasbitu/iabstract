{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ec6e8fcdfaed4f61b1da4bff79af6e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c8fa03d1e7f431ea6d624475fdfea84",
              "IPY_MODEL_9320d85e1a084c7b804208dafd829c3c",
              "IPY_MODEL_4747ea38701f45799c0fec2cb0ed000b"
            ],
            "layout": "IPY_MODEL_9cfb4227ec114759b32d7a7300404c46"
          }
        },
        "0c8fa03d1e7f431ea6d624475fdfea84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60b8dc37f6e4455caa406b9561d7c5ea",
            "placeholder": "​",
            "style": "IPY_MODEL_adc772fb7cec4606922c5846f0e200b0",
            "value": "Downloading: 100%"
          }
        },
        "9320d85e1a084c7b804208dafd829c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4fee0eb241143c18a8fa68048d5b1a9",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_114e429ad9a94dbc96423e574fd1924d",
            "value": 28
          }
        },
        "4747ea38701f45799c0fec2cb0ed000b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11121c60619d47d2acb3b38e041c956e",
            "placeholder": "​",
            "style": "IPY_MODEL_8a225e8a977043118df5843b23170f85",
            "value": " 28.0/28.0 [00:00&lt;00:00, 302B/s]"
          }
        },
        "9cfb4227ec114759b32d7a7300404c46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60b8dc37f6e4455caa406b9561d7c5ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adc772fb7cec4606922c5846f0e200b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4fee0eb241143c18a8fa68048d5b1a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "114e429ad9a94dbc96423e574fd1924d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11121c60619d47d2acb3b38e041c956e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a225e8a977043118df5843b23170f85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd192e31b45a4b5c9c30da23e2524247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95021229a5794dada610c71d6da7fe9c",
              "IPY_MODEL_1275a9a3d9cb4f73a1c1ac5a9a09cade",
              "IPY_MODEL_8bf936941e1840c1b200f9d993261e63"
            ],
            "layout": "IPY_MODEL_f0062bd08d9f457090d7a793239e6934"
          }
        },
        "95021229a5794dada610c71d6da7fe9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f668d24aeb8d40a1a9da210d854bee4d",
            "placeholder": "​",
            "style": "IPY_MODEL_49930d69049b491d9a8d500fcbab9660",
            "value": "Downloading: 100%"
          }
        },
        "1275a9a3d9cb4f73a1c1ac5a9a09cade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef48e054ba4e40f3bfa3358090c08f57",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76f7a4d43017424b849d2bd3189cd434",
            "value": 570
          }
        },
        "8bf936941e1840c1b200f9d993261e63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a87e0f96d224ff0ad998eba9822975e",
            "placeholder": "​",
            "style": "IPY_MODEL_4592e9c2ebc746af90aadce1686115cb",
            "value": " 570/570 [00:00&lt;00:00, 10.2kB/s]"
          }
        },
        "f0062bd08d9f457090d7a793239e6934": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f668d24aeb8d40a1a9da210d854bee4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49930d69049b491d9a8d500fcbab9660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef48e054ba4e40f3bfa3358090c08f57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76f7a4d43017424b849d2bd3189cd434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a87e0f96d224ff0ad998eba9822975e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4592e9c2ebc746af90aadce1686115cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35470a1cbcf7407e83b7774c9ac212ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3d5dda2dd354adebe1b22f1c17a32a6",
              "IPY_MODEL_460deb22df5c4f0aac6c08df67dcda82",
              "IPY_MODEL_4fbf6770f27341348a54c8abe9942869"
            ],
            "layout": "IPY_MODEL_0d8c8e81af1e489f9aa1edeaf64bfc39"
          }
        },
        "c3d5dda2dd354adebe1b22f1c17a32a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_856b298fb46140efb4e68dd4d1dd26a6",
            "placeholder": "​",
            "style": "IPY_MODEL_217f9505891d4f768c1135738ad666a3",
            "value": "Downloading: 100%"
          }
        },
        "460deb22df5c4f0aac6c08df67dcda82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_874235ed629e425f8a6bec5784c334ed",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1d934bb8c7e4b05bf83d6c2f4ad522e",
            "value": 231508
          }
        },
        "4fbf6770f27341348a54c8abe9942869": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7a6fffb966c4d6c8f6ee7990005bb2a",
            "placeholder": "​",
            "style": "IPY_MODEL_8f3b50d99b32475fbbe3e94e4d556d98",
            "value": " 232k/232k [00:00&lt;00:00, 1.42MB/s]"
          }
        },
        "0d8c8e81af1e489f9aa1edeaf64bfc39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "856b298fb46140efb4e68dd4d1dd26a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "217f9505891d4f768c1135738ad666a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "874235ed629e425f8a6bec5784c334ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1d934bb8c7e4b05bf83d6c2f4ad522e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7a6fffb966c4d6c8f6ee7990005bb2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f3b50d99b32475fbbe3e94e4d556d98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97f34f92c3564bc7aee1e55ca73bbdb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_409208e6a5894d01b300f7852df08038",
              "IPY_MODEL_a6f9895fbdb2439486890f69869dea6d",
              "IPY_MODEL_5119ca035cd1432bb468be834dd19c39"
            ],
            "layout": "IPY_MODEL_5ddb5942e68642aaa6ad6e642be4c32d"
          }
        },
        "409208e6a5894d01b300f7852df08038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c348112580d4e808593ca76b245c818",
            "placeholder": "​",
            "style": "IPY_MODEL_c8ca339bb2c744b8bbc3a0793d5503f0",
            "value": "Downloading: 100%"
          }
        },
        "a6f9895fbdb2439486890f69869dea6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1f5e33a7b6542c399833121d1b984c6",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9eed4b986aa45819b25ba5d1a383fd0",
            "value": 466062
          }
        },
        "5119ca035cd1432bb468be834dd19c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6e6c1eaefc24cbababfbbff2d9ff435",
            "placeholder": "​",
            "style": "IPY_MODEL_84d5e9c7949b4147b330e660f1e0c357",
            "value": " 466k/466k [00:00&lt;00:00, 1.02MB/s]"
          }
        },
        "5ddb5942e68642aaa6ad6e642be4c32d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c348112580d4e808593ca76b245c818": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8ca339bb2c744b8bbc3a0793d5503f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1f5e33a7b6542c399833121d1b984c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9eed4b986aa45819b25ba5d1a383fd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6e6c1eaefc24cbababfbbff2d9ff435": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84d5e9c7949b4147b330e660f1e0c357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9175e7f54c79461195a4f760b892cb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc20f8d61b5f430cbea422a995a1a869",
              "IPY_MODEL_35fb998895fa4f579e277d1bb42ed42b",
              "IPY_MODEL_02d2d46a31f54ff186b5971fa15637c1"
            ],
            "layout": "IPY_MODEL_a40c2a134a874951ac33a2b3b2603ce4"
          }
        },
        "bc20f8d61b5f430cbea422a995a1a869": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21d4138ff2084a2e97d2ad442525b936",
            "placeholder": "​",
            "style": "IPY_MODEL_3e339bc4cea140958769a4b79e1dd6b3",
            "value": "Downloading builder script: 100%"
          }
        },
        "35fb998895fa4f579e277d1bb42ed42b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2fa0b8ded8c44c48ca53be0163e77b8",
            "max": 6270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c3882e3091141049af4b981bddbf577",
            "value": 6270
          }
        },
        "02d2d46a31f54ff186b5971fa15637c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3df9d43eb1034d99bca956a677a2bf90",
            "placeholder": "​",
            "style": "IPY_MODEL_94c80ff7ccc24d88b7f6a981660746e0",
            "value": " 6.27k/6.27k [00:00&lt;00:00, 152kB/s]"
          }
        },
        "a40c2a134a874951ac33a2b3b2603ce4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21d4138ff2084a2e97d2ad442525b936": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e339bc4cea140958769a4b79e1dd6b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2fa0b8ded8c44c48ca53be0163e77b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c3882e3091141049af4b981bddbf577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3df9d43eb1034d99bca956a677a2bf90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94c80ff7ccc24d88b7f6a981660746e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3b39088b855495697719b60cd5932be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_477ca5715e114a529324f90d8b53aea6",
              "IPY_MODEL_68887fea87634f669ac03b736a4890e0",
              "IPY_MODEL_5b168f8a98cb4201ad51ba1f3f0dc2f6"
            ],
            "layout": "IPY_MODEL_ffce278c85134a5a87c3aefbc19c47fe"
          }
        },
        "477ca5715e114a529324f90d8b53aea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_729eedd51b874000a23c6699fb95302c",
            "placeholder": "​",
            "style": "IPY_MODEL_165559d5b4b54ff4ab9b7d18950fbe16",
            "value": "Downloading: 100%"
          }
        },
        "68887fea87634f669ac03b736a4890e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf161a17bf124c08a3489455ed36ce2d",
            "max": 1175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d75dff51b1664727bcf16c8d72ae5c07",
            "value": 1175
          }
        },
        "5b168f8a98cb4201ad51ba1f3f0dc2f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b323232212a3473698522c716c6c69d5",
            "placeholder": "​",
            "style": "IPY_MODEL_90afebb387394feebce1db7761348437",
            "value": " 1.18k/1.18k [00:00&lt;00:00, 8.73kB/s]"
          }
        },
        "ffce278c85134a5a87c3aefbc19c47fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "729eedd51b874000a23c6699fb95302c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "165559d5b4b54ff4ab9b7d18950fbe16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf161a17bf124c08a3489455ed36ce2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d75dff51b1664727bcf16c8d72ae5c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b323232212a3473698522c716c6c69d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90afebb387394feebce1db7761348437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1ddddb1c9e44d5eae262d1678992b63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d1bba22cc0349baaa368b1c95fc6a94",
              "IPY_MODEL_4abfc3420e794f6583acb580799395ba",
              "IPY_MODEL_9b04b7cf15874cbebb665e9fa6282427"
            ],
            "layout": "IPY_MODEL_b423f76a4b94478fa7ba5b8d4b28c237"
          }
        },
        "2d1bba22cc0349baaa368b1c95fc6a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dc8621df02a4a2ca14886fb853e03d6",
            "placeholder": "​",
            "style": "IPY_MODEL_990b5bedb1124f5ca68b4688c036d159",
            "value": "Downloading: 100%"
          }
        },
        "4abfc3420e794f6583acb580799395ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_154f49fbc17249ec9df4e4043b8ddf6d",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f29a42f088c439f96bec59785260059",
            "value": 190
          }
        },
        "9b04b7cf15874cbebb665e9fa6282427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70c445e64e294659bbd077396c035f29",
            "placeholder": "​",
            "style": "IPY_MODEL_8e8dff9244a74f3daf3008a8e0292e41",
            "value": " 190/190 [00:00&lt;00:00, 2.10kB/s]"
          }
        },
        "b423f76a4b94478fa7ba5b8d4b28c237": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dc8621df02a4a2ca14886fb853e03d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "990b5bedb1124f5ca68b4688c036d159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "154f49fbc17249ec9df4e4043b8ddf6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f29a42f088c439f96bec59785260059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70c445e64e294659bbd077396c035f29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e8dff9244a74f3daf3008a8e0292e41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "097c0d80265f4acdb6023d621cdc7165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e80be009bfa46109def7234b3584f07",
              "IPY_MODEL_052fb2ee69594b288f234230aebc4388",
              "IPY_MODEL_c8f0f79b3ba34eb98b84187e85fc4837"
            ],
            "layout": "IPY_MODEL_74a50e2051824814b57cbf1a7bfc92d2"
          }
        },
        "1e80be009bfa46109def7234b3584f07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f406c61259d4b2fb480c07a0ec739f2",
            "placeholder": "​",
            "style": "IPY_MODEL_55636254297c4a2f8b9b03c48a8a93aa",
            "value": "Downloading: 100%"
          }
        },
        "052fb2ee69594b288f234230aebc4388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8f6da120ef446b484f8ec75ec6c5bf5",
            "max": 10610,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00676cbf6b874466835af1a1cd27c5ee",
            "value": 10610
          }
        },
        "c8f0f79b3ba34eb98b84187e85fc4837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e01bfc8b77854dbea9b896c37c708736",
            "placeholder": "​",
            "style": "IPY_MODEL_a00b6f7cd94e447caaf800c30147ff69",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 104kB/s]"
          }
        },
        "74a50e2051824814b57cbf1a7bfc92d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f406c61259d4b2fb480c07a0ec739f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55636254297c4a2f8b9b03c48a8a93aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8f6da120ef446b484f8ec75ec6c5bf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00676cbf6b874466835af1a1cd27c5ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e01bfc8b77854dbea9b896c37c708736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a00b6f7cd94e447caaf800c30147ff69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5435dc1f959544399efaee664b4e532d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7f81be7d49c474e983b2fd42ed55c6d",
              "IPY_MODEL_6014b2ed9c2c4ef1a2df0511c549c6e5",
              "IPY_MODEL_53abde030e304170aae513022ce3707d"
            ],
            "layout": "IPY_MODEL_63f89def94a045d2ac532a4c7cff1f4d"
          }
        },
        "a7f81be7d49c474e983b2fd42ed55c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16ac5eecd7704876bfddab562adeb1c2",
            "placeholder": "​",
            "style": "IPY_MODEL_9fd05416de37427596c4f0332153d677",
            "value": "Downloading: 100%"
          }
        },
        "6014b2ed9c2c4ef1a2df0511c549c6e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9952d1540c6b485a80e0c6a1fafd6da2",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2ff481b7677489c8e4523970b2534dd",
            "value": 612
          }
        },
        "53abde030e304170aae513022ce3707d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b956b2e50203415caf5dec7dd8b8cdd4",
            "placeholder": "​",
            "style": "IPY_MODEL_1a3c5dd0c8a44c928ac400c57d1f1cec",
            "value": " 612/612 [00:00&lt;00:00, 11.6kB/s]"
          }
        },
        "63f89def94a045d2ac532a4c7cff1f4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16ac5eecd7704876bfddab562adeb1c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fd05416de37427596c4f0332153d677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9952d1540c6b485a80e0c6a1fafd6da2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2ff481b7677489c8e4523970b2534dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b956b2e50203415caf5dec7dd8b8cdd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a3c5dd0c8a44c928ac400c57d1f1cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0486b2425b3f41c588b790b6af84c48b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f4891e4994349bbadda9b6953fa2df7",
              "IPY_MODEL_06eeb90bd7fb45e182d7d9b77ff26fe5",
              "IPY_MODEL_b5e0b141dc5448a1b2d90c66bcec8a99"
            ],
            "layout": "IPY_MODEL_fe8eb6886692417682bd057f689ed650"
          }
        },
        "8f4891e4994349bbadda9b6953fa2df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73b2f44221524429a07173de18bcc387",
            "placeholder": "​",
            "style": "IPY_MODEL_51a5fe9c8cc44a03be6798d2a3be7bf4",
            "value": "Downloading: 100%"
          }
        },
        "06eeb90bd7fb45e182d7d9b77ff26fe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a62a4dc4b6a442429cec06100bf4541d",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2f2f38282734a2b81472e497e33a460",
            "value": 116
          }
        },
        "b5e0b141dc5448a1b2d90c66bcec8a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17a2179d3dfc4b80b12b214d7a5c186c",
            "placeholder": "​",
            "style": "IPY_MODEL_46dd1bd908834116b4a51aaffe2eb4df",
            "value": " 116/116 [00:00&lt;00:00, 3.77kB/s]"
          }
        },
        "fe8eb6886692417682bd057f689ed650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73b2f44221524429a07173de18bcc387": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51a5fe9c8cc44a03be6798d2a3be7bf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a62a4dc4b6a442429cec06100bf4541d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2f2f38282734a2b81472e497e33a460": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17a2179d3dfc4b80b12b214d7a5c186c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46dd1bd908834116b4a51aaffe2eb4df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "156e4d57987e4306a711e1c36c2f5245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95c0cc9b45f74eb3896abd4ea85e2332",
              "IPY_MODEL_078d7ee17843459aa81a3666c6e75a52",
              "IPY_MODEL_a9f014ce156c413b9a41fa442554b18e"
            ],
            "layout": "IPY_MODEL_a601afd5a43a42ddaa3476b2fc5c1053"
          }
        },
        "95c0cc9b45f74eb3896abd4ea85e2332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00aa7f845c6941aba9b77d83670aa896",
            "placeholder": "​",
            "style": "IPY_MODEL_3a9069771c4547ed83d5fee7f1767f0a",
            "value": "Downloading: 100%"
          }
        },
        "078d7ee17843459aa81a3666c6e75a52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de58fd8c31654622a1c53020beacfb7a",
            "max": 39265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b315ad78bdc04a3e86d629c945d51b9c",
            "value": 39265
          }
        },
        "a9f014ce156c413b9a41fa442554b18e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78fc0d280c6e47a489e610e6243ee581",
            "placeholder": "​",
            "style": "IPY_MODEL_063f1b7f57ff4ffca19d2afe3540bdfe",
            "value": " 39.3k/39.3k [00:00&lt;00:00, 889kB/s]"
          }
        },
        "a601afd5a43a42ddaa3476b2fc5c1053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00aa7f845c6941aba9b77d83670aa896": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a9069771c4547ed83d5fee7f1767f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de58fd8c31654622a1c53020beacfb7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b315ad78bdc04a3e86d629c945d51b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "78fc0d280c6e47a489e610e6243ee581": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "063f1b7f57ff4ffca19d2afe3540bdfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e669728a23c4e09a6fb72617563ec0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc0a5949f7d04718a2adb34f69b9fc36",
              "IPY_MODEL_273e27a9e6d64f15a6773fcd09f6efd4",
              "IPY_MODEL_e9cf70d8e35744e2986ad53b3a7e31cd"
            ],
            "layout": "IPY_MODEL_2331c8e5341849e095e9fb74ea1bfef8"
          }
        },
        "dc0a5949f7d04718a2adb34f69b9fc36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a56ad411da424deaa2857822ee2e5e3b",
            "placeholder": "​",
            "style": "IPY_MODEL_8518e6a30404449d96a026b03db485bd",
            "value": "Downloading: 100%"
          }
        },
        "273e27a9e6d64f15a6773fcd09f6efd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1763979ee11848468c6a662a183a2b2b",
            "max": 90888945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00424fd15c54482593e1d0d9602d1bff",
            "value": 90888945
          }
        },
        "e9cf70d8e35744e2986ad53b3a7e31cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee2f3e5c5cbd42019f9d64e8abcb6d1e",
            "placeholder": "​",
            "style": "IPY_MODEL_e99c491cc98b41e5b6ad886fe4103155",
            "value": " 90.9M/90.9M [00:01&lt;00:00, 64.6MB/s]"
          }
        },
        "2331c8e5341849e095e9fb74ea1bfef8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a56ad411da424deaa2857822ee2e5e3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8518e6a30404449d96a026b03db485bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1763979ee11848468c6a662a183a2b2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00424fd15c54482593e1d0d9602d1bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee2f3e5c5cbd42019f9d64e8abcb6d1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e99c491cc98b41e5b6ad886fe4103155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f4691bdfa454952b11286b83e2ab816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d2b6d42461e4b7395e74333d1143398",
              "IPY_MODEL_0d2571fcc8b4448b925654b3473e468a",
              "IPY_MODEL_5dae72632f3f424199018362d344f165"
            ],
            "layout": "IPY_MODEL_16c89f0a018f4d7597a20ba94c1861d4"
          }
        },
        "5d2b6d42461e4b7395e74333d1143398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e64b14bc0754685afcb3c16986bf6a5",
            "placeholder": "​",
            "style": "IPY_MODEL_086dca3623ca4bd89437fd187edb9f86",
            "value": "Downloading: 100%"
          }
        },
        "0d2571fcc8b4448b925654b3473e468a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42313643e3244ee4ab0bb572125bbc59",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e340db9954c49e19ff8063c7351aca1",
            "value": 53
          }
        },
        "5dae72632f3f424199018362d344f165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba1bc434a9f748b9bd87613e98759154",
            "placeholder": "​",
            "style": "IPY_MODEL_fa4fdc42558d46d58b6bf0898a043ea2",
            "value": " 53.0/53.0 [00:00&lt;00:00, 1.29kB/s]"
          }
        },
        "16c89f0a018f4d7597a20ba94c1861d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e64b14bc0754685afcb3c16986bf6a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "086dca3623ca4bd89437fd187edb9f86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42313643e3244ee4ab0bb572125bbc59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e340db9954c49e19ff8063c7351aca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba1bc434a9f748b9bd87613e98759154": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa4fdc42558d46d58b6bf0898a043ea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e20c058afa494c839c6f65d710f3002e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dda249404b974bc28fa001eb52d776ba",
              "IPY_MODEL_32fc402ed275482eafd55e23b9dd4ef4",
              "IPY_MODEL_c1f45feed261456e883f839f1eb91964"
            ],
            "layout": "IPY_MODEL_34b510620e374dbdaa18ab4c35591b62"
          }
        },
        "dda249404b974bc28fa001eb52d776ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_391cdb7fb6204040b3b8d6a0a8ab2616",
            "placeholder": "​",
            "style": "IPY_MODEL_0ee86f4487044b2a9cac48349ac5f91a",
            "value": "Downloading: 100%"
          }
        },
        "32fc402ed275482eafd55e23b9dd4ef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3af04412b6514e19b4fe7a5f7850a864",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b85f25ab013b4339abac821e8e41f6c8",
            "value": 112
          }
        },
        "c1f45feed261456e883f839f1eb91964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5d2d3a9528f4bf69ab0b259691aff79",
            "placeholder": "​",
            "style": "IPY_MODEL_1c1ab734dbd44ba29305eb4935f2ef18",
            "value": " 112/112 [00:00&lt;00:00, 3.77kB/s]"
          }
        },
        "34b510620e374dbdaa18ab4c35591b62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "391cdb7fb6204040b3b8d6a0a8ab2616": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ee86f4487044b2a9cac48349ac5f91a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3af04412b6514e19b4fe7a5f7850a864": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b85f25ab013b4339abac821e8e41f6c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5d2d3a9528f4bf69ab0b259691aff79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c1ab734dbd44ba29305eb4935f2ef18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c729e7334124054b82ef9995a921477": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f55bdc2483124a638d98b809bab18f72",
              "IPY_MODEL_925cd7ffb47c49cabc5c9b3ffbe25e2c",
              "IPY_MODEL_919de4ca377941849e4290f83f32f42c"
            ],
            "layout": "IPY_MODEL_3a22f0c78363413689d981965050e2c7"
          }
        },
        "f55bdc2483124a638d98b809bab18f72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28aa2e87a8ec4719893f0f3daf7001ce",
            "placeholder": "​",
            "style": "IPY_MODEL_f3a04f087169474d8a8a47412352de6a",
            "value": "Downloading: 100%"
          }
        },
        "925cd7ffb47c49cabc5c9b3ffbe25e2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa2da8e1a1ec4953bf381731a80d7911",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e7d1654397a47fe945a6093a72a54df",
            "value": 466247
          }
        },
        "919de4ca377941849e4290f83f32f42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a71f01b101c646d9a6e098c0f9901c70",
            "placeholder": "​",
            "style": "IPY_MODEL_dc9d27e958614367ac0cec18b2c6a892",
            "value": " 466k/466k [00:00&lt;00:00, 4.66MB/s]"
          }
        },
        "3a22f0c78363413689d981965050e2c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28aa2e87a8ec4719893f0f3daf7001ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3a04f087169474d8a8a47412352de6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa2da8e1a1ec4953bf381731a80d7911": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e7d1654397a47fe945a6093a72a54df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a71f01b101c646d9a6e098c0f9901c70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc9d27e958614367ac0cec18b2c6a892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d0cee7a999c4b3bb5851f4041ed7308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc339996e23641869e4fe17811b39bd7",
              "IPY_MODEL_3c0396688eb547628751743e0cd3c56c",
              "IPY_MODEL_c9b53666a0894328ae654e523c4571b5"
            ],
            "layout": "IPY_MODEL_3b84c98cb74f4f24a27ad5f71436f407"
          }
        },
        "dc339996e23641869e4fe17811b39bd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2188fa8174ed425e83191136944f9636",
            "placeholder": "​",
            "style": "IPY_MODEL_0dff709d22e2439fa251c0b111336a18",
            "value": "Downloading: 100%"
          }
        },
        "3c0396688eb547628751743e0cd3c56c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5de913d8d7424d168466140349f0663c",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6483a6453d104e2d8cd48012aaaf3453",
            "value": 350
          }
        },
        "c9b53666a0894328ae654e523c4571b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88fa915a4a794666909a73a74124febd",
            "placeholder": "​",
            "style": "IPY_MODEL_a8e0146d7d3c476fa97d4f36abbdeac6",
            "value": " 350/350 [00:00&lt;00:00, 11.4kB/s]"
          }
        },
        "3b84c98cb74f4f24a27ad5f71436f407": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2188fa8174ed425e83191136944f9636": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dff709d22e2439fa251c0b111336a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5de913d8d7424d168466140349f0663c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6483a6453d104e2d8cd48012aaaf3453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88fa915a4a794666909a73a74124febd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8e0146d7d3c476fa97d4f36abbdeac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e59b070f01748f5937fd7c884409821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a3d1a7bcf4d4bcd95078129d8aee36b",
              "IPY_MODEL_a466b36404444748b80e53469816363e",
              "IPY_MODEL_05b406f5f03c4154b5f1872417147a78"
            ],
            "layout": "IPY_MODEL_f1d6e46abc394eadb70719cb31a9b732"
          }
        },
        "0a3d1a7bcf4d4bcd95078129d8aee36b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b71ccf59acaa46988d588840e02cac26",
            "placeholder": "​",
            "style": "IPY_MODEL_f7c2b75e5acb4b399a6073d8a972ce7d",
            "value": "Downloading: 100%"
          }
        },
        "a466b36404444748b80e53469816363e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a979e22176214165afe1b5f6b5df7e8c",
            "max": 13156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5581d9069dd04d4d92183f9fded638a9",
            "value": 13156
          }
        },
        "05b406f5f03c4154b5f1872417147a78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63b7f9141edd4870be4b8fd165878b86",
            "placeholder": "​",
            "style": "IPY_MODEL_a551c73057604508a229b037712fc078",
            "value": " 13.2k/13.2k [00:00&lt;00:00, 237kB/s]"
          }
        },
        "f1d6e46abc394eadb70719cb31a9b732": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b71ccf59acaa46988d588840e02cac26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7c2b75e5acb4b399a6073d8a972ce7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a979e22176214165afe1b5f6b5df7e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5581d9069dd04d4d92183f9fded638a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63b7f9141edd4870be4b8fd165878b86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a551c73057604508a229b037712fc078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7088b66e4830427c89d2188d6058d97f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a612ed2eb24458eb84a1b86d0616b23",
              "IPY_MODEL_a61fa34a269b4c91be92418671245299",
              "IPY_MODEL_9c3806d674bf45f2a3691e054a7984ea"
            ],
            "layout": "IPY_MODEL_15f47ed7d8134fc5aa17defe993cd250"
          }
        },
        "4a612ed2eb24458eb84a1b86d0616b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c9df6f69bca41978a1b6ee606b8d836",
            "placeholder": "​",
            "style": "IPY_MODEL_0283a845dc5144a587d2f3ecc0e5ffec",
            "value": "Downloading: 100%"
          }
        },
        "a61fa34a269b4c91be92418671245299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e16a39bda51841c2bcc433a79facdaf2",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7142c335c89e49bc9f65965c151bc0c5",
            "value": 231508
          }
        },
        "9c3806d674bf45f2a3691e054a7984ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b02e97db4d64a09883996ff71ccff6f",
            "placeholder": "​",
            "style": "IPY_MODEL_5178445f2e194ecab497831f86043adc",
            "value": " 232k/232k [00:00&lt;00:00, 1.69MB/s]"
          }
        },
        "15f47ed7d8134fc5aa17defe993cd250": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c9df6f69bca41978a1b6ee606b8d836": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0283a845dc5144a587d2f3ecc0e5ffec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e16a39bda51841c2bcc433a79facdaf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7142c335c89e49bc9f65965c151bc0c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b02e97db4d64a09883996ff71ccff6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5178445f2e194ecab497831f86043adc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b15878849bce44c8bacb689c171e499b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ffa7ff1765f74120827261de29861dff",
              "IPY_MODEL_0cbce1f18f0e4a65ba25ce58ebc89b2c",
              "IPY_MODEL_8fa7e7261f5c48dfbd868f5f38371f65"
            ],
            "layout": "IPY_MODEL_2b9504f8b62a4424b582ab9fcedcd111"
          }
        },
        "ffa7ff1765f74120827261de29861dff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb19d1c99b164d80b5dc214cbe04e6e5",
            "placeholder": "​",
            "style": "IPY_MODEL_d911c8507d4946159217408ead154b39",
            "value": "Downloading: 100%"
          }
        },
        "0cbce1f18f0e4a65ba25ce58ebc89b2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f97db775564041e9bef014b1e11375d2",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4312fa978b304d43bd98bf9e0521578d",
            "value": 349
          }
        },
        "8fa7e7261f5c48dfbd868f5f38371f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c48104256ba14372b85306a79966edb9",
            "placeholder": "​",
            "style": "IPY_MODEL_39190c4c13d14c48a4b3a53848753e2a",
            "value": " 349/349 [00:00&lt;00:00, 3.95kB/s]"
          }
        },
        "2b9504f8b62a4424b582ab9fcedcd111": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb19d1c99b164d80b5dc214cbe04e6e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d911c8507d4946159217408ead154b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f97db775564041e9bef014b1e11375d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4312fa978b304d43bd98bf9e0521578d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c48104256ba14372b85306a79966edb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39190c4c13d14c48a4b3a53848753e2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ralfferreira/generate-abstract/blob/main/Abstract.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando as bibliotecas\n",
        "\n",
        "!pip install -q datasets\n",
        "!pip install -q transformers\n",
        "!pip install -q evaluate"
      ],
      "metadata": {
        "id": "Rpl9nEfALUtH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FgSkrDl-I2Ov"
      },
      "outputs": [],
      "source": [
        "# Importando a base de dados, definindo a de teste\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"scientific_papers\", \"arxiv\", split = 'test', streaming=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando o modelo\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "model = AutoModelWithLMHead.from_pretrained(\"t5-base\") # Iremos testar outros modelos"
      ],
      "metadata": {
        "id": "Jjtq8vMAMHjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df864488-d566-4fe8-8e66-7d42d5531ad9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/auto/modeling_auto.py:1177: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dois tipos de base de dados\n",
        "\n",
        "## Treino\n",
        "small_train_dataset = dataset.shuffle(seed=42, buffer_size=200)\n",
        "\n",
        "## Avaliação\n",
        "small_eval_dataset = dataset.shuffle(seed=42, buffer_size=200)"
      ],
      "metadata": {
        "id": "aOGiJF8qPg_u"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar os dados\n",
        "\n",
        "dataset_head = small_train_dataset.take(2)\n",
        "list(dataset_head)"
      ],
      "metadata": {
        "id": "xT7kz0mEPxdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c3f8be2-c3db-44e0-f95d-259e6e68330a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'article': \"a cluster category is a certain 2-calabi - yau orbit category of the derived category of a hereditary abelian category .\\ncluster categories were introduced in @xcite in order to give a categorical model for the combinatorics of fomin - zelevinsky cluster algebras @xcite .\\nthey are triangulated @xcite and admit ( cluster-)tilting objects , which model the clusters of a corresponding ( acyclic ) cluster algebra @xcite . each cluster in a fixed cluster algebra comes together with a finite quiver , and in the categorical model this quiver is in fact the gabriel quiver of the corresponding tilting object @xcite .\\na principal ingredient in the construction of a cluster algebra is quiver mutation .\\nit controls the exchange procedure which gives a rule for producing a new cluster variable and hence a new cluster from a given cluster .\\nexchange is modeled by cluster categories in the acyclic case @xcite in terms of a mutation rule for tilting objects , i.e. a rule for replacing an indecomposable direct summand in a tilting object with another indecomposable rigid object , to get a new tilting object .\\nquiver mutation describes the relation between the gabriel quivers of the corresponding tilting objects .\\nanalogously to the definition of the cluster category , for a positive integer @xmath0 , it is natural to define a certain @xmath1-calabi - yau orbit category of the derived category of a hereditary abelian category .\\nthis is called the _\\n@xmath0-cluster category_. implicitly , @xmath0-cluster categories was first studied in @xcite , and their ( cluster-)tilting objects have been studied in @xcite .\\ncombinatorial descriptions of @xmath0-cluster categories in dynkin type @xmath2 and @xmath3 are given in @xcite .    in cluster categories\\nthe mutation rule for tilting objects is described in terms of certain triangles called _ exchange triangles_. by @xcite the existence of exchange triangles generalizes to @xmath0-cluster categories .\\nit was shown in @xcite that there are exactly @xmath1 non - isomorphic complements to an almost complete tilting object , and that they are determined by the @xmath1 exchange triangles defined in @xcite .\\nthe aim of this paper is to give a combinatorial description of mutation in @xmath0-cluster categories . _ a priori _\\n, one might expect to be able to do this by keeping track of the gabriel quivers of the tilting objects .\\nhowever , it is easy to see that the gabriel quivers do not contain enough information .    we proceed to associate to a tilting object a quiver each of whose arrows has an associated colour @xmath4 . the arrows with colour 0 form the gabriel quiver of the tilting object .\\nwe then define a mutation operation on coloured quivers and show that it is compatible with mutation of tilting objects .\\na consequence is that the effect of an arbitrary sequence of mutations on a tilting object in an @xmath0-cluster category can be calculated by a purely combinatorial procedure .\\nour definition of a coloured quiver associated to a tilting object makes sense in any @xmath1-calabi - yau category , such as for example those studied in @xcite .\\nwe hope that our constructions may shed some light on mutation of tilting objects in this more general setting .    in section 1 , we review some elementary facts about higher cluster categories . in section 2 , we explain how to define the coloured quiver of a tilting object , we define coloured quiver mutation , and we state our main theorem . in sections 3 and 4 , we state some further lemmas about higher cluster categories , and we prove certain properties of the coloured quivers of tilting objects .\\nwe prove our main result in sections 5 and 6 . in sections 7 and 8\\nwe point out some applications . in section 9\\nwe interpret our construction in terms of @xmath0-cluster complexes . in section 10\\n, we give an alternative algorithm for computing coloured quiver mutation .\\nsection 11 discusses the example of @xmath0-cluster categories of dynkin type @xmath2 , using the model developed by baur and marsh @xcite .\\nwe would like to thank idun reiten , in conversation with whom the initial idea of this paper took shape .\\nlet @xmath5 be an algebraically closed field , and let @xmath6 be a finite acyclic quiver with @xmath7 vertices .\\nthen the path algebra @xmath8 is a hereditary finite dimensional basic @xmath5-algebra    let @xmath9 be the category of finite dimensional left @xmath10-modules .\\nlet @xmath11 be the bounded derived category of @xmath10 , and let @xmath12 $ ] be the @xmath13th shift functor on @xmath14 .\\nwe let @xmath15 denote the auslander - reiten translate , which is an autoequivalence on @xmath14 such that we have a bifunctorial isomorphism in @xmath14 @xmath16 ) \\\\simeq\\nd{\\\\operatorname{hom}\\\\nolimits}(b,\\\\tau a).\\\\ ] ] in other words @xmath17 \\\\tau$ ] is a serre functor .\\nlet @xmath18 $ ] .\\nthe @xmath0-cluster category is the orbit category @xmath19 $ ] .\\nthe objects in @xmath20 are the objects in @xmath14 , and two objects @xmath21 are isomorphic in @xmath20 if and only if @xmath22 in @xmath14 .\\nthe maps are given by @xmath23 .\\nby @xcite , the category @xmath20 is triangulated and the canonical functor @xmath24 is a triangle functor .\\nwe denote therefore by @xmath25 $ ] the suspension in @xmath20 .\\nthe @xmath0-cluster category is also krull - schmidt and has an ar - translate @xmath15 inherited from @xmath14 , such that the formula ( [ ar ] ) still holds in @xmath20 . if follows that @xmath17 \\\\tau$ ] is a serre functor for @xmath20 and that @xmath20 is @xmath1-calabi - yau , since @xmath26 $ ] .\\nthe indecomposable objects in @xmath14 are of the form @xmath27 $ ] , where @xmath28 is an indecomposable @xmath10-module and @xmath29 .\\nwe can choose a fundamental domain for the action of @xmath18 $ ] on @xmath14 , consisting of the indecomposable objects @xmath27 $ ] with @xmath30 , together with the objects @xmath31 $ ] with @xmath28 an indecomposable projective @xmath10-module .\\nthen each indecomposable object in @xmath20 is isomorphic to exactly one of the indecomposables in this fundamental domain .\\nwe say that @xmath32 $ ] has degree @xmath33 , denoted @xmath34 ) = d$ ] .\\nfurthermore , for an arbitrary object @xmath35 in @xmath36 , we let @xmath37 $ ] be the @xmath10-module which is the ( shifted ) direct sum of all summands @xmath38 of @xmath39 with @xmath40 .    in the following theorem\\nthe equivalence between ( i ) and ( ii ) is shown in @xcite and the equivalence between ( i ) and ( iii ) is shown in @xcite .\\nlet @xmath41 be an object in @xmath20 satisfying @xmath42 ) = 0 $ ] for @xmath43 .\\nthen the following are equivalent    * if @xmath44 ) = 0 $ ] for @xmath43 then @xmath45 is in @xmath46 . * if @xmath47 ) = 0 $ ] for @xmath43 then @xmath45 is in @xmath46 .\\n* @xmath41 has @xmath48 indecomposable direct summands , up to isomorphism .\\nhere @xmath46 denotes the additive closure of @xmath41 . a ( cluster-)tilting object @xmath41 in an @xmath0-cluster is an object satisfying the conditions of the above theorem . for a tilting object @xmath49 , with each @xmath50 indecomposable , and @xmath51 an indecomposable direct summand , we call @xmath52 an almost complete tilting object .\\nwe let @xmath53 denote the @xmath5-space of irreducible maps @xmath54 in a krull - schmidt @xmath5-category @xmath55 .\\nthe following crucial result is proved in @xcite and @xcite .\\n[ p : number ] there are , up to isomorphism , @xmath1 complements of an almost complete tilting object .\\nlet @xmath51 be an indecomposable direct summand in an @xmath0-cluster tilting object @xmath56 .\\nthe complements of @xmath57 are denoted @xmath58 for @xmath59 , where @xmath60 . by @xcite\\n, there are @xmath1 exchange triangles @xmath61 here the @xmath62 are in @xmath63 and the maps @xmath64 ( resp .\\n@xmath65 ) are minimal left ( resp .\\nright ) @xmath63-approximations , and hence not split mono or split epi . note that by minimality , the maps @xmath64 and @xmath65 have no proper zero summands .\\nwe first recall the definition of quiver mutation , formulated in @xcite in terms of skew - symmetric matrices .\\nlet @xmath66 be a quiver with vertices @xmath67 and with no loops or oriented two - cycles , where @xmath68 denotes the number of arrows from @xmath13 to @xmath69 .\\nlet @xmath70 be a vertex in @xmath71 .\\nthen , a new quiver @xmath72 is defined by the following data    @xmath73 it is easily verified that this definition is equivalent to the one of fomin - zelevinsky .\\nnow we consider coloured quivers .\\nlet @xmath0 be a positive integer .\\nan @xmath0-coloured ( multi-)quiver @xmath71 consists of vertices @xmath67 and coloured arrows @xmath74 , where @xmath75 .\\nlet @xmath76 denote the number of arrows from @xmath13 to @xmath70 of colour @xmath77 .\\nwe will consider coloured quivers with the following additional conditions .    *\\nno loops : @xmath78 for all @xmath79 .\\n* monochromaticity : if @xmath80 , then @xmath81 for @xmath82 * skew - symmetry : @xmath83 .\\nwe will define an operation on a coloured quiver @xmath71 satisfying the above conditions .\\nlet @xmath70 be a vertex in @xmath71 and let @xmath84 be the coloured quiver defined by    @xmath85    in an @xmath0-cluster category @xmath20 , for every tilting object @xmath86 , with the @xmath50 indecomposable , we will define a corresponding @xmath0-coloured quiver @xmath87 , as follows .\\nlet @xmath88 be two non - isomorphic indecomposable direct summands of the @xmath0-cluster tilting object @xmath41 and let @xmath89 denote the multiplicity of @xmath90 in @xmath91 .\\nwe define the @xmath0-coloured quiver @xmath87 of @xmath41 to have vertices @xmath13 corresponding to indecomposable direct summands @xmath50 , and @xmath92 .\\nnote , in particular , that the @xmath93-coloured arrows are the arrows from the gabriel quiver for the endomorphism ring of @xmath41 .    by definition\\n, @xmath87 satisfies condition ( i ) .\\nwe show in section [ s : higher ] that ( ii ) is satisfied ( this also follows from @xcite ) , and in section [ s : symmetry ] that ( iii ) is also satisfied .    the aim of this paper is to prove the following theorem , which is a generalization of the main result of @xcite .\\n[ t : main ] let @xmath86 and @xmath94 be @xmath0-tilting objects , where there is an exchange triangle @xmath95\\n. then @xmath96 .    in the case\\n@xmath97 the coloured quiver of a tilting object @xmath41 is given by @xmath98 and @xmath99 where @xmath100 denotes the number of arrows in the gabriel quiver of @xmath41\\n. then coloured mutation of the coloured quiver corresponds to fz - mutation of the gabriel quiver .\\nlet @xmath6 be @xmath101 with linear orientation , i.e. the quiver @xmath102 .\\nthe ar - quiver of the 2-cluster category of @xmath103 is @xmath104 & & { i_3 } \\\\ar[dr ] & & * + + [ o][f-]{p_3[1 ] } \\\\ar[dr ] & & i_1[1 ] \\\\ar[dr ] & & p_1[2]\\\\ar[dr ] \\\\\\\\   p_2[2 ] \\\\ar[ur ] \\\\ar[dr ] & & { p_2 } \\\\ar[ur ] \\\\ar[dr ] & & * + [ o][f-]{i_2 } \\\\ar[ur ] \\\\ar[dr ] & & p_2[1 ] \\\\ar[ur ] \\\\ar[dr ] & &   i_2[1]\\\\ar[ur]\\\\ar[dr ] & & p_2[2 ]    \\\\\\\\   & { p_3 } \\\\ar[ur ] & & * + [ o][f-]{i_1 } \\\\ar[ur ] & & p_1[1 ] \\\\ar[ur ] & & i_3[1]\\\\ar[ur ] & & p_3[2 ] \\\\ar[ur ] & &   } \\\\ ] ] the direct sum @xmath105 $ ] of the encircled indecomposable objects gives a tilting object .\\nits coloured quiver is @xmath106 & i_2 \\\\ar@<0.6ex>^{(0)}[r ] \\\\ar@<0.6ex>^{(2)}[l ] &   p_3[1 ] \\\\ar@<0.6ex>^{(2)}[l ]   } \\\\ ] ] now consider the exchange triangle @xmath107 \\\\to i_3[1 ] \\\\to\\\\ ] ] and the new tilting object @xmath108 \\\\amalg p_3[1]$ ] .\\nthe coloured quiver of @xmath109 is @xmath110 \\\\ar@<0.6ex>^{(1)}[r ] & i_3[1 ] \\\\ar@<0.6ex>^{(2)}[r ] \\\\ar@<0.6ex>^{(1)}[l ] &   p_3[1 ] \\\\ar@<0.6ex>^{(0)}[l ]   \\\\ar@<0.6ex>^{(2)}@/^3.5pc/[ll ] } \\\\ ] ]\\nin this section we summarize some further known results about @xmath0-cluster categories .\\nmost of these are from @xcite and @xcite .\\nwe include some proofs for the convenience of the reader\\n.    tilting objects in @xmath111 give rise to partial tilting modules in @xmath9 , where a _\\npartial tilting module _ @xmath28 in @xmath9 , is a module with @xmath112 .\\n[ l : partial ]    * when @xmath41 is a tilting object in @xmath36 , then each @xmath113 is a partial tilting module in @xmath9 . * the endomorphism ring of a partial tilting module has no oriented cycles in its ordinary quiver .\\n\\\\(a ) is obvious from the definition .\\nsee ( * ? ? ?\\n4.2 ) for ( b ) .    in the following note that degrees of objects are always considered with a fixed choice of fundamental domain , and sums and differences of degrees are always computed modulo @xmath1 .    [\\nl : div ] assume @xmath114 .\\n* @xmath115 for any indecomposable exceptional object @xmath39 .\\n* we have that @xmath116 * the distribution of degrees of complements is one of the following * * there is exactly one complement of each degree , or * * there is no complement of degree @xmath0 , two complements in one degree @xmath117 , and exactly one complement in all degrees @xmath118 . * if @xmath119 , then @xmath120 . * for @xmath121 we have @xmath122 ) = \\\\begin{cases }\\nk & \\\\text { if $ c'-c+t = 0 ( { \\\\operatorname{mod}\\\\nolimits}m+1)$ } \\\\\\\\ 0 & \\\\text { else } \\\\end{cases}\\\\ ] ]    \\\\(a ) follows from the fact that @xmath123 for exceptional objects and the definition of maps in a @xmath0-cluster category .\\n\\\\(b ) follows from the fact that @xmath124 ) \\\\neq 0 $ ] , since in the exchange triangles , the @xmath125 are not split mono and ( c ) follows from ( b ) .\\nconsidering the two different possible distributions of complements , we obtain from ( c ) that if @xmath126 and @xmath127 and @xmath128 , then @xmath129 .\\nconsider the case @xmath130 .\\nwe can assume @xmath131 , since else the statement is void .\\nhence we can clearly assume that @xmath132 .\\nthere is an exchange triangle induced from an exact sequence in @xmath9 ,    @xmath133.\\\\ ] ] it is clear that @xmath134 , t_i^{(c-1 ) } ) = 0 $ ] , since @xmath131 .\\nwe claim that also @xmath135 .\\nthis holds since @xmath136 is a partial tilting object in @xmath10 , and so there are no cycles in the endomorphism ring , by lemma [ l : partial ] .\\nhence also @xmath137 follows , and this finishes the proof for ( d ) .\\nfor ( e ) we first apply @xmath138 to the exchange triangle @xmath139 and consider the corresponding long - exact sequence , to obtain that @xmath140 ) = \\\\begin{cases } k & \\\\text { if $ t = 1 $ } \\\\\\\\ 0 & \\\\text { if $ t=0 $ or $ t \\\\in \\\\{2 , \\\\dots , m \\\\}$ } \\\\end{cases}.\\\\ ] ] now consider @xmath141)$ ] . when @xmath142 , we have that @xmath143 ) \\\\simeq { \\\\operatorname{hom}\\\\nolimits}(t_i^{(c+u+1 ) } , t_i^{(c)}[v+1 ] ) \\\\simeq \\\\\\\\ { \\\\operatorname{hom}\\\\nolimits}(t_i^{(c-1 ) } , t_i^{(c)}[v+m - u ] ) \\\\simeq { \\\\operatorname{hom}\\\\nolimits}(t_i^{(c ) } , t_i^{(c-1)}[1+u - v ] ) .\\n\\\\end{gathered}\\\\ ] ]    when @xmath144 , we have that @xmath145 ) \\\\simeq { \\\\operatorname{hom}\\\\nolimits}(t_i^{(c+u-1 ) } , t_i^{(c)}[v-1 ] ) \\\\simeq \\\\\\\\ { \\\\operatorname{hom}\\\\nolimits}(t_i^{(c ) } , t_i^{(c)}[v - u]).\\\\ ] ]    combining these facts , ( e ) follows .\\n[ l : div2 ] the following statements are equivalent    * @xmath146 ) = 0 $ ] * @xmath90 is not a direct summand in @xmath147 * @xmath50 is not a direct summand in @xmath148    furthermore , @xmath149 ) = 0 $ ] for @xmath150 .\\nnote that @xmath151 , so ( b ) and ( c ) are equivalent .\\nconsider the exact sequence @xmath152 ) \\\\to { \\\\operatorname{hom}\\\\nolimits}(t_i^{(c ) } , b_j^{(0)}[1 ] ) \\\\to \\\\\\\\   { \\\\operatorname{hom}\\\\nolimits}(t_i^{(c ) } , t_j^{(1)}[1 ] ) \\\\to { \\\\operatorname{hom}\\\\nolimits}(t_i^{(c ) } , t_j^{(0)}[2 ] ) \\\\to \\\\end{gathered}\\\\ ] ] coming from applying @xmath153 to the exchange triangle @xmath154 the first and fourth terms are always zero . using [ l : div](e ) we get that the second term ( and hence the third ) is non - zero if and only if @xmath155 and @xmath50 is a direct summand in @xmath148 .\\n( @xcite)[l : composing ] for @xmath156 , the composition @xmath157   \\\\circ h_k^{(v-2)}[2 ] \\\\circ \\\\cdots \\\\circ   h_k^{(v - l+1)}[l-1 ] \\\\colon t_k^{(v ) } \\\\to t_k^{(v - l)}[l]\\\\ ] ] is non - zero and a basis for @xmath158)$ ] .    for @xmath97 ,\\nsee @xcite .\\nassume @xmath159 .\\nfor the first claim see @xcite , while the second claim then follows from lemma [ l : div](e ) .\\nwe include an independent proof of the following crucial property .\\n@xcite [ l : disjoint ] @xmath160 and @xmath161 has no common non - zero direct summands whenever @xmath162 .    when @xmath97 , this is proved in @xcite .\\nassume @xmath163 .\\nwe consider two cases , @xmath164 or @xmath165 .\\nconsider first the case @xmath166 . without loss of generality\\nwe can assume @xmath167 and @xmath168 , and that @xmath169 .\\nassume that there exists a ( non - zero ) indecomposable @xmath170 , which is a direct summand in @xmath171 and in @xmath172 .\\nwe have that @xmath173 by lemma [ l : div](b ) .\\nassume first @xmath174 .\\nthen the exchange triangle @xmath175 is induced from the degree 0 part of the derived category , and hence from an exact sequence in @xmath9 .\\nthen the endomorphism ring of the partial tilting module @xmath176 has a cycle , which is a contradiction to lemma [ l : partial ] .\\nassume now that @xmath177 .\\nthen @xmath178 , where 0 can only occur if @xmath179 . if @xmath180 , then clearly @xmath181 , and hence the partial tilting module @xmath182 contains a cycle , which is a contradiction .\\nassume that @xmath183 ( and hence @xmath179 ) . then @xmath184 . if @xmath181 , we get a contradiction as in the previous case .\\nif @xmath185 , consider the exchange triangle @xmath186 which is induced from an exact sequence in @xmath9 .\\nhence there is a _ non - zero _\\nmap @xmath187 obtained by composing @xmath188 with the monomorphism @xmath189 , and thus there are cycles in the endomorphism ring of the partial tilting module @xmath190 , a contradiction .\\nthis finishes the case with @xmath191 .\\nassume now that @xmath192 .\\nthen we have @xmath193 .\\nsince @xmath194 and @xmath195 , we have by lemma [ l : div](c ) that @xmath196 .\\nso without loss of generality we can assume @xmath197 .\\nassume that @xmath198 . then @xmath199 using lemma [ l : div](c ) and the fact that @xmath194\\n. then also @xmath200 .\\nbut @xmath201 , so @xmath202 , contradicting the fact that @xmath195 .\\n@xmath87 satisfies condition ( ii ) .\\nlet @xmath203 be a tilting object . in this section we show that the coloured quiver @xmath87 satisfies condition ( iii ) .\\n[ p : symmetry ] with the notation of the previous section , we have @xmath204 .    by lemma [ l : div2 ]\\nwe only need to consider the case @xmath205 .\\nit is enough to show that @xmath206 .\\nwe first prove    [ l : non - van ] let @xmath207 be irreducible in @xmath208 . then the composition @xmath209\\n\\\\circ \\\\gamma_i^{(0,c)}[-c ]   \\\\colon t_j^{(c)}[-c ] \\\\to t_i^{(m - c+1)}$ ] is non - zero .\\nwe have already assumed @xmath210 .\\nassume @xmath211 \\\\circ h_i^{(0)}[-c ] \\\\colon t_j^{(c)}[-c ] \\\\to t_i^{(0)}[-c ] \\\\to t_i^{(m)}[-c+1]\\\\ ] ] is zero .\\nthis means that @xmath212 must factor through @xmath213 .\\nsince @xmath50 is by assumption a summand in @xmath214 , we have that @xmath50 is not a summand in @xmath148 by proposition [ l : disjoint ] .\\nsince @xmath215 , we have that @xmath90 is not a direct summand in @xmath147 .\\nthis means that @xmath216 is not irreducible in @xmath208 , a contradiction .\\nso @xmath209 \\\\circ h_i^{(0)}[-c ] \\\\colon t_j^{(c)}[-c ] \\\\to t_i^{(m)}[-c+1]$ ] is non - zero .\\nassume @xmath217 .\\nif the composition @xmath209 \\\\circ h_i^{(0)}[-c ] \\\\circ h_i^{(m)}[-c+1]$ ] is zero , then @xmath209 \\\\circ h_i^{(0)}[-c]$ ] factors through @xmath218 \\\\to t_i^{(m)}[-c+1].\\\\ ] ] we claim that @xmath219 , b_i^{(m-1)}[-c+1 ] ) \\\\simeq { \\\\operatorname{hom}\\\\nolimits}(t_j^{(c ) } , b_i^{(m-1)}[1 ] ) = 0 $ ] .\\nthis clearly holds if @xmath90 is not a summand of @xmath220 .\\nin addition we have that @xmath221 ) = 0 $ ] since @xmath217 , using lemma [ l : div](e ) .\\nthis is a contradiction , and this argument can clearly be iterated to see that @xmath209 \\\\circ \\\\gamma_i^{(0,c)}[-c ]   \\\\colon t_j^{(c)}[-c ] \\\\to t_i^{(m - c+1)}$ ] is non - zero , using lemma [ l : div](e ) .    we now show that any irreducible map @xmath207 gives rise to an irreducible map @xmath222\\n.    consider the composition @xmath223 \\\\overset{g_j^{(c)}[-c]}{\\\\longrightarrow } t_j^{(c)}[-c ] \\\\longrightarrow t_i^{(m - c+1)}.\\\\ ] ] since @xmath50 is a summand in @xmath214 by assumption , it is not a summand in @xmath224 .\\nthus , @xmath224 is in @xmath225 .\\nsince @xmath226)= 0 $ ] for any @xmath39 in @xmath227 , the composition vanishes .    using the exchange triangle @xmath223 \\\\overset{g_j^{(c)}[-c]}{\\\\longrightarrow } t_j^{(c)}[-c ] \\\\overset{h_j^{(c)}[-c]}{\\\\longrightarrow } t_j^{(c-1)}[-c+1],\\\\ ] ] we see that @xmath209 \\\\circ \\\\gamma_i^{(0,c)}[-c ] \\\\colon t_j^{(c)}[-c ] \\\\to t_i^{(m - c+1)}$ ] factors through the map @xmath228 \\\\overset{h_j^{(c)}[-c]}{\\\\longrightarrow } t_j^{(c-1)}[-c+1]$ ] , i.e. there is a commutative diagram    @xmath229 \\\\ar^{g_j^{(c)}[-c]}[r ]   & t_j^{(c)}[-c ] \\\\ar^{h_j^{(c)}[-c]}[r ] \\\\ar[d ] &   t_j^{(c-1)}[-c+1 ] \\\\ar[r ]   \\\\ar^{\\\\phi_1}[dl ] &   \\\\\\\\ & t_i^{(m - c+1 ) } & & } \\\\ ] ]    similarly , using the exchange triangle @xmath230 \\\\overset{g_j^{(c-1)}[-c+1]}{\\\\longrightarrow } t_j^{(c-1)}[-c+1 ] \\\\overset{h_j^{(c-1)}[-c+1]}{\\\\longrightarrow } t_j^{(c-2)}[-c+2]\\\\ ] ] we obtain a map @xmath231 \\\\to   t_i^{(m - c+1)}$ ]    repeating this argument @xmath79 times we obtain a map @xmath232 , such that @xmath233 \\\\circ \\\\phi_c = \\\\alpha[-c ] \\\\circ \\\\gamma_i^{(0,c)}$ ]\\n.    @xmath234 \\\\ar_{h_j^{(c)}[-c]}[d ] \\\\ar[r ] & t_i^{(m - c+1 ) }   \\\\\\\\    t_j^{(c-1)}[-c+1 ] \\\\ar_{h_j^{(c-1)}[-c+1]}[d ] \\\\ar^{\\\\phi_1}[ur ] & \\\\\\\\ t_j^{(c-2)}[-c+2 ] \\\\ar_{h_j^{(c-2)}[-c+2]}[d ] \\\\ar^{\\\\phi_2}[uur ] & \\\\\\\\\\n\\\\vdots \\\\ar[d ] &   \\\\\\\\\\nt_j    \\\\ar^{\\\\phi_c}[uuuur ] & \\\\\\\\ & } \\\\ ] ]    we claim that    [ l : irred ] there is a map @xmath235 , such that @xmath233 \\\\circ \\\\beta = \\\\alpha[-c ] \\\\circ \\\\gamma_i^{(0,c)}$ ] , and such that @xmath236 is irreducible in @xmath237\\n.    let @xmath238 be a minimal left @xmath239-approximation , with @xmath240 in @xmath241 and @xmath242 in @xmath243 .\\nlet @xmath244 be as above , and factor it as @xmath245 since @xmath246 factors through @xmath247 $ ] , we have that @xmath233 \\\\psi '' = 0 $ ] , so we have @xmath248(\\\\psi ' \\\\epsilon ' + \\\\psi '' \\\\epsilon'')= \\\\gamma_j^{(c , c)}[-c ] \\\\psi ' \\\\epsilon'.\\\\ ] ] hence , let we let @xmath249 and since the summands in @xmath250 are isomorphisms , it is clear that @xmath236 is irreducible .\\nnext , assume @xmath251 is a basis for the space of irreducible maps from @xmath252 to @xmath50 .\\nthen , by lemma [ l : non - van ] the set @xmath253 is also linearly independent . for each @xmath254 , consider the corresponding map @xmath255 , such that @xmath233 \\\\circ \\\\beta_t = \\\\alpha_t[-c ] \\\\circ \\\\gamma_i^{(0,c)}$ ] , and which we by lemma [ l : irred ] can assume is irreducible . assume a non - trivial linear combination @xmath256 is zero\\n. then also @xmath257 \\\\circ \\\\beta_t ) =   \\\\sum k_t \\\\alpha_t \\\\circ \\\\gamma_i^{(0,c)}=0 $ ] . but\\nthis contradicts lemma [ l : non - van ] since @xmath258 is irreducible .\\nhence it follows that @xmath259 is also linearly independent .\\nhence , in the exchange triangle @xmath260 , we have that @xmath90 appears with multiplicity at least @xmath261 in @xmath262 .\\nso , we have that @xmath206 , and the proof of the proposition is complete .\\nin this section we show how mutation in the vertex @xmath70 affects the complements of the almost complete tilting object @xmath263 . as\\nbefore , let @xmath264 be an @xmath0-tilting object , and let @xmath94 .    we need to consider @xmath265 \\\\ar^{(e)}[r ] & t_j \\\\ar^{(d)}[r ] & t_k } \\\\ ] ] for all possible values of @xmath266 . however , we have the following restriction on the colour of arrows .    [\\np : limits ] assume @xmath267 and @xmath268\\n. then @xmath269 .\\nconsider the exchange triangle @xmath270 .\\nnote that @xmath90 is a direct summand in the middle term @xmath271 by the assumption that @xmath272 .\\nconsider also the exchange triangle @xmath273 .\\npick an arbitrary non - zero map @xmath274 , and consider the map @xmath275 .\\nit suffices to show that whenever @xmath276 , then @xmath277 is not irreducible in @xmath46 .\\nso assume that @xmath276 .\\nwe claim that there is a commutative diagram @xmath278 \\\\ar[d ] &   t_j \\\\amalg x ' \\\\ar[r ] \\\\ar^{\\\\left ( \\\\begin{smallmatrix } h & 0 \\\\\\\\ 0 & 0 \\\\end{smallmatrix } \\\\right)}[d ] &    t_i^{(e+1 ) } \\\\ar[r ] \\\\ar[d ] & \\\\\\\\\\nt_i^{(c ) } \\\\ar[r ] & t_k \\\\amalg z \\\\ar[r ] & t_i^{(c+1 ) } \\\\ar[r ] &   } \\\\ ] ] where the rows are the exchange triangles .\\nthe composition @xmath279 is zero since    * if @xmath280 @xmath281 by using @xmath276 and lemma [ l : div](e ) * if @xmath282 , there is no non - zero composition @xmath283    hence the leftmost vertical map exists , and then the rightmost map exists , using that @xmath20 is a triangulated category .\\nthen , since @xmath284 , t_i^{(c ) } ) = 0 $ ] by lemma [ l : div](e ) , there is a map @xmath285 , such that @xmath286 .\\nhence there is map @xmath287 such that @xmath288 . by restriction\\nwe get @xmath289    under the assumption @xmath290 we have that @xmath291 can not be irreducible in @xmath292 .\\nhence @xmath293 , where @xmath51 is not summand in @xmath294 .\\nalso , by proposition [ l : disjoint ] we have that @xmath90 is not a summand in @xmath294 .\\nif @xmath295 was irreducible in @xmath296 , then there would be an irreducible map @xmath297 in @xmath298 , and since @xmath299 , this does not hold , by proposition [ l : disjoint ] .\\nhence , @xmath300 , where @xmath90 is not a direct summand of @xmath301 . also by proposition [ l : disjoint ] we have that @xmath51 is not a summand of @xmath301 . by ( [ factor ] ) , this shows that @xmath274 is not irreducible in @xmath46 .\\nlet @xmath302 .\\nfor @xmath303 , let @xmath304 denote the complements of @xmath305 , where there are exchange triangles @xmath306    we first want to compare @xmath304 with @xmath307 .    [ l : samecomp ] assume that @xmath308 for @xmath309 and that @xmath310 .\\n* for @xmath311 , the minimal left @xmath312-approximation @xmath313 is also an @xmath314-approximation . * for @xmath315\\n, we have @xmath316 .    by assumption @xmath90\\nis not a direct summand in any of the @xmath317 .\\nassume there is a map @xmath318 and consider the diagram    @xmath319 \\\\ar[r ] &   t_i^{(u ) } \\\\ar[r ] \\\\ar [ d ] & b_i^{(u ) } \\\\ar [ r ] & \\\\\\\\   & t_j^{(1 ) } & & } \\\\ ] ] since @xmath320)= 0 $ ] by lemma [ l : div2 ] , we see that the map @xmath318 factors through @xmath313 .\\nhence the minimal left @xmath312-approximation @xmath313 is also an @xmath314-approximation , so we have proved ( a ) .\\nthen ( b ) follows directly .\\n[ l : comp ] assume that @xmath321 and there are exchange triangles @xmath322 and @xmath323 where @xmath324 and @xmath325 , i.e. @xmath326 and @xmath327 , where @xmath51 is not isomorphic to any direct summand in @xmath328 .    * the composition @xmath329 is a left @xmath314-approximation .\\n* there is a triangle @xmath330 with @xmath331 in @xmath332 and @xmath333 .\\n* there is a triangle @xmath334 .\\nconsider an arbitrary map @xmath335 with @xmath45 in @xmath314 .\\nwe have that @xmath336 ) = 0 $ ] , by lemma [ l : div2 ] .\\nhence , by applying @xmath337 to the triangle ( [ i - tri ] ) we get that @xmath338 factors through @xmath339 . by applying @xmath337 to the triangle ( [ j - tri ] ) , and using that @xmath340 ) = 0 $ ] , we get that @xmath338 factors through @xmath341 .\\nthis proves ( a ) . for ( b ) and ( c )\\nwe use the exchange triangles ( [ i - tri ] ) and ( [ j - tri ] ) and the octahedral axiom to obtain the commutative diagram of triangles    @xmath342 \\\\ar@{=}[d ] & ( t_j)^p \\\\amalg x \\\\ar[r ] \\\\ar[d ]             & t_i^{(e+1 ) } \\\\ar[d ] \\\\ar[r ] & \\\\\\\\\\nt_i^{(e ) } \\\\ar[r ]         & ( t_k)^{pq } \\\\amalg y^p \\\\amalg x \\\\ar[r ]   \\\\ar[d ] & c \\\\ar[r ] \\\\ar[d ]          &   \\\\\\\\                      & ( t_j^{(1)})^p \\\\ar@{=}[r ]                     & ( t_j^{(1)})^p \\\\ar[r ]     &   } \\\\ ] ] by ( a ) the map @xmath343 is a left @xmath314-approximation , and by lemma [ l : samecomp ] we have that @xmath344 .\\nhence @xmath345 , where @xmath331 is in @xmath346 , and with no copies isomorphic to @xmath51 in @xmath328 .\\nnote that the induced @xmath314-approximation is in general not minimal .\\n[ l : modtri ] assume @xmath321 and @xmath347 .\\n* then there is a triangle @xmath348 where @xmath216 is a minimal left @xmath314-approximation , and @xmath331 is as in lemma [ l : comp ] .\\n* there is an induced exchange triangle @xmath349 where @xmath350 . *\\n@xmath351 .\\nconsider the exchange triangle @xmath352 \\\\to t_i^{(e+1 ) } \\\\to b_i^{(e+1 ) } \\\\to\\\\ ] ] and the triangle from lemma [ l : comp ] ( b ) @xmath353 apply the octahedral axiom , to obtain the commutative diagram of triangles    @xmath354 \\\\ar[r ] \\\\ar@{=}[d ] & t_i^{(e+1 ) } \\\\ar[r ] \\\\ar[d ]             & b_i^{(e+1 ) } \\\\ar[d ] \\\\ar[r ] & \\\\\\\\\\nt_i^{(e+2)}[-1 ] \\\\ar[r ]             & ( t_i^{(e+1 ) } ) ' \\\\amalg c ' \\\\ar[r ] \\\\ar[d ]   & g \\\\ar[r ] \\\\ar[d ]          & \\\\\\\\                                    & ( t_j^{(1)})^p \\\\ar@{=}[r ]            & ( t_j^{(1)})^p \\\\ar[r ]     &   } \\\\ ] ] since @xmath90 does not occur as a summand in @xmath294 by proposition [ l : disjoint ] , we have that @xmath355 ) = 0 $ ] . hence the rightmost\\ntriangle splits , so we have a triangle @xmath356 \\\\to ( t_i^{(e+1 ) } ) ' \\\\amalg c ' \\\\to b_i^{(e+1 ) } \\\\amalg ( t_j^{(1)})^p \\\\to\\\\ ] ] by lemma [ l : div2 ] we have that @xmath357)= 0 $ ] . by lemma [ l : div](e )\\nwe get that @xmath358 ) = 0 $ ] , and clearly @xmath359 ) = 0 $ ] , for @xmath360 .\\nwe hence get that all maps @xmath361 , with @xmath45 in @xmath362 , factor through @xmath363 .\\nminimality is clear from the triangle ( [ octa - tri ] ) .\\nthis proves ( a ) , and ( b ) follows from the fact that @xmath331 contains no copies of @xmath90 , and hence splits off .\\n( c ) is a direct consequence of ( b ) .\\n[ p : summarize ]    * if @xmath308 for @xmath364 , then @xmath365 for all @xmath366 . * if @xmath321 and @xmath347 , then @xmath365 for @xmath367 .\\n\\\\(a ) is a direct consequence of [ l : samecomp ] . for\\n( b ) note that by lemmas [ l : samecomp ] and [ l : modtri ] we have @xmath365 for @xmath368 and @xmath369 . for @xmath370\\nconsider the exchange triangles @xmath371 since @xmath372 ) = 0 $ ] by lemma [ l : div2 ] and @xmath373 , it is clear that the map @xmath374 is a left @xmath375-approximation .\\nhence ( b ) follows .\\nthis section contains the proof of the main result , theorem [ t : main ] . as before , let @xmath264 be an @xmath0-tilting object , and let @xmath94 .\\nwe will compare the numbers of @xmath77-coloured arrows from @xmath13 to @xmath69 , in the coloured quivers of @xmath41 and @xmath109 , i.e. we will compare @xmath376 and @xmath377 .\\nwe need to consider an arbitrary @xmath41 whose coloured quiver locally looks like @xmath265 \\\\ar^{(e)}[r ] & t_j \\\\ar^{(d)}[r ] & t_k } \\\\ ] ] for any possible value of @xmath266 . our aim is to show that the formula @xmath378 holds .\\nthe case where @xmath379 is directly from the definition .\\nthe case where @xmath380 follows by condition ( ii ) for @xmath381 . for the rest of the proof\\nwe assume @xmath382 .\\nwe will divide the proof into four cases , where @xmath383 denotes the number of arrows from @xmath13 to @xmath70 , and @xmath384 .    * @xmath385 * @xmath386 , @xmath321 and @xmath387 * @xmath386 , @xmath321 and @xmath388 . * @xmath386 and @xmath389    note that in the three first cases , the formula reduces to @xmath390 and in the first two cases it further reduces to @xmath391    case i.\\nwe first consider the situation where there is no coloured arrow @xmath392 , i.e. @xmath308 for all @xmath393 .\\nthat is , we assume @xmath87 locally looks like this @xmath265   & t_j \\\\ar^{(d)}[r ] & t_k } \\\\ ] ] with @xmath394 arbitrary .\\nit is a direct consequence of proposition [ p : summarize ] that @xmath395 for all @xmath393 which shows that the formula holds .\\n+   + case ii .\\nwe consider the setting where we assume @xmath87 locally looks like this @xmath265 \\\\ar^{(e)}[r ] & t_j \\\\ar^{(d)}[r ] & t_k } \\\\ ] ] with @xmath321 and @xmath396 .\\nwe then claim that we have the following , which shows that the formula holds .    in the above setting @xmath395 for all @xmath393 .\\nit follows directly from proposition [ p : summarize ] that @xmath395 for @xmath397 .\\nwe claim that @xmath398 .    by lemma [ l : comp ]\\nwe have the ( not necessarily minimal ) left @xmath314-approximation    @xmath399    first , assume that @xmath51 does not appear as a summand in @xmath326 , then the same holds for @xmath400 , and hence for @xmath401 which is a direct summand in @xmath400\\n.    next , assume @xmath51 appears as a summand in @xmath271 , and hence in @xmath39 .\\nthen @xmath51 is by proposition [ l : disjoint ] not a summand in @xmath294 , and by lemma [ l : modtri ] we have that @xmath51 is also not a summand in @xmath331 .\\ntherefore @xmath51 appears with the same multiplicity in @xmath271 as in @xmath401 , also in this case .\\nwe now show that @xmath395 for @xmath402 .    if @xmath403 , then @xmath404 for @xmath402 and we are finished .\\nso assume @xmath405 , i.e. @xmath51 does not appear as a direct summand of @xmath39 .\\nconsider the map @xmath406 we have that @xmath407 . by assumption , @xmath51 is not a direct summand in @xmath408 , and thus not in @xmath331 .\\nhence it follows that @xmath409 .\\nsince , by proposition [ p : summarize ] we have for @xmath410 , that @xmath316 and the left @xmath411-approximation coincide with the left @xmath412-approximations of @xmath413 , it now follows that @xmath395 for all @xmath393 .\\n+ case iii .\\nwe now consider the setting with @xmath414 non - zero , @xmath388 and @xmath321 .\\nthat is , we assume @xmath87 locally looks like this @xmath265 \\\\ar^{(e)}[r ] & t_j \\\\ar^{(0)}[r ] & t_k } \\\\ ] ] where @xmath415 by proposition [ p : limits ] , and where there are @xmath416 arrows from @xmath50 to @xmath51 .\\n[ l : formulas ] in the above setting , we have that @xmath381 is given by    @xmath417    @xmath418    and    @xmath419    we first deal with the case where @xmath420 and @xmath421 . by assumption @xmath39 in the triangle ( [ i - tri ] )\\nhas @xmath422 copies of @xmath51 , so @xmath423 has @xmath424 copies of @xmath51 . hence to show ( [ form1 ] ) it is sufficient to show that @xmath331 in the triangle @xmath425 has no copies of @xmath51 .\\nthis follows directly from the lemma [ l : modtri ] and the fact that @xmath51 ( by the assumption that @xmath421 and proposition [ l : disjoint ] ) is not a summand in @xmath294 . in this case\\n( [ form2 ] ) and ( [ form3 ] ) follow directly from proposition [ l : disjoint ] .\\nconsider the case with @xmath426 and @xmath427 .\\nwe have that @xmath39 in the triangle ( [ i - tri ] ) does not have @xmath51 as a direct summand .\\nassume @xmath51 appears as a direct summand of @xmath331 with multiplicity @xmath428 .\\nwe claim that @xmath429 .\\nassume first @xmath430 , then on one hand @xmath51 appears with multiplicity @xmath431 in @xmath432 . on\\nthe other hand @xmath51 appears with multiplicity @xmath433 in @xmath401 .\\nthis contradicts proposition [ l : disjoint ] .\\nhence @xmath429 .    therefore @xmath401 has @xmath434 copies of @xmath51 and ( [ form1 ] ) and ( [ form2 ] ) hold . if @xmath435 , then ( [ form3 ] ) follows directly from the above and proposition [ l : disjoint ] . in the case\\n@xmath436 , we also need to show that @xmath51 does not appear as a summand in @xmath437 for @xmath438 .\\nsince @xmath439 , we have @xmath440 , and the result follows from proposition [ p : summarize ] .\\nnow assume @xmath426 and @xmath441 .\\nassume @xmath442 , where @xmath51 is not a summand in @xmath443 .\\nnow since @xmath444 with @xmath51 not a summand in @xmath328 , is a minimal left @xmath362-approximation , we have that @xmath445 and @xmath51 appears with multiplicity @xmath446 in the minimal left @xmath447-approximation of @xmath448 , hence @xmath51 can not appear as a summand in the minimal left @xmath375-approximation of @xmath449 .\\nhence @xmath450 , and we have completed the proof of ( [ form1 ] ) and ( [ form2 ] ) in this case .\\nthe case ( [ form3 ] ) , i.e. @xmath451 follows from proposition [ l : disjoint ] .\\n+ case iv .\\nwe now consider the case with @xmath452 .\\nassume first there are no arrows from @xmath70 to @xmath69 .\\nthen we can use the symmetry proved in proposition [ p : symmetry ] and reduce to case i. the formula is easily verified in this case .\\nassume @xmath453 , again we can use the symmetry , this time to reduce to case iii .\\nit is straightforward to verify that the formula holds also in this case .\\nassume now that @xmath454 , i.e. we need to consider the following case @xmath455 \\\\ar@<0.6ex>^{(m)}[r ] & t_j \\\\ar@<0.6ex>^{(0)}[r ] \\\\ar@<0.6ex>^{(0)}[l ] & t_k \\\\ar@<0.6ex>^{(m)}[l ]   \\\\ar@<0.6ex>^{(m - c)}@/^3.5pc/[ll ] } \\\\ ] ] now by proposition [ p : limits ] we have that @xmath79 is in @xmath456 .\\nassume there are @xmath457 @xmath77-coloured arrows    the coloured quiver of @xmath109 is of the form @xmath458 \\\\ar@<0.6ex>^{(0)}[r ] & t_j^{(1 ) } \\\\ar@<0.6ex>^{(m)}[r ] \\\\ar@<0.6ex>^{(m)}[l ] & t_k \\\\ar@<0.6ex>^{(0)}[l ]   \\\\ar@<0.6ex>^{(m - c')}@/^3.5pc/[ll ] } \\\\ ] ] and applying the symmetry of proposition [ p : symmetry ] we have that if @xmath421 , then @xmath459 by proposition [ p : limits ] .\\nhence for all @xmath460 we have that @xmath461 .\\ntherefore it suffices to show that @xmath462 , for @xmath463 .\\nthis is a direct consequence of the following .\\nassume we are in the above setting .\\na map @xmath464 or @xmath465 is irreducible in @xmath46 if and only if it is irreducible in @xmath362 .\\nassume @xmath464 is not irreducible in @xmath362 , and that @xmath466 for some @xmath467 , with @xmath468 the indecomposable direct summands of @xmath45 .\\nnote that by lemma [ l : div](a ) , we can assume that all @xmath469 and all @xmath470 are non - isomorphisms . if there is some index @xmath471 such that @xmath472 , the map @xmath470 factors through some @xmath473 in @xmath474 , since there are no @xmath475-coloured arrows @xmath476 or @xmath477 in the coloured quiver of @xmath41 .\\nthis shows that @xmath464 is not irreducible in @xmath46 .\\nassume @xmath464 is not irreducible in @xmath46 , and that @xmath478 for some @xmath479 , with @xmath480 the indecomposable direct summands of @xmath481 . if there is some index @xmath471 such that @xmath482 , the map @xmath483 factors through @xmath484 , which is in @xmath485 , since there are no @xmath93-coloured arrows @xmath392 or @xmath486 in the coloured quiver of @xmath41 .\\nthis shows that @xmath464 is not irreducible in @xmath362 .    by symmetry\\n, the same property holds for maps @xmath465 .\\nthus we have proven that the formula holds in all four cases , and this finishes the proof of theorem [ t : main ] .\\nan @xmath0-cluster - tilted algebra is an algebra given as @xmath487 for some tilting object @xmath41 in an @xmath0-cluster category @xmath488 . obviously , the subquiver of the coloured quiver of @xmath41 given by the @xmath93-coloured maps is the gabriel quiver of @xmath487 .\\nan application of our main theorem is that the quivers of the @xmath0-cluster - tilted algebras can be combinatorially determined via repeated ( coloured ) mutation . for this one\\nneeds transitivity in the tilting graph of @xmath0-tilting objects .\\nmore precisely , we need the following , which is also pointed out in @xcite .    any @xmath0-tilting object can be reached from any other @xmath0-tilting object via iterated mutation .\\nwe sketch a proof for the convenience of the reader .\\nlet @xmath109 be a tilting object in an @xmath0-cluster category @xmath20 of the hereditary algebra @xmath489 , and let @xmath490 be the @xmath491-cluster category of @xmath10 .\\nby @xcite , there is a tilting object @xmath41 of degree 0 , i.e. all direct summands in @xmath41 have degree 0 , such that @xmath41 can be reached from @xmath109 via mutation .\\nit is sufficient to show that the canonical tilting object @xmath10 can be reached from @xmath41 via mutation .\\nsince @xmath41 is of degree 0 , it is induced from a @xmath10-tilting module .\\nespecially @xmath41 is a tilting object in @xmath490 . since @xmath41 and @xmath10 are tilting objects in @xmath490 , by @xcite there are @xmath490-tilting objects @xmath492 , such that @xmath50 mutates to @xmath493 ( in @xmath490 ) for @xmath494 .\\nnow each @xmath50 is induced by a tilting module for some @xmath495 where all @xmath496 are derived equivalent to @xmath497 . hence , each @xmath50 is easily seen to be an @xmath0-cluster tilting object . since @xmath493 differs from @xmath50 in only one summand the mutations in @xmath490 are also mutations in @xmath20 .\\nthis concludes the proof .\\na direct consequence of the transitivity is the following .\\nfor an @xmath0-cluster category @xmath111 of the acyclic quiver @xmath71 , all quivers of @xmath0-cluster - tilted algebras are given by repeated coloured mutation of @xmath71 .\\nin this section , we discuss concrete computation with tilting objects in an @xmath0-cluster tilting category .\\nan exceptional indecomposable object in @xmath9 is uniquely determined by its image @xmath498 $ ] in the grothendieck group @xmath499 .\\nthere is a map from @xmath500 to @xmath499 which , for @xmath501 , takes @xmath502 $ ] to @xmath503 $ ] .\\nan exceptional indecomposable in @xmath500 can be uniquely specified by its class in @xmath499 together with its degree .\\nthe map from @xmath500 to @xmath499 does not descend to @xmath504 .\\nhowever , if we fix our usual choice of fundamental domain in @xmath500 , then we can identify the indecomposable objects in it as above .\\nlet us define the combinatorial data corresponding to a tilting object @xmath41 to be @xmath87 together with @xmath505 , \\\\deg t_i)$ ] for @xmath506 .\\ngiven the combinatorial data for a tilting object @xmath41 in @xmath504 , it is possible to determine , by a purely combinatorial procedure , the combinatorial data for the tilting object which results from an arbitrary sequence of mutations applied to @xmath41 .    clearly , it suffices to show that , for any @xmath13 , we can determine the class and degree for @xmath507 .\\nif we can do that then , by the coloured mutation procedure , we can determine the coloured quiver for @xmath508 , and by applying this procedure repeatedly , we can calculate the result of an arbitrary sequence of mutations .    since we are given @xmath87 , we know @xmath509 , and we can calculate @xmath510 $ ] .\\nnow we have the following lemma :    [ one ] @xmath511=[b_i^{(0)}]-[t^{(0)}_i]$ ] , and @xmath512 or @xmath513 , whichever is consistent with the sign of the class of @xmath511 $ ] , unless this yields a non - projective indecomposable object in degree @xmath0 , or an indecomposable of degree @xmath1 .\\nthe proof is immediate from the exchange triangle @xmath514 .    applying this lemma , and supposing that we are not in the case where its procedure fails\\n, we can determine the class and degree @xmath515 . by the coloured mutation procedure\\n, we can also determine the coloured quiver for @xmath516 .\\nwe therefore have all the necessary data to apply lemma  [ one ] again . repeatedly applying the lemma\\n, there is some @xmath69 such that we can calculate the class and degree of @xmath507 for @xmath517 , and the procedure described in the lemma fails to calculate @xmath518 .\\nwe also have the following lemma :    [ two ] @xmath519=[b_i^{(m)}]-[t^{(0)}_i]$ ] , and @xmath520 or @xmath521 , whichever is consistent with the sign of @xmath519 $ ] , unless this yields an indecomposable in degree @xmath522 .\\napplying this lemma , starting again with @xmath41 , we can obtain the degree and class for @xmath523 .\\nwe can then determine the coloured quiver for @xmath524 , and we are now in a position to apply lemma  [ two ] again . the last complement which lemma  [ two ] will successfully determine is @xmath525 .\\nit follows that we can determine the degree and class of any complement to @xmath263 .\\nin this section , we discuss the application of our results to the study of the @xmath0-cluster complex , a simplicial complex defined in @xcite for a finite root system @xmath526 .\\nwe shall begin by stating our results for the @xmath0-cluster complex in purely combinatorial language , and then briefly describe how they follow from the representation - theoretic perspective in the rest of the paper . for simplicity ,\\nwe restrict to the case where @xmath526 is simply laced .\\nnumber the vertices of the dynkin diagram for @xmath526 from 1 to @xmath48 .\\nthe @xmath0-coloured almost positive roots , @xmath527 , consist of @xmath0 copies of the positive roots , numbered @xmath491 to @xmath0 , together with a single copy of the negative simple roots .\\nwe refer to an element of the @xmath13-th copy of @xmath528 as having colour @xmath13 , and we write such an element as @xmath529 .\\nsince the dynkin diagram for @xmath526 is a tree , it is bipartite ; we fix a bipartition @xmath530 .\\nthe @xmath0-cluster complex , @xmath531 , is a simplicial complex on the ground set @xmath527 .\\nits maximal faces are called @xmath0-clusters .\\nthe definition of @xmath531 is combinatorial ; we refer the reader to @xcite .\\nthe @xmath0-clusters each consist of @xmath48 elements of @xmath527 ( * ? ? ? * theorem 2.9 ) .\\nevery codimension 1 face of @xmath531 is contained in exactly @xmath1 maximal faces ( * ? ? ?\\n* proposition 2.10 ) .\\nthere is a certain combinatorially - defined bijection @xmath532 , which takes faces of @xmath531 to faces of @xmath531 ( * ? ? ?\\n* theorem 2.4 )\\n.    it will be convenient to consider _ ordered @xmath0-clusters_. an ordered @xmath0-cluster is just a @xmath48-tuple from @xmath527 , the set of whose elements forms an @xmath0-cluster .\\nwrite @xmath533 for the set of ordered @xmath0-clusters .    for each ordered @xmath0-cluster @xmath534\\n, we will define a coloured quiver @xmath535 .\\nwe will also define an operation @xmath536 , which takes ordered @xmath0-clusters to ordered @xmath0-clusters , changing only the @xmath70-th element .\\nwe will define both operations inductively .\\nthe set @xmath537 of negative simple roots forms an @xmath0-cluster .\\nits associated quiver is defined by drawing , for each edge @xmath538 in the dynkin diagram , a pair of arrows .\\nsuppose @xmath539 and @xmath540 .\\nthen we draw an arrow from @xmath13 to @xmath70 with colour @xmath541 , and an arrow from @xmath70 to @xmath13 with colour @xmath0 .\\nsuppose now that we have some ordered @xmath0-cluster @xmath542 , together with its quiver @xmath535 .\\nwe will now proceed to define @xmath543 . write @xmath544 for the number of arrows in @xmath535 of colour @xmath541 from @xmath70 to @xmath69 .\\ndefine :    @xmath545    let @xmath79 be the colour of @xmath546 .\\nwe define @xmath543 by replacing @xmath546 by some other element of @xmath527 , according to the following rules :    * if @xmath546 is positive and @xmath236 is positive , replace @xmath546 by @xmath547 . *\\nif @xmath546 is positive and @xmath236 is negative , replace @xmath546 by @xmath548 . *\\nif @xmath546 is negative simple @xmath549 , define @xmath550 by @xmath551 , and then replace @xmath546 by @xmath552 , with colour zero .\\ndefine the quiver for the @xmath0-cluster @xmath543 by the coloured quiver mutation rule from section 2 .\\nsince any @xmath0-cluster can be obtained from @xmath537 by a sequence of mutations , the above suffices to define @xmath543 and @xmath535 for any ordered @xmath0-cluster @xmath542 .\\nthe operation @xmath553 defined above takes @xmath0-clusters to @xmath0-clusters , and the @xmath0-clusters @xmath554 for @xmath555 are exactly those containing all the @xmath556 for @xmath557 .\\nthe connection between the combinatorics discussed here and the representation theory in the rest of the paper is as follows .\\n@xmath527 corresponds to the indecomposable objects of ( a fundamental domain for ) @xmath36 .\\nthe cluster tilting objects in @xmath36 correspond to the @xmath0-clusters .\\nthe operation @xmath558 corresponds to @xmath25 $ ] . for further details on the translation ,\\nthe reader is referred to @xcite .\\nthe above proposition then follows from the approach taken in section [ sec : cc ] .\\nhere we give an alternative description of coloured quiver mutation at vertex @xmath70 .    1 .   for each pair of arrows @xmath559 & j\\\\ar^{(0)}[r ] & k } \\\\ ] ] with @xmath560 , the arrow from @xmath13 to @xmath70 of arbitrary colour @xmath79 , and the arrow from @xmath70 to @xmath69 of colour @xmath541 , add a pair of arrows : an arrow from @xmath13 to @xmath69 of colour @xmath79 , and one from @xmath69 to @xmath13 of colour @xmath561 .\\n2 .   if the graph violates property ii , because for some pair of vertices @xmath13 and @xmath69 there are arrows from @xmath13 to @xmath69 which have two different colours , cancel the same number of arrows of each colour , until property ii is satisfied .\\n3 .   add one to the colour of any arrow going into @xmath70 and subtract one from the colour of any arrow going out of @xmath70 .\\nthe above algorithm is well - defined and correctly calculates coloured quiver mutation as previously defined .\\nfix a quiver @xmath71 and a vertex @xmath70 at which the mutation is being carried out .    to prove that the algorithm is well - defined\\n, we must show that at step 2 , there are only two colours of arrows running from @xmath13 to @xmath69 for any pair of vertices @xmath13 , @xmath69 .\\n( otherwise there would be more than one way to carry out the cancellation procedure of step 2 . )    since in the original quiver @xmath71 , there was only one colour of arrows from @xmath13 to @xmath69 , in order for this problem to arise , we must have added two different colours of arrows from @xmath13 to @xmath69 at step 1 .\\ntwo colours of arrows will only be added from @xmath13 to @xmath69 if , in @xmath71 , there are both @xmath93-coloured arrows from @xmath70 to @xmath69 and from @xmath70 to @xmath13 . in this case , by property iii , there are @xmath562-coloured arrows from @xmath13 to @xmath70 and from @xmath69 to @xmath70 .\\nit follows that in step 1 , we will add both @xmath93-coloured and @xmath562-coloured arrows . applying proposition 5.1 , we see that any arrows from @xmath13 to @xmath69 in @xmath71 are of colour 0 or @xmath0 . thus , as desired , after step 1 , there are only two colours of arrows in the quiver , so step 2 is well - defined .\\nwe now prove correctness .\\nlet @xmath563 .\\nwrite @xmath76 for the number of @xmath79-coloured arrows from @xmath13 to @xmath70 in @xmath71 , and similarly @xmath564 for @xmath565 .\\nwrite @xmath566 and @xmath567 for the result of applying the above algorithm .\\nit is clear that only the final step of the algorithm is relevant for @xmath568 where one of @xmath13 or @xmath69 coincides with @xmath70 , and therefore that in this case @xmath569 as desired .\\nsuppose now that neither @xmath13 nor @xmath69 coincides with @xmath70 .\\nsuppose further that in @xmath71 there are no @xmath93-coloured arrows from either @xmath13 or @xmath69 to @xmath70 , and therefore also no @xmath0-coloured arrows from @xmath69 to @xmath13 or @xmath70 .\\nin this case , @xmath570 . in the algorithm , no arrows will be added between @xmath13 and @xmath69 in step 1 , and therefore no further changes will be made in step 2 .\\nthus @xmath571 , as desired .\\nsuppose now that there are @xmath93-coloured arrows from @xmath70 to both @xmath13 and @xmath69 . in this case ,\\n@xmath572 . in this case , as discussed in the proof of well - definedness , an equal number of @xmath93-coloured and @xmath562-coloured arrows will be introduced at step 1 .\\nthey will therefore be cancelled at step 2 .\\nthus @xmath573 as desired .\\nsuppose now that there is a @xmath93-coloured arrow from @xmath70 to @xmath69 , but not from @xmath70 to @xmath13 .\\nlet the arrows from @xmath13 to @xmath70 , if any , be of colour @xmath79 .\\nat step 1 of the algorithm , we will add @xmath574 arrows of colour @xmath79 to @xmath71 . by proposition 5.1 ,\\nthe arrows in @xmath71 from @xmath13 to @xmath69 are of colour @xmath79 or @xmath575 .\\none verifies that the algorithm yields the same result as coloured quiver mutation , in the three cases that the arrows from @xmath13 to @xmath69 in @xmath71 are of colour @xmath79 , that they are of colour @xmath575 but there are fewer than @xmath574 , and that they are of colour @xmath575 and there are at least as many as @xmath574 .    the final case , that there is a @xmath93-coloured arrow from @xmath70 to @xmath13 but not from @xmath70 to @xmath69 , is similar to the previous one .\\nin @xcite , a certain category @xmath576 is constructed , which is shown to be equivalent to the @xmath0-cluster category of dynkin type @xmath2 .\\nthe description of @xmath576 is as follows .\\ntake an @xmath577-gon @xmath578 , with vertices labelled clockwise from 1 to @xmath577 .\\nconsider the set @xmath39 of diagonals @xmath550 of @xmath578 with the property that @xmath550 divides @xmath578 into two polygons each having a number of sides congruent to 2 modulo @xmath0 . for each @xmath579 , there is an object @xmath580 in @xmath576 .\\nthese objects @xmath580 form the indecomposables of the additive category @xmath576 .\\nwe shall not recall the exact definition of the morphisms , other than to note that they are generated by the morphisms @xmath581 which exist provided that @xmath538 and @xmath582 are both diagonals in @xmath39 , and that , starting at @xmath70 and moving clockwise around @xmath578 , one reaches @xmath69 before @xmath13 .\\na collection of diagonals in @xmath39 is called non - crossing if its elements intersect pairwise only on the boundary of the polygon\\n. an inclusion - maximal such collection of diagonals divides @xmath578 into @xmath583-gons ; we therefore refer to such a collection of diagonals as an @xmath583-angulation .\\nif we remove one diagonal @xmath550 from an @xmath583-angulation @xmath584 , then the two @xmath583-gons on either side of @xmath550 become a single @xmath585-gon .\\nwe say that @xmath550 is a _ diameter _ of this @xmath585-gon , since it connects vertices which are diametrically opposite ( with respect to the @xmath585-gon ) .\\nif @xmath586 is another diameter of this @xmath585-gon , then @xmath587 is another maximal noncrossing collection of diagonals from @xmath39 .\\n( in particular , @xmath588 . )    for @xmath584 an @xmath583-angulation , let @xmath589\\n. then we have that @xmath590 is a basic ( @xmath0-cluster-)tilting object for @xmath576 , and all basic tilting objects of @xmath576 arise in this way .\\nit follows from the previous discussion that if @xmath591 is a basic tilting object , and @xmath592 , then the complements to @xmath593 will consist of the objects @xmath594 where @xmath586 is a diameter of the @xmath585-gon obtained by removing @xmath550 from the @xmath583-angulation determined by @xmath584 .\\nin fact , we can be more precise . define @xmath595 to be the diameter of the @xmath585-gon obtained by rotating the vertices of @xmath550 by @xmath13 steps counterclockwise ( within the @xmath585-gon )\\n. then @xmath596 .      the coloured quiver @xmath598 of @xmath591 has an arrow from @xmath550 to @xmath586 if and only if @xmath550 and @xmath586 both lie on some @xmath583-gon in the @xmath583-angulation defined by @xmath584 . in this case\\n, the colour of the arrow is the number of edges forming the segment of the boundary of the @xmath583-gon which lies between @xmath550 and @xmath586 , counterclockwise from @xmath550 and clockwise from @xmath586 .\\nwe return to the example from section 2 .\\nthe quadrangulation of a decagon corresponding to the tilting object @xmath41 is on the left .\\nthe quadrangulation corresponding to @xmath109 is on the right .\\npassing from the figure on the left to the figure on the right , the diagonal 27 ( which corresponds to the summand @xmath599 ) has been rotated one step counterclockwise within the hexagon with vertices 1,2,3,4,7,10 .\",\n",
              "  'abstract': ' we define mutation on coloured quivers associated to tilting objects in higher cluster categories . \\n we show that this operation is compatible with the mutation operation on the tilting objects . \\n this gives a combinatorial approach to tilting in higher cluster categories and especially an algorithm to determine the gabriel quivers of tilting objects in such categories .    \\n [ section ] [ lemma]proposition [ lemma]corollary [ lemma]theorem [ lemma]remark [ lemma]definition [ lemma]example ',\n",
              "  'section_names': 'introduction\\nhigher cluster categories\\ncoloured quiver mutation\\nfurther background on higher cluster categories\\nsymmetry\\ncomplements after mutation\\nproof of the main result\\n@xmath0-cluster-tilted algebras\\ncombinatorial computation\\nthe @xmath0-cluster complex\\nan alternative algorithm for coloured mutation\\nexample: type @xmath2'},\n",
              " {'article': \"atom interferometry has opened new frontiers in precision metrology .\\nhighly sensitive gravimeters , gravity gradiometers , and gyroscopes have been constructed , and promising work has been done to integrate these sensors into a robust apparatus that can operate outside the laboratory with applications in inertial navigation and geodesy @xcite .\\nmoreover , atom interferometers have been used to make competitive measurements of the fine structure constant @xcite .\\nsince atom interferometric measurements of the fine structure constant do not assume the validity of quantum electrodynamics ( qed ) , while determinations of the fine structure constant based on measurements of the electron magnetic moment do make this assumption , comparison between the results of these two methods provides a stringent test of qed @xcite .\\nin addition , an experiment to test einstein s equivalence principle with unprecedented precision is underway @xcite , and atom interferometric gravitational wave detectors offer the possibility to study gravitational radiation in frequency ranges complementary to ligo and lisa @xcite .\\natom interferometers have traditionally relied on matter gratings or light pulses to act as beam splitters and mirrors for matter waves , with atomic wave packets traveling freely between these interaction zones .\\nlight - pulse schemes using either raman pulses ( where the internal state of the atom is changed ) or bragg pulses ( where the internal state of the atom remains unchanged ) have been implemented , such as those described in @xcite . for a number of applications of light - pulse atom interferometers , such as measurements of gravity and rotation ,\\nthe sensitivity is proportional to the separation in momentum that can be attained between the two arms @xcite . in measurements of the fine structure constant\\n, the sensitivity scales as the square of this separation @xcite .\\ntherefore , significant efforts have been devoted to the development of large momentum transfer ( lmt ) beam splitters .\\nlmt beam splitters achieving momentum splittings of @xmath0 using multi - photon bragg pulses have recently been demonstrated @xcite .\\nhowever , the required laser intensities to make significant improvements on this result may prove to be prohibitive @xcite .\\nin contrast , lmt beam splitters that use several two - photon bragg pulses or a multi - photon bragg pulse of relatively small order to separate the two arms of the interferometer in momentum space , followed by the acceleration of one of the arms with an optical lattice , could potentially provide multiple order of magnitude increases in attainable momentum separations with relatively modest laser intensity requirements .\\nan atom interferometer that uses this method has been successfully operated in a proof of principle experiment ( with a maximum demonstrated momentum splitting of @xmath1 ) @xcite . in a separate experiment ,\\nan atom interferometer with @xmath2 lmt beam splitters has been realized using a similar technique @xcite .\\nalternatively , both arms of the interferometer could be simultaneously accelerated in opposite directions by two different optical lattices after the initial splitting .\\nusing this second scheme , an interferometer with @xmath0 lmt beam splitters that achieves 15@xmath3 contrast and an individual beam splitter that provides an @xmath4 momentum separation have been demonstrated @xcite .\\nthe utility of atom interferometry hinges upon the ability to precisely calculate the phase accumulated along the different arms of an interferometer @xcite , of which the phase acquired during interactions of the atoms with light is an important component . indeed , the phase obtained by an atom during a raman or bragg pulse is well - understood @xcite .\\nanalogously , in order to take full advantage of the potential of lattice beam splitters , we must have a detailed understanding of the phase evolution of an atom in an optical lattice . in this paper\\n, we provide a rigorous analytical treatment of this problem . to our knowledge ,\\nsuch a treatment has not been previously presented in the literature .    based on this analysis\\n, we propose atom interferometer geometries in which optical lattices are used to continuously guide the atoms , so that the atomic trajectories are precisely controlled for the duration of the interferometer sequence , with a different lattice guiding each arm of the interferometer ( as illustrated in fig .\\n[ fig : guidedinterf ] ) .\\nwe point out here a distinction in terminology between a lattice waveguide and a lattice beam splitter . here , a lattice waveguide is the use of a lattice to continuously control the trajectory of an arm of an atom interferometer .\\nwe note that two separate lattice waveguides can independently control the two arms of an interferometer , or a single lattice waveguide can simultaneously control both arms . in contrast\\n, a lattice beam splitter is an interaction of relatively short time ( in comparison to a waveguide ) with the primary purpose of splitting the arms of the interferometer in momentum space rather than providing continuous trajectory control .\\nthe underlying physics behind lattice waveguides and lattice beam splitters is the same , and they can be treated with a common formalism . a single lattice waveguide that simultaneously transfers @xmath5 of momentum to the two arms of a ramsey - bord interferometer has been previously achieved in @xcite .\\nhowever , to our knowledge , our idea of using optical lattice waveguides to create a fully confined atom interferometer has not been previously considered .\\nour analysis indicates that these lattice interferometers will offer unprecedented sensitivities for a wide variety of applications and that they will be able to operate effectively over distance scales previously considered too small to be studied by precision atom interferometry .\\nfor example , one particularly interesting configuration involves using two optical lattice waveguides to continuously pull the two arms of the interferometer apart , subsequently holding the two arms a fixed distance from each other in a single lattice waveguide that is common to the two arms , and then using two lattice waveguides to recombine the arms .\\nsuch a configuration could be used , for instance , as a gravimeter .\\nthe sensitivity of lattice interferometers is illustrated by the fact that , given the experimental parameters stated in @xcite ( @xmath6 atoms / shot and @xmath7 shots / s ) , a shot noise limited lattice gravimeter whose arms are separated by 1 m for an interrogation time of 10 s has a sensitivity of @xmath8 @xmath9/hz@xmath10 .\\nwe perform phase shift calculations for these lattice interferometers using the theoretical groundwork formulated in this paper , and we discuss how lattice interferometers can both exceed the performance of conventional atom interferometers in many standard applications and expand the types of measurements that can effectively be carried out using atom interferometry .        the paper is organized as follows .\\nii . describes the hamiltonian for an atom in an optical lattice in the different frames we use in the paper .\\niii . discusses the phase evolution of an atom in an optical lattice under the adiabatic approximation .\\niv . introduces the formalism of perturbative adiabatic expansion to calculate corrections to the adiabatic approximation , and sec .\\nv. applies this formalism to calculate phase corrections to a lattice beam splitter .\\nvi . proposes a number of interferometer geometries that make use of lattice manipulations of the atoms .\\nthe main results of the paper are eqs .\\n( [ eqn : phasecorrection ] ) and ( [ eqn : phasecorrectionref ] ) , which show how to obtain analytical corrections to the lowest order phase shift estimates .\\nthese corrections are surprisingly large , and understanding them is vital to realizing the full accuracy of the sensor geometries proposed in sec .\\nvi . , as well as other geometries utilizing optical lattice manipulations of the atoms .\\nfor example , the gravitational wave detector proposed in @xcite will likely make use of lattice beam splitters and/or waveguides .\\npreviously , the phase evolution induced by lattice manipulations was not sufficiently well - understood to allow for a detailed design of the atom optics system or an estimation of the corresponding systematic effects .\\nan optical lattice is a periodic potential formed by the superposition of two counter - propagating laser beams .\\natoms can be loaded into the ground state of the lattice by ramping up the lattice depth adiabatically , and the lattice can then be used to impart momentum to the atoms and/or to control the atoms trajectories .\\noptical lattices are thus a useful tool for atom optics .\\nwe begin our discussion of the lattice - atom interaction by finding a useful form for the hamiltonian .\\nas is typical for many applications of atom interferometry , to minimize decoherence we assume that we work with atomic gases dilute enough so that the effects of atom - atom interactions are negligible .\\nwe first consider the hamiltonian in the lab frame , where for now we assume a vertical configuration with constant gravitational acceleration @xmath9 so that we have a gravitational potential given by @xmath11 .\\nwe expose the atom to a superposition of an upward propagating beam with phase @xmath12 and a downward propagating beam with phase @xmath13 , which couples an internal ground state @xmath14 to an internal excited state @xmath15 .\\nthe two - photon rabi frequency is @xmath16 , where we let @xmath17 denote the single - photon rabi frequency of the upward propagating beam , @xmath18 denote the single - photon rabi frequency of the downward propagating beam , and @xmath19 denote the detuning from the excited state .\\nwe depict the physical setup in fig .\\n[ fig : setup ] . making the rotating wave approximation and\\nadiabatically eliminating the excited state as is standard procedure @xcite , we obtain the following hamiltonian  where the periodic term in the potential arises from a spatially varying ac stark shift and where @xmath20 is the magnitude of the wave vector of the laser beams @xcite :    @xmath21+mg\\\\hat{x}\\\\ ] ]    note that where the difference between the frequency of the upward propagating beam and the frequency of the downward propagating beam is denoted by @xmath22 , we will have the relation @xmath23 .\\nfor a given @xmath24 , the lattice standing wave will be translated by @xmath25 in the @xmath26 direction from the origin .\\nthus , the velocity of the lattice in the lab frame is :    @xmath27    and we rewrite the lab frame hamiltonian as :    @xmath28+mg\\\\hat{x}\\\\ ] ]    in order to most readily describe the dynamics of an atom in an accelerating optical lattice , it is useful to work in momentum space . the @xmath29 term that appears in the lab frame hamiltonian makes such an approach difficult , especially when considering non - adiabatic corrections to the phase shift .\\nhowever , we can change frames by performing a unitary transformation in order to obtain a hamiltonian that is easier to handle analytically . in the end\\n, we will see that approaching the problem from the point of view of dressed states provides a convenient hamiltonian for our purposes .\\nwe consider the transformation procedure from the lab frame to the dressed state frame in appendix a , where we also introduce an intermediate frame that freely falls with gravity ( which we call the freely falling frame ) .\\nwe note that the general form of the unitary transformations considered in appendix a as well as the specific transformations to the different frames we consider can also be found in the appendix of @xcite .     and\\nthe lasers are detuned from the transition between the atom s internal ground state and excited state so that the atom s external momentum states are coupled through two - photon transitions , creating an effective lattice potential.,width=672 ]    it is convenient to absorb the initial velocity @xmath31 of the atom in the lab frame into the dressed state frame , so that velocity @xmath31 in the lab frame corresponds to velocity zero in the dressed state frame .\\nthe hamiltonian in the dressed state frame is , as derived in appendix a :    @xmath32    now , we will show how working in momentum space allows us to represent @xmath33 as an infinite dimensional , discrete matrix .\\nthis matrix is discrete because the optical lattice potential term , @xmath34 , only couples a momentum eigenstate @xmath35 to the eigenstates @xmath36 and @xmath37 @xcite . for the moment\\n, we will examine the evolution of individual eigenstates of the dressed state hamiltonian @xmath33 .\\nthese eigenstates reduce to single momentum eigenstates @xmath35 when @xmath38 .\\nthe knowledge of how each of these eigenstates evolves under @xmath33 will allow us to describe the dynamics of an entire wavepacket .\\nfor the moment , we will only consider momentum eigenstates corresponding to an integer multiple of @xmath39 , since we have boosted away the initial velocity @xmath31 of the atom in the lab frame .\\nwe note that it is always possible to transform to a particular dressed state frame in which a given momentum eigenstate in the lab frame corresponds to zero momentum in that dressed state frame .\\nthe results we derive here can thus be readily generalized to arbitrary momentum eigenstates in a wavepacket , as we discuss in greater detail in appendix b.    we consider a discrete hilbert space spanned by the momentum eigenstates @xmath40 for integers\\n@xmath41 , so that we can express any vector in this hilbert space as :    @xmath42    since this hilbert space is discrete , it is natural to adopt the normalization convention that @xmath43 .\\nwhen considered as an operator acting on this discrete hilbert space , @xmath33 can be written as @xcite :    @xmath44    where we drop the common light shift .\\nnow , it is convenient to introduce the recoil frequency @xmath45 and the recoil velocity @xmath46 . in order to make our notation as compact as possible\\n, we will be interested in the quantity @xmath47 , which is the velocity of the lattice in the dressed state frame in units of @xmath48 .\\nwe can express the second term of @xmath49 in a useful way by noting that @xmath50 .\\nfurthermore , we define @xmath51\\n. we can now write the discrete hamiltonian in a simplified form :    @xmath52 \\\\\\\\\\\\ ] ]    the matrix elements of this hamiltonian are :    @xmath53\\\\ ] ]    in matrix notation , the schrodinger equation for the discrete hilbert space takes the form @xmath54 , where we let @xmath55 be the matrix whose element in the @xmath56th row and @xmath41th column is given by @xmath57 and we let @xmath58 be the column vector whose @xmath41th entry is @xmath59 .\\nnow that we have determined the hamiltonian matrix @xmath55 , we have the appropriate machinery in place to describe the phase evolution of an atom in an optical lattice .\\nwe consider the process in which momentum is transferred to the atom through bloch oscillations .\\nreference @xcite provides a thorough and insightful description of bloch oscillations in a number of different pictures . given our choice of hamiltonian\\n, we work in the dressed state picture , which is discussed in sec . iv.b . of @xcite , making extensive use of bloch s theorem , the concept of brillouin zones , and the band structure of the lattice @xcite . as in the previous discussion , we consider the evolution of single eigenstates of the dressed state hamiltonian , noting that we can easily generalize our results to the case of a wave packet of finite width , as we address in appendix b.    initially , we consider the system to be in a momentum eigenstate .\\nfirst , we adiabatically ramp up the lattice depth by increasing the laser power so that we load the system into an eigenstate of the hamiltonian . for the purposes considered here , we want the system to enter the ground eigenstate ( corresponding to the zeroth band of the lattice ) . in order for this to be achieved\\n, a resonance condition must be met , which states that the velocity of the lattice must match the velocity of the atom to within @xmath48 .\\nthe loading will be adiabatic if the adiabatic condition @xmath60 is satisfied , where @xmath61 and @xmath62 respectively denote the ground state and the first excited state of the hamiltonian @xcite\\n. this condition will be easier to meet near the center of the band ( where the velocity of the lattice is identical to the velocity of the atom ) , because the energy gap @xmath63 between the zeroth band ( which corresponds to ground state ) and the first band ( which corresponds to the first excited state ) becomes smaller as the velocity difference between the lattice and the atom becomes larger ( which corresponds to moving toward the border of the first brillouin zone ) .\\nthe resonance condition is discussed further in appendix c.    in the lab frame , the atom accelerates under gravity , increasing the deviation between its velocity and the lattice velocity during the loading process . in the freely falling and dressed state frames , in which gravity\\nis boosted away , this corresponds to the lattice accelerating upward while the atom remains at rest .\\nthis effect can negatively impact the loading efficiency if the loading sequence is sufficiently long so that the accrued velocity difference becomes a significant fraction of @xmath48 .\\nin such a scenario , the effect can be ameliorated by accelerating the lattice in the lab frame to fall with the atom , which corresponds to the lattice velocity remaining constant in the freely falling and dressed state frames .\\nfurthermore , we note that in the case of a lattice lmt beam splitter , the lattice should only be resonant with one arm of the interferometer , so that negligible population from the other arm is affected .\\notherwise , the signal could be distorted by multi - path interference , causing a systematic error in the estimation of the interferometer phase shift .\\nconditions for when the negative effects of off - resonant lattices can be avoided can be estimated using the hamiltonian matrix for an off - resonant lattice given in eq .\\n( [ eqn : offresmatrix ] )\\n. we discuss off - resonant lattices quantitatively and in more detail in sec .\\nvi .    after the adiabatic loading of the atom into the ground state of the hamiltonian\\n, the frequency difference between the laser beams is swept to accelerate the lattice , periodically imparting momentum to the atom in units of @xmath39 . in the dressed state frame , this phenomenon can be understood in terms of avoided line crossings , which occur because the coupling between the atom and the laser beams lifts the degeneracy at the crossing points .\\nwe refer the reader to fig .\\n10 of @xcite for a clear illustration of these avoided crossings . as the frequency difference\\nis swept so that the system passes through the avoided crossings , the system remains in the ground state of the dressed state hamiltonian as long as the process is adiabatic .\\nconsequently , at each of the avoided crossings , the momentum of the atom increases by @xmath39 , which corresponds to a bloch oscillation . finally , after the acceleration of the lattice , the frequency difference is held constant while the lattice depth is adiabatically ramped down , delivering the system into the momentum eigenstate @xmath64 , where @xmath65 is the momentum before the bloch oscillations and @xmath66 is the number of bloch oscillations .\\n[ fig : latticeaccel ] depicts the lattice depth and velocity as functions of time for the process described above and shows a numerical simulation of a particular instance of this process : the adiabatic loading of the lattice from an initial state @xmath67 , the transfer of @xmath2 of momentum through 5 bloch oscillations , and the ramping down of the lattice to deliver the system into the final state @xmath68 .\\nsince under the adiabatic approximation we assume that the atom always stays in the ground state of the hamiltonian , the phase @xmath69 of the atom evolves as follows :    @xmath70    where @xmath71 is the instantaneous ground state eigenvalue of the hamiltonian and @xmath72 is the initial phase of the atom .\\nin addition to eq .\\n( [ eqn : adiabaticphase ] ) , there is also a berry s phase term @xcite .\\nhowever , this term is zero for a linear external potential .\\ntherefore , there is no contribution from the berry s phase under the semiclassical approximation , as long as the external potential is treated as linear  we discuss the validity and ramifications of this approximation at the end of appendix a. we note that any such contribution would arise from the residual external potential terms of the dressed state hamiltonian that are non - linear in @xmath26 ( we collectively denote these terms as @xmath73 in appendix a and explain why they can often be neglected ) .     of momentum .\\nthe lattice depth and velocity are shown as a function of time in the right panel , where in this particular case the relevant parameters are @xmath74 , @xmath75 , @xmath76 , and @xmath77 ( corresponding to a final lattice velocity of 10 @xmath48 ) .\\nfirst , we adiabatically ramp up the lattice to a depth of @xmath78 so that the lattice is loaded into the ground state of the dressed state hamiltonian .\\nsubsequently , we accelerate and ramp down the lattice , leaving the atom in a single momentum eigenstate.,width=720 ]    we now consider how the eigenvectors and eigenvalues of @xmath55 change with @xmath79 , which we recall is the dimensionless velocity of the atom in the dressed state frame .\\nsay that the eigenvalues of @xmath80 are given by @xmath81 with corresponding eigenvectors @xmath82 , where the index @xmath41 runs from @xmath83 to @xmath84 . we choose to index the eigenvalues so that @xmath81 denotes the the @xmath41th eigenvalue labeled in order of increasing value .\\nmoreover , we let @xmath85 be the @xmath86th element of the column vector @xmath82 , so that @xmath87 . the transformation properties of the eigenvectors and eigenvalues under changes in @xmath79 can be deduced from bloch s theorem .\\nit can be shown that when the lattice velocity is increased by @xmath88 , which corresponds to @xmath79 being increased by two , while @xmath89 is kept fixed , the new eigenvectors can be obtained through the following relation @xcite :    @xmath90    this simply represents a shift of the wavefunction in momentum space by @xmath39 , which is exactly what we expect , since increasing the lattice velocity by @xmath91 corresponds to undergoing a single bloch oscillation .\\nthe dependence of the eigenvalues on the lattice velocity can be expressed as follows :    @xmath92    where @xmath93 is periodic in @xmath79 such that @xmath94 for integer @xmath56 holds for all @xmath79 and @xmath93 vanishes when the condition @xmath95 holds .\\nthis periodicity follows directly from bloch s theorem , since increasing @xmath79 by @xmath96 corresponds to increasing the lattice velocity by @xmath97 , meaning that @xmath79 and @xmath98 are at the same point in the first brillouin zone .\\nnote that the dependence of the eigenvalue on @xmath89 can be calculated using the truncated matrix approximation discussed in sec .\\nthe relevance of this dependence to the phase shift of an interferometer and how this dependence varies with momentum are discussed in sec .\\nv. and appendix b.    the first term in eq .\\n( [ eqn : evalue ] ) has a simple physical interpretation . where @xmath99 is the velocity of the lattice in the dressed state frame ( so that @xmath100 ) , note that :    @xmath101    which is simply the kinetic energy of an atom traveling along a classical trajectory defined by the motion of the lattice .\\nthe @xmath102 and @xmath93 terms represent the band structure of the lattice , with the @xmath93 term accounting for the bands deviating from being flat . for interferometer geometries in which lattices act as waveguides for the atoms , the net contributions to the phase shift from the @xmath103 and @xmath104 terms in the ground state energy and from corrections to the adiabatic approximation are often negligible , as explained in the following sections .\\nin this case , the phase difference between the two arms of the interferometer is given by ( assuming that the two arms arrive at the same endpoint ) :    @xmath105 \\\\\\\\ \\\\nonumber & = & \\\\frac{1}{\\\\hbar}\\\\left [ \\\\int_{0}^{t } \\\\frac{1}{2}mv_{\\\\text{lattice}}^{\\\\text{arm1}}(t)^2d t-\\\\int_{0}^{t } \\\\frac{1}{2}mv_{\\\\text{lattice}}^{\\\\text{arm2}}(t)^2 d t \\\\right ] \\\\\\\\\\\\end{aligned}\\\\ ] ]    where @xmath106 is the time elapsed during the interferometer sequence .\\nobserve that this expression for the phase difference can be obtained by assuming that the lattice potential acts as a constraint that forces the atoms in each arm to traverse the classical path traveled by the lattice guiding that arm . in this case\\nthe phase shift is just the difference of the respective action integrals over the two classical paths , as we would expect from the feynman path integral formulation of quantum mechanics @xcite . since the lattice is the only potential in the freely falling and dressed state frames , the action integrals yield eq .\\n( [ eqn : feynman ] ) ( for an insightful treatment of the applications of path integrals in atom interferometry , we refer the reader to @xcite ) . the terms that we have neglected in eq . ( [ eqn : feynman ] ) embody corrections to the simple picture of the lattice as a force of constraint arising from the quantum nature of the motion ( e.g. , a small portion of the population leaving the ground state of the lattice ) , which can sometimes be important .\\nhowever , our simple picture provides physical intuition into the lattice phase shift and is often sufficient to derive quantitative results .    to summarize , the eigenvalues of the lattice consist of a kinetic energy term , a term that depends only on the lattice depth , and a term that is periodic in @xmath79 , and the eigenvectors transform under a simple shift operation when @xmath79 is changed by @xmath107 for integer @xmath56 .\\nthese properties are a direct result of bloch s theorem .\\nthe symmetries that we have discussed allow us to conclude that if , for a given @xmath89 , we know the eigenvalues and eigenvectors of the hamiltonian for all @xmath79 within any range @xmath108 $ ] , we can subsequently determine the eigenvalues and eigenvectors for arbitrary @xmath79 .\\nthis result will prove to be useful from a computational standpoint , since the dynamics of the system are completely described by the solution within a finite range of @xmath79 .\\nwe now present the method of perturbative adiabatic expansion @xcite to determine corrections of arbitrary order to the adiabatic approximation .\\nwe note that the particular adiabatic approximation that we correct here refers to the adiabatic evolution of the ground state of the dressed state hamiltonian , rather than the adiabatic elimination of the excited state during the raman process , which is treated in @xcite .\\nthe corrections we consider will always be present to some extent , since lattice depth and velocity ramps occurring over a finite time can never be perfectly adiabatic .\\nin addition , non - adiabatic corrections can be caused by perturbations arising from laser frequency noise and amplitude noise . although our analytical and numerical computations indicate that the contribution of non - adiabatic corrections to the overall phase shift will be highly suppressed for many interferometer geometries , it is important to have a generalized framework with which to treat these corrections in order to determine when they are important and to precisely calculate them when necessary .    during the course of this derivation ,\\nit is convenient to parameterize the hamiltonian , eigenvalues , and eigenvectors using the single variable @xmath109 rather than the two variables @xmath110 and @xmath111 .\\nnote that much of our discussion will follow a similar outline as the proof of the adiabatic theorem in @xcite .\\na more detailed version of the derivation presented here can be found in @xcite .\\nfor all times @xmath109 , we can express any state vector @xmath58 in hilbert space as a linear combination of the instantaneous eigenvectors @xmath112 of the dressed state hamiltonian , where in general the coefficients of each eigenvector can vary in time .\\nthe instantaneous eigenvectors satisfy the relation @xmath113 . choosing coefficients with a phase @xmath114 factored out , we can write :    @xmath115    to simplify matters further ,\\nwe choose the phase of @xmath112 so that each element of @xmath112 is real for all @xmath109 and varies continuously with @xmath109 , which we can do because the particular hamiltonian matrix @xmath116 we consider is a real - valued , hermitian matrix .    when applied to eq .\\n( [ eqn : stateexpansion ] ) , the schrodinger equation gives us the relation :    @xmath117}\\\\ ] ]    we note that the inner products @xmath118 are zero for the case of a linear external potential , as verified numerically , and we will thus drop them from the sum .\\nthese inner products are closely related to the berry s phase , because integrating @xmath118 with respect to time gives the berry s phase for the @xmath86th eigenvector .\\n( [ eqn : coefderiv ] ) provides us with a relation which we could directly use to perform adiabatic expansion . but\\nto see more clearly how the adiabatic expansion series relates to the rate at which the hamiltonian changes in time , we will express @xmath119 in a more transparent form .\\nas long as @xmath120 , we can express @xmath121 in terms of a matrix element of @xmath122 and an energy difference @xcite :    @xmath123    we can therefore write eq .\\n( [ eqn : coefderiv ] ) as :    @xmath124 } \\\\right )   - \\\\left(\\\\sum_{n \\\\in s_{nd}(t ) } b_n(t ) \\\\frac{\\\\vec{\\\\psi}^{\\\\dag}_j(t ) \\\\dot{h}(t ) \\\\vec{\\\\psi}_n(t)}{\\\\varepsilon_n(t)-\\\\varepsilon_j(t ) } e^{i \\\\left[\\\\varphi_n(t ) - \\\\varphi_j(t ) \\\\right ] } \\\\right)\\\\ ] ]    where @xmath125 is the set of all @xmath41 such that @xmath126 and @xmath127 and @xmath128 is the set of all @xmath41 such that @xmath129 .\\n( [ eqn : non - adiabaticexpansion ] ) illuminates the rationale behind the adiabatic approximation . under the adiabatic approximation\\n, we assume that @xmath116 and hence also its eigenvectors vary slowly enough in time so that the conditions @xmath130 ( for @xmath129 ) and @xmath131 ( for @xmath127 and where @xmath132 is the time scale of the approximation ) hold . the righthand side of eq .\\n( [ eqn : non - adiabaticexpansion ] ) can therefore be approximated as zero .\\nthen , all the coefficients @xmath133 are constant in time .    to compute higher order corrections ,\\nwe employ the method of adiabatic expansion , which mathematically follows in the spirit of the born approximation . to zeroth order\\n, we take the coefficients @xmath133 to be constant as dictated by the adiabatic approximation . to obtain the first order corrections to these coefficients , we substitute the constant zeroth order coefficients @xmath134 into the righthand side of eq .\\n( [ eqn : coefderiv ] ) with @xmath126 and integrate to find the first order coefficients @xmath135 .\\nwe can repeat this process recursively , substituting the coefficients @xmath135 into eq .\\n( [ eqn : coefderiv ] ) to calculate the coefficients @xmath136 and continuing until we know the coefficients to the necessary precision .\\nthis method provides a way to construct a series expansion for each @xmath133 .\\nwe adopt a matrix notation for the terms in this series to facilitate the discussion . the first order correction to @xmath133\\nconsists of contributions from each nonzero @xmath137 where @xmath126 , and we depict the contribution from @xmath137 as @xmath138 .\\nso to first order , we can write :    @xmath139    where :    @xmath140 } dt^{\\\\prime}\\\\ ] ]    the second order solution for @xmath133 will then be :    @xmath141 } dt^{\\\\prime } = b^{(0)}_j+ \\\\sum_{n \\\\neq j } c_{n \\\\rightarrow j } +   \\\\sum_{n \\\\neq j } \\\\sum_{m \\\\neq n } c_{m \\\\rightarrow n \\\\rightarrow j}\\\\ ] ]    where :    @xmath142 } dt^{\\\\prime}\\\\ ] ]    under the implicit assumption that the time variable associated with @xmath143 is appropriate for the context in which it appears .\\nthe calculation of corrections of higher order is discussed in appendix d.    to find the eigenvectors and eigenvalues that we need to calculate the terms that make a non - negligible contribution to the expansion , we must approximate the infinite dimensional hamiltonian matrix as a finite dimensional truncated matrix . at the end of sec .\\n, we concluded that the problem of determining the eigenvalues and eigenvectors for all @xmath79 reduces to finding the eigenvalues and eigenvectors for a range @xmath144 $ ] for arbitrary @xmath145 .\\nin addition , it suffices to calculate the inner products @xmath121 just in this range of @xmath79 , which follows from the symmetry @xmath146 for integer @xmath56 @xcite . to make the calculation less cumbersome\\n, we can look at the range @xmath147 $ ] . for @xmath89 not too large and @xmath79 in this range ,\\nthe eigenvectors with lower energies are populated almost entirely by momentum eigenstates @xmath148 with relatively small @xmath149 .\\nthis is the case because for @xmath79 in the range , the diagonal elements of the hamiltonian will be smallest for values of @xmath56 close to zero .\\nwe note that for @xmath150 , the diagonal elements are the eigenvalues . in the limit of @xmath151\\n, each eigenvector will consist of only a single momentum eigenstate , where in general eigenvectors corresponding to momentum eigenstates with @xmath56 closer to zero will have lower eigenvalues . increasing @xmath89 will allow the lower eigenvectors to spread out in momentum space to a certain extent , but this will not change the fact that the lower eigenvectors will be linear combinations of momentum eigenstates corresponding to smaller values of @xmath149 . as discussed previously , the eigenvectors we care about for calculational purposes will be those with eigenvalues closer to the ground state eigenvalue .\\nwe can thus consider a truncated @xmath152 hamiltonian matrix centered around @xmath153 , where we choose @xmath41 to be large enough so that for the particular dynamics being described , a sufficient number of eigenvectors and eigenvalues can be calculated .\\nwe illustrate the above method by calculating phase corrections to a lattice beam splitter . in this example , we consider the case where two optical lattices of the same depth but different accelerations are used to separate the two arms of the interferometer ( after an initial momentum space splitting is achieved through bragg diffraction ) .\\nwe note that the analysis here is equally applicable to the situation where two separate optical lattice waveguides are used to address the two arms of the interferometer , an example of which is illustrated in fig .\\n[ fig : conj ] . to calculate the phase shift for applications in precision measurement\\n, we need to determine the non - adiabatic correction to the phase difference between the two arms that accrues during the beam splitter . in practice\\n, we do this by first calculating corrections to the ground state coefficient @xmath154 and then evaluating how these corrections affect the phase difference between the arms .\\nwe note that the dominant contribution to the phase difference will come from the zeroth order term as given in eq .\\n( [ eqn : feynman ] ) .    for this example\\n, we consider the situation shown in fig .\\n[ fig : latticeaccel ] , where the interaction of the atoms with the lattice is divided into three distinct parts . from @xmath155 to @xmath156 we ramp up the lattice , from @xmath156 to @xmath157 we accelerate the lattice , and from @xmath157 to @xmath158 we ramp down the lattice .\\nfor the sake of simplicity , the ramps are chosen to be symmetric so that the lattice depth decrease ramp is the time reversed lattice depth increase ramp .\\nwe assume that initially all of the population is in the ground state , so that @xmath159 , and we make the lattice depth and velocity ramps adiabatic enough so that almost all of the population remains in the ground state . in order to find the non - adiabatic correction to the phase shift , we determine the non - adiabatic correction to the phase of the ground state for each arm .\\nsince to lowest order only the ground state is populated , the leading corrections to @xmath154 will come at second order . using the formalism in eq .\\n( [ eqn : secondorder ] ) , the second order correction will be :    @xmath160    the largest contribution comes from @xmath161 , and it is this term on which we focus . because the ground state is non - degenerate , eqs .\\n( [ eqn : non - adiabaticexpansion ] ) and ( [ eqn : factor2 ] ) give us :    @xmath162 }    \\\\\\\\ \\\\nonumber & = & -\\\\int_{0}^{t } dt_1 \\\\left(-\\\\int_{0}^{t_1 } dt_2 \\\\frac{m_{10}(t_2)}{\\\\delta \\\\varepsilon_{10}(t_2 ) } e^{-\\\\frac{i}{\\\\hbar } \\\\int_{0}^{t_2 } \\\\delta \\\\varepsilon_{10}(t_3 ) dt_3 } \\\\right ) \\\\frac{m_{10}(t_1)}{-\\n\\\\delta \\\\varepsilon_{10}(t_1 ) } e^{\\\\frac{i}{\\\\hbar } \\\\int_{0}^{t_1 } \\\\delta \\\\varepsilon_{10}(t_3 ) dt_3 } \\\\\\\\\\\\end{aligned}\\\\ ] ]    where we define @xmath163 ( note that the two matrix elements are equal because we choose the eigenvectors to be real ) and @xmath164 .\\nwe examine the ultimate contribution of @xmath165 to @xmath166 .\\nsince during the ramp up and ramp down stages @xmath167 depends only on @xmath168 but not on @xmath169 , some portions of @xmath165 will be common to both arms of the interferometer , because we assume that the lattice interaction processes for the two arms differ only in the magnitude of the lattice acceleration .\\nwe denote these common terms as @xmath170 .\\nthe remaining terms depend on the lattice acceleration and will thus differ between the arms\\n. there will be a term @xmath171 that depends both on @xmath168 and @xmath169 .\\nhowever , it can be shown that under the assumption that the lattice depth decrease ramp is the time reversed lattice depth increase ramp , this term is zero @xcite . finally , there will be a term @xmath172 that depends quadratically on @xmath169 and is not explicitly dependent on @xmath168 .\\nwe note that @xmath172 implicitly depends on the maximum lattice depth @xmath173 , which can be seen by the fact that @xmath173 affects what value the energy gap @xmath174 takes on during the acceleration stage ( a deeper lattice leads to a larger energy gap ) .\\nwe can thus write :    @xmath175    in calculating @xmath172 , it is useful to note that @xmath167 takes on a convenient form during the acceleration stage . recalling the form of the hamiltonian matrix from eq .\\n( [ eqn : hamiltonianelements ] ) , we observe that @xmath122 will be a diagonal matrix with matrix elements @xmath176 .\\nwe can thus write @xmath177 , where @xmath178 is a weighted dot product .    in order to more clearly illuminate the general points we are illustrating with this example , we make the simplifying assumption that @xmath179 is constant throughout the acceleration stage .\\nmoreover , we assume that the lattice is deep enough so that @xmath180 and @xmath181 are also constant during the acceleration stage , which is an accurate approximation for typical experimental situations . during the acceleration stage\\n, we respectively denote these constant quantities as @xmath169 , @xmath182 , and @xmath183 .\\nnote that these assumptions , along with the assumption of mirror symmetry between the ramp up and ramp down stages , are certainly not necessary to carry out the calculation .\\nthey only serve to make the final result take a particularly simple form that provides physical insight into the process . in the absence of these assumptions , the calculation will be only slightly more complicated and can easily be performed .\\nwe note in particular that the mirror symmetry assumption is not stringent , for even when this symmetry is largely violated , the @xmath171 term is typically an order of magnitude or more smaller than the @xmath172 term , as we verify by estimating the relevant integrals in eq .\\n( [ eqn : correctionintegrals ] ) .\\nif needed , @xmath171 can be calculated by evaluating these integrals .\\nin addition , we note that the treatment given in this example can readily be generalized to the case where the lattice depth and velocity are changed simultaneously . performing the necessary integrals\\n, we find that :    @xmath184\\\\ ] ]    we note that for @xmath185 , we can solve the problem by employing adiabatic expansion over a single time interval .\\nhowever , there may be times when we must divide the problem into multiple parts , as discussed in appendix d. for typical experimental parameters , the term proportional to @xmath186 in @xmath172 will dominate both the second term in @xmath172 and the @xmath170 term .\\nwe have verified that the @xmath170 term ( which embodies the non - adiabatic loading of the lattice ) is typically much smaller than the second term in @xmath172 by estimating the integrals in eq .\\n( [ eqn : correctionintegrals ] ) that correspond to @xmath170 and by checking these estimates numerically .\\nthus , we can express the condition @xmath185 as :    @xmath187    ) , as calculated using a simplified adiabatic expansion method in which we keep only leading terms versus numerical simulations of the schrodinger equation .\\nthe curve represents the prediction made by the adiabatic expansion method , while the red dots represent the numerical results .\\nthe lattice depth ramps and acceleration time are identical to those of the acceleration sequence shown in fig .\\n[ fig : latticeaccel ] , with an acceleration time of @xmath188 and with @xmath76 .\\nin addition to the leading second order term , we keep the leading fourth order correction term in @xmath189 , @xmath190 , calculated in @xcite , which becomes significant for larger accelerations ( e.g. , for an acceleration of @xmath191 , this term is smaller than the leading second order correction by a factor of @xmath192 ) .\\nwe neglect corrections arising from the term @xmath170 , for these corrections are common to both arms of the interferometer to lowest order .\\neven the simple approximation used to obtain the curve agrees remarkably well with the simulations ( with an rms deviation of @xmath193 radians ) , and we note that we could easily improve this approximation by including more terms in the adiabatic expansion series . as expected\\n, we observe that the correction scales quadratically with acceleration.,width=672 ]    this condition will often hold , since in many experimentally relevant cases the acceleration time or acceleration will be sufficiently small .    we now show how to determine the correction to the phase shift between the two arms arising from the non - adiabatic correction @xmath194 in the case where this correction is small . as we recall from eq .\\n( [ eqn : stateexpansion ] ) , the coefficient of the ground state eigenvector at time @xmath195 is @xmath196 .\\nnow , note that the argument of any complex number @xmath197 can be written as @xmath198 $ ] .\\nthe non - adiabatic correction to the phase of the coefficient of the ground state eigenvector for an arm with acceleration @xmath169 is thus :    @xmath199 \\\\\\\\ \\\\nonumber \\\\\\\\\\n\\\\nonumber   & \\\\approx & \\\\left|g_{\\\\text{ramp}}\\\\right| \\\\sin \\\\left(\\\\arg\\\\left[g_{\\\\text{ramp}}\\\\right]\\\\right ) + \\\\left|g_{\\\\text{accel}}\\\\right| \\\\sin \\\\left(\\\\arg\\\\left[g_{\\\\text{accel}}\\\\right]\\\\right ) \\\\\\\\\\\\end{aligned}\\\\ ] ]    where we have taylor expanded to first order in small quantities . to calculate the correction to the phase shift between two arms , we take the difference @xmath200 . the @xmath201\\\\right )\\n$ ] term is common to both arms .\\nthe non - adiabatic correction to the phase difference between an arm with acceleration @xmath169 and an unaccelerated arm is :    @xmath202\\\\right)\\\\ ] ]    eqs .\\n( [ eqn : phasecorrection ] ) and ( [ eqn : phasecorrectionref ] ) provide us with a means to calculate the leading correction to the phase shift , and comparison with numerical results for a variety of experimentally conceivable lattice depth ramps shows excellent agreement .\\na comparison of the adiabatic expansion method with numerical calculations is illustrated in fig .\\n[ fig : correction ] . in particular , fig .\\n[ fig : correction ] shows @xmath203 as a function of @xmath169 for an acceleration time of @xmath188 with @xmath76 .\\nthe numerical values shown in the figure come from a numerical simulation of the schrodinger equation .\\nthe non - adiabatic correction to the phase shift between the two arms of the interferometer for arbitrary arm acceleration differences is @xmath204 .\\nnote that the leading order results depicted in fig .\\n[ fig : correction ] will often be sufficient , but it may sometimes be necessary to calculate higher order corrections , examples of which can be found in @xcite .    in an experimental implementation ,\\nthe two arms of the beam splitter are addressed by two different lattices .\\ntherefore , any imbalance in the depths of the two lattices will lead to a phase error between the arms .\\nwhere the intensities of the two beams forming a given lattice are @xmath205 and @xmath206 , we note that the lattice depth is proportional to the product @xmath207 , meaning that intensity imbalances lead to lattice depth imbalances .\\nwe calculate that once a lattice is ramped up , the @xmath103 term in the expression for the lowest eigenvalue of the dressed state hamiltonian from eq .\\n( [ eqn : evalue ] ) is much larger than the @xmath104 term .\\nthus , for an interaction lasting from time @xmath208 to time @xmath209 , the dominant contribution to the phase error , which we denote as @xmath210 , arising from the imbalance in the lattice depths is :    @xmath211 d t\\\\ ] ]    where we note that @xmath103 can be calculated using the truncated matrix approximation discussed in sec .\\niv . in order to put eq .\\n( [ eqn : balance ] ) in a more convenient form for making order of magnitude estimates , we use the fact that for @xmath212 , @xmath213 .\\nthis result is verified by direct comparison with the values of @xmath103 obtained with the truncated matrix approximation . recalling that @xmath214 , it is convenient to rephrase this statement in terms of @xmath215 as follows : for @xmath216 , @xmath217 .\\nthis range of @xmath218 is of considerable experimental interest and is amenable to simple approximation .\\nhowever , if needed , smaller lattice depths can be treated using the general relation given in eq .\\n( [ eqn : balance ] ) and the truncated matrix approximation . substituting the above result into eq .\\n( [ eqn : balance ] ) , we obtain :    @xmath219    where @xmath220 and @xmath221 respectively denote the average values of @xmath222 and @xmath223 between time @xmath208 and time @xmath209 .\\nnote that fluctuations in the difference between @xmath222 and @xmath224 that occur at frequencies that are large with respect to the beam splitter time @xmath225 will largely average out , suppressing their net effect on the phase shift .\\nfurthermore , in many geometries , arm 1 will be addressed by one pair of laser beams , that we call lattice a , during the splitting of the arms and then will be addressed by a second pair of laser beams , that we call lattice b , during the recombination of the arms . conversely , arm 2 will be addressed by lattice b during the splitting stage and by lattice a during the recombination stage .\\nin such a geometry , the effect on the phase shift of a constant offset in depth between lattice a and lattice b will cancel , and the effect on the phase shift of fluctuations in the depth difference between lattice a and lattice b that occur at low frequencies with respect to the time scale of the interferometer sequence will be highly suppressed .\\nalso , as we discuss in the following section , we will often be interested in the difference in the phase shifts of two interferometers in a differential configuration .\\nif both of the interferometers remain well within the rayleigh ranges of the laser beams so that beam divergence is a small effect , any lattice depth imbalance will be largely common to the two interferometers , suppressing its effect on the phase shift difference by orders of magnitude .\\nlight - pulse atom interferometer geometries have had tremendous success in performing many types of high - precision measurements . however , in many cases\\n, we would like to be able to push the capabilities of atom interferometry by making more precise measurements using spatially compact interferometers .\\natom interferometers that use optical lattices as waveguides for the atoms offer the potential to make such measurements attainable .\\nin such a scheme , we can use an initial beam splitter composed of multiple bragg pulses , a multi - photon bragg pulse , or a hybrid bragg pulse / lattice acceleration scheme as described in @xcite to split the arms of the interferometer in momentum space .\\nwe can then control each arm independently with an optical lattice .\\nwe will once again use bragg pulses during the @xmath226-pulse and final @xmath227-pulse stages of the interferometer sequence , with lattices acting as waveguides between these stages .\\nthe preceding analysis has developed the theoretical machinery for calculating phase shifts for these lattice interferometers .\\nwe now examine several of the most promising applications of lattice interferometers .\\nlattice interferometers can be used to make extremely precise measurements of the local gravitational acceleration @xmath9 .\\nwe proceed to calculate the phase shift for a lattice gravimeter .\\nit is essential to note that whenever the two arms are addressed by different lattices they will be in different dressed state frames ( where we recall that a dressed state frame is defined by the velocity of the corresponding lattice and by the distance that the lattice has traveled since the beginning of the interferometer sequence ) .\\nlet the velocities in the lab frame of the two lattices be denoted as @xmath228 and @xmath229 , respectively .\\nthe lattice velocities for the two arms in their respective dressed state frames will thus be @xmath230 and @xmath231 .\\nwe note that @xmath31 is the velocity of the atom before the initial bragg diffraction that splits the arms in momentum space , so that the two arms have different momenta after the bragg diffraction . during the lattice loading period\\n, the two lattices must be resonant ( as described in sec .\\niii and in appendix c ) with the respective portions of the atomic wavefunction that they are addressing .\\nalso , let @xmath232 be the velocity difference between the two arms and @xmath233 be the distance between the two arms . where the interferometer sequence lasts for a time @xmath106 , we can derive the phase shift for a lattice gravimeter using eq .\\n( [ eqn : feynman ] ) .\\nwe note that an additional contribution to the phase shift will arise if the two arms of the interferometer end up in slightly different dressed state frames .\\nthe arms begin in the same dressed state frame , and if @xmath234 they will end up in the same dressed state frame , in which case eq .\\n( [ eqn : feynman ] ) provides the final say on the phase shift .\\nif @xmath235 differs slightly from zero , the two arms will end up in slightly different dressed state frames . since we must compare phases in a common frame , it is convenient to boost both arms into the freely falling frame using the transformation given in eq .\\n( [ eqn : dstransform ] ) and the mathematical framework discussed in appendix b. in addition to the phase difference from eq .\\n( [ eqn : feynman ] ) , we will then have an additional term in the phase shift equal to @xmath236 , where @xmath237 is the lab frame momentum at time @xmath106 of a particular momentum eigenstate in the atomic wavepacket .\\nafter averaging , it follows from the discussion in @xcite that @xmath237 will take on the value of the center of the momentum space wavepacket .\\nwe can combine the contribution from eq .\\n( [ eqn : feynman ] ) and the contribution from boosting the two arms to a common frame to calculate the phase shift , where we also include a term @xmath238 to embody the net contribution to the phase shift arising from the bragg pulses :    @xmath239 -\\\\frac{1}{\\\\hbar } m(v_f - v_0 + gt ) \\\\delta d(t ) + \\\\phi_{\\\\text{bragg}}\\\\\\\\ \\\\nonumber \\\\\\\\ \\\\nonumber & = & \\\\frac{1}{\\\\hbar}\\\\left [ \\\\int_{0}^{t } \\\\frac{1}{2}m(v_{\\\\text{lab}}^{\\\\text{arm1}}(t)^2-v_{\\\\text{lab}}^{\\\\text{arm2}}(t)^2)d t+\\\\int_{0}^{t}mg \\\\delta v(t ) t dt - \\\\int_{0}^{t}m v_0 \\\\delta v(t ) dt \\\\right ] -\\\\frac{1}{\\\\hbar } m(v_f - v_0 + gt ) \\\\delta d(t ) + \\\\phi_{\\\\text{bragg}}\\\\\\\\\\\\end{aligned}\\\\ ] ]    integrating the second term by parts and simplifying yields :    @xmath240    when expressed in terms of lab frame quantities , it is apparent that @xmath241 contains terms corresponding to the propagation phase and the separation phase that typically appear in standard atom interferometer phase shift calculations .\\nwe note that in the dressed state frame , ideal bragg pulses simply yield contributions in units of @xmath242 to the overall phase shift between the arms , and these contributions can easily be made to cancel so that @xmath243\\n. however , we note that in some cases , corrections to the simplified picture of an ideal bragg pulse due to such factors as gravity gradients , finite pulse and detuning effects ( which can sometimes lead to a non - negligible propagation phase during the bragg pulse ) , phase noise , or population loss may need to be considered . to avoid unnecessarily complicating our presentation , we will not present these corrections here .\\ninstead , we emphasize that they are well - understood effects and refer the reader to other sources for further discussion @xcite .\\nmoreover , we note that these effects gain additional suppression for interferometers in a differential configuration so that they will often be below the mrad level @xcite , as in the case of the gravity gradiometer discussed below .    in the symmetric case where the velocities of the two arms in the lab frame are either opposite to each other or equal so that @xmath244 .\\nsince we need the two arms of the interferometer to overlap at time @xmath106 , it is convenient for us to choose @xmath234 .\\nthus , the first term in eq .\\n( [ eqn : phaseshift ] ) will constitute the only contribution to @xmath241 .\\nhowever , in an experiment , the parameters in eq .\\n( [ eqn : phaseshift ] ) will undergo small fluctuations around their desired values from shot to shot , so that the other terms in eq .\\n( [ eqn : phaseshift ] ) act as a source of noise .\\nin order to cancel the effects of this noise , we can adopt a gradiometer setup in which an array of two or more gravimeters interacts with the same lattice beams .\\nalthough fluctuations in @xmath245 will still affect phase differences between gravimeters , which take the form @xmath246 , modern phase lock techniques will typically allow us to control the phase differences between the lattice beams well enough so that these effects are smaller than shot noise @xcite .\\nwhen measuring a gravity gradient , the value of @xmath9 will vary due to the gradient over the range of a single gravimeter . for linear gradients\\n, we can calculate the phase shift in the presence of a gravity gradient by assuming that the value of @xmath9 corresponding to the gravimeter is equal to its value at the center of mass position of the atom ( see @xcite for a rigorous justification of this procedure ) .\\nwhen higher order derivatives of the gravitational field become sizable in comparison to the first derivative , this simple prescription may not suffice , and we can treat the problem perturbatively . if we want to measure an acceleration as well as a gravitational gradient in a noisy environment in which the fringes of the individual gravimeters are washed out , we can use dissimilar conjugate interferometers whose phase noise is strongly correlated as suggested in @xcite .\\nappropriate statistical methods can then be used to extract the desired signal @xcite .\\nthe effects on the phase shift of non - adiabatic corrections , lattice depth imbalances , and the finite spread of the atomic wavefunction in momentum space are considered in sec .\\niv . , sec .\\nv. , and appendix b. based on the analysis in these sections , we conclude that a wide range of experimentally feasible gravimeter geometries exist that contain sufficiently adiabatic lattice depth and velocity ramps and that make use of symmetry in such a way that the net contribution of these corrections to the phase shift will be below the mrad level in a gradiometer configuration .\\nwe note that this paper has developed the mathematical machinery to calculate any such corrections to arbitrary precision if necessary .\\nwhen two lattices are used to manipulate the arms of an atom interferometer , one lattice will be on resonance with a given arm , while the other will be highly detuned .\\nas long as we keep this detuning large enough and/or employ geometries with sufficient symmetry between the arms , the net effect of the off - resonant lattices on the final phase shift can often be made to be smaller than the mrad level in a differential configuration ( e.g. , a gradiometer ) .\\nfor arm 1 , the detuned lattice will manifest as an additional term in the discrete hamiltonian , given by :    @xmath247    where @xmath248 4 \\\\omega_rdt^{\\\\prime}=-2 k \\\\delta d(t)$ ] and where @xmath249 is the lattice depth parameter corresponding to the detuned lattice that addresses arm 2 @xcite . to obtain the correction to the hamiltonian for arm 2 , which comes from the detuned lattice addressing arm 1\\n, we replace @xmath250 with @xmath251 and @xmath249 with @xmath252 . for large detunings\\n, @xmath250 will vary rapidly with time so that the contribution from the detuned hamiltonian will be small due to the rotating wave approximation .\\ncorrections arising from the detuned hamiltonian can be solved for perturbatively using methods such as adiabatic perturbation theory .\\nbut we emphasize again that we can often avoid situations where this will be necessary .\\nfor example , we find from perturbation theory that to avoid population loss due to the off - resonant lattice as described in sec .\\n, we should choose the off - resonant lattice to have a velocity that differs from that of the particular arm of the interferometer under consideration by an amount @xmath253 ( where @xmath89 is the depth parameter of the off - resonant lattice ) .\\nwe have verified this result with numerical simulations . however , in this regime , the off - resonant lattice can still sometimes cause a non - negligible energy shift , which we estimate with perturbation theory .\\nthe energy shift for arm 1 is @xmath254 .\\nanalagously , the energy shift for arm 2 is @xmath255 .\\nthe relevant quantity in determining the correction to the phase difference between the arms is the difference in energy shifts @xmath256 , which is determined by the lattice depth imbalance between the arms . where we let @xmath257 and @xmath258 :    @xmath259    this result agrees with numerical simulations .\\n[ fig : shift ] illustrates the dependence of @xmath260 on @xmath261 . for the same reasoning as in the discussion in the previous section , the net effect of lattice depth imbalances on the correction term treated here\\nwill often be further suppressed by orders of magnitude for interferometers in a differential configuration .\\nthus , as stated previously , in many cases the net effect of phase corrections due to the off - resonant lattices will be below the mrad level in a differential configuration .     on @xmath261 as given in eq .\\n[ eqn : offresshift ] , which quantifies the effect of the off - resonant lattices .\\nthis plot takes the lattice depth for arm 1 to be @xmath257 and the lattice depth for arm 2 to be @xmath258 , where @xmath262 and @xmath263 .\\nthe recoil frequency is taken to be @xmath264.,width=720 ]    a lattice gravimeter can provide extraordinary levels of sensitivity .\\nthis sensitivity can be achieved over small distance scales by implementing a hold sequence in which the two arms are separated , manipulated into the same momentum eigenstate , held in place by a single lattice , and then recombined . in achieving compactness , we note that the fact that lattice interferometers are confined and can thus keep the atoms from falling under gravity during the separation and recombination stages of the interferometer as well as during the hold sequence is essential .\\notherwise , for many configurations , the desired arm separation could not be reached without the atoms falling too great a distance , which would ruin the compactness of the interferometer .\\nhold times will be limited by spontaneous emission , which decreases contrast .\\nmodern laser technology will allow us to use detunings of hundreds or even thousands of ghz , making hold times on the order of 10 s within reach @xcite .\\ngravimeter sensitivities using the hold method greatly exceed the sensitivities of light - pulse gravimeters while simultaneously allowing for a significantly smaller interrogation region .\\nfor example , for @xmath265 atoms / shot and @xmath266 shots / s , a shot noise limited conventional light - pulse interferometer with a 10 m interrogation region can achieve a sensitivity of @xmath267 @xmath9/hz@xmath10 . with similar experimental parameters ,\\na shot noise limited lattice interferometer with a 10 s hold time and an interrogation region of 1 cm will have a sensitivity of @xmath268 @xmath9/hz@xmath10 .\\nif we expand the interrogation region to 1 m , we obtain a sensitivity of @xmath269 @xmath9/hz@xmath10 .\\nthis remarkable sensitivity has a plethora of potential applications .\\nextremely precise gravimeters and gravity gradiometers can be constructed to perform tests of general relativity , make measurements relevant to geophysical studies , and build highly compact inertial sensors . moreover\\n, the fact that lattice interferometers can operate with such high sensitivities over small distance scales makes them prime candidates for exploring short distance gravity .\\none could set up an array of lattice gravimeters to precisely map out gravitational fields over small spatial regions , as shown in fig .\\n[ fig : array ] .\\nthe knowledge obtained about the local gravitational field could be useful in searching for extra dimensions @xcite as well as in studying the composition and structure of materials .    ,\\n@xmath208 , @xmath209 , and @xmath270 .\\nsuch an array could be used to study general relativistic effects , search for extra dimensions , examine local mass distributions , or measure newton s constant .\\nin addition , the ability of lattice interferometers with hold sequences to provide extremely precise measurements with a small interrogation region makes them ideal candidates for compact , mobile sensors.,width=288 ]      ultra - high precision gravitational measurements are certainly among the most promising applications of lattice interferometers , but the usefulness of lattice interferometers is certainly not limited to the study of gravity . by exposing the two arms of a lattice interferometer to different electrostatic potentials , tests of atom charge neutrality with unprecedented accuracy could be achieved @xcite .\\nthe main advantage of a lattice interferometer in such a measurement is that the interrogation time can be significantly increased in comparison to the interrogation time achievable in a light - pulse geometry through the use of a hold sequence . where @xmath271 is the duration of the hold\\n, @xmath272 is the electrostatic potential difference between the two arms of the interferometer , @xmath273 is the electron charge , and @xmath274 is the ratio of the atomic charge to the electron charge , the phase shift is given by @xmath275 @xcite .\\nbased on the results of sec .\\niv . , sec .\\nv. , and appendix b , and assuming an identical configuration to that described in @xcite except for the inclusion of a hold sequence , we estimate that the phase error induced by the undesirable effects we consider will be below the proposed shot noise limit for this experiment ( 1 mrad ) .\\nany systematic phase error can be characterized by the methods we have developed . for a hold time of 10 s and for integration over @xmath276 shots , atom charges can be probed down to the region of @xmath277 .\\nthe ratio @xmath278 is of particular interest because of its direct relation to the fine structure constant .\\natom interferometry has previously been used to provide exquisite measurements of @xmath278 .\\nthe most precise atom interferometric measurement to date was performed by cadoret _\\n_ , who performed an elegant experiment combining bloch oscillations and a ramsey - bord interferometer to measure the fine structure constant to within a relative uncertainty of @xmath279 @xcite .\\n( [ eqn : phaseshift ] ) indicates that if we apply different accelerations to the two arms of the interferometer , we will see a phase shift proportional to @xmath280 that depends on the kinetic energy difference between the arms , which can be made extremely large . a differential configuration using conjugate interferometers ( shown in fig .\\n[ fig : conj ] ) could reduce the net contribution of such unwanted effects as laser phase noise and cancel the gravitational phase shift up to gradients @xcite .\\nsuch a geometry could provide an extremely precise measurement of @xmath278 , as illustrated by the fact that we can achieve a phase shift of @xmath281 radians for a 5 m interrogation region and a 0.6 s interrogation time , corresponding to a shot noise limited sensitivity of @xmath282/hz@xmath10 for the experimental parameters stated above .\\nin this situation , the dominant unwanted effect would arise from non - adiabatic corrections , and the methods for calculating these corrections that are presented in the previous sections would need to be applied ( we estimate a phase error @xmath283 10 rad ) .\\nwe note that even if the shot noise limit is not reached , the technique we have proposed could still improve over current @xmath278 measurements .\\nwe emphasize again that to take full advantage of the sensitivity offered by lattice manipulations in atom interferometry , the methods we develop in this paper for calculating non - adiabatic corrections are absolutely essential .    .\\nthe trajectories shown hold in the lab frame .\\nbragg pulses are used at times @xmath284 , @xmath208 , and @xmath209 ( at @xmath284 , a sequence of multiple bragg pulses is necessary to split the system into the two arms of the two conjugate interferometers ) .\\nthe phase shift of the lower interferometer is subtracted from the phase shift of the upper interferometer , which suppresses the effects of laser phase noise and eliminates the gravitational phase shift up to gradients @xcite . with such a scheme ,\\na measurement of @xmath278 to a part in @xmath285 may be possible , which could lead to the most accurate determination of the fine structure constant to date.,width=288 ]    the fact that all terms in eq .\\n( [ eqn : phaseshift ] ) are proportional to @xmath56 ( except for the @xmath238 term , which as we have explained , will often be negligible ) can be exploited to provide high - precision measurements of isotope mass ratios by using an interferometer geometry in which the two isotopes follow identical trajectories .\\nisotope mass ratios could be relevant to studies of advanced models of the structure of the nucleus @xcite .\\nconversely , if we know the mass ratio of two isotopes sufficiently well , we can use such a geometry to precisely measure accelerations , where the two isotopes provide two dissimilar conjugate interferometers .\\nthe phase noise of these two interferometers will be extremely well correlated because they are topologically identical .\\nthis also eliminates the need for the additional lattice beams required to form the second , topologically distinct interferometer that would be needed if we only had a single isotope .\\nnote that this scheme to construct dissimilar , topologically identical conjugate accelerometers would not be possible for a light - pulse geometry , for the leading order phase shift of light - pulse accelerometers is independent of isotope mass .\\nlattice interferometers can also be used to build compact and highly sensitive gyroscopes .\\nthere are multiple possible schemes in which optical lattices can enhance gyroscope sensitivity .\\none such scheme is to modify a typical atom - based gyroscope by replacing the raman pulses with lmt lattice beam splitters , increasing the enclosed area of the interferometer and hence its sensitivity to rotations .\\nthe phase shift can be written in sagnac form as @xmath286 , where @xmath287 is the rotation rate vector and @xmath288 is the normal vector corresponding to the enclosed area of the interferometer @xcite .\\nthe gyroscope described in @xcite achieves a sensitivity of @xmath289/hz@xmath10 .\\nreplacing the raman pulses in this experiment with @xmath290 lattice beam splitters would increase the sensitivity by a factor of 100 .\\nin such a configuration , we estimate that each beam splitter could introduce a non - adiabatic phase error of @xmath283 1 rad if the arms of the interferometer are not split symmetrically .\\nhowever , beam splitter configurations that exploit symmetry between the arms can reduce this effect by orders of magnitude .\\nanother option is to use optical lattices along multiple axes to provide complete control of the motion of the atoms in two or three dimensions ( this control is only achieved in the region in which the lattices overlap , necessitating the use of wide beams ) .\\nanalagous to a fiber - optic gyroscope , the atoms could be guided in repeated loop patterns , with the two arms rotating in opposite directions .\\ngeometries in which atomic motion is controlled in multiple dimensions could also expand the possibilities for other applications of lattice interferometry ( such as measurements of gravity ) by allowing for the measurement of potential energy differences between arbitrary paths .\\nfor instance , a compact array of three orthogonal lattice gravity gradiometers could be used to measure the nonzero divergence of the gravitational field in free space predicted by general relativity @xcite .\\nwe have presented a detailed analytical description of the interaction between an atom and an optical lattice , using the adiabatic approximation as a starting point and then proceeding to rigorously develop a method to calculate arbitrarily small corrections to this approximation using perturbative adiabatic expansion .\\nwe have applied this theoretical framework to calculate the phase accumulated during a lattice acceleration in an lmt beam splitter .\\nand we have proposed atom interferometer geometries that use optical lattices as waveguides and discussed applications of such geometries , using our theoretical methods to add rigor to this discussion .\\nwe are working toward the experimental implementation of lattice interferometers and lmt lattice beam splitters , and we hope to explore the applications we have discussed . in this experimental work , we realize that we will have to contend with a number of unwanted systematic effects , such as spatially varying magnetic fields , imperfections in the lattice beam wavefronts , and inhomogeneity of the lattice depth across the atomic cloud .\\nwe have studied these effects using both analytical and numerical methods , and we are optimistic that they can be significantly mitigated for a wide range of experimental parameters  a conclusion that we hope to verify experimentally . many of the unwanted systematic effects that are relevant to lattice interferometers are also shared by light - pulse interferometers and can therefore be dealt with using similar methods .\\ntherefore , we believe that it will likely be possible to realize lattice interferometers in existing apparatuses originally constructed with light - pulse geometries in mind .\\nwe would like to thank philippe bouyer , sheng - wey chiow , gerald gabrielse , and holger mueller for valuable discussions .\\ntk acknowledges support from a fannie and john hertz foundation fellowship and an nsf fellowship .\\ntk and dj acknowledge support from a stanford graduate fellowship .\\nfor the purposes of this paper , we consider unitary transformations that consist of a translation in position space , a boost in momentum space , and a time - dependent change of phase .\\nsuch a transformation has the general form :    @xmath291    we note we are free to choose the boost parameters @xmath292 and @xmath293 arbitrarily .\\nthat is , we do not have to choose them so that @xmath293 is the rate of change of @xmath292 .\\nthe operators @xmath294 and @xmath295 transform under @xmath296 as follows :    @xmath297    we now consider the hamiltonian in a frame that is freely falling with gravity , which takes the form :    @xmath298\\\\ ] ]    we can transform from the freely falling frame to the lab frame by applying the appropriate galilean transformation @xmath299 , which corresponds to specifying @xmath300 , @xmath301 , and @xmath302 , so that @xmath303 .\\nsince the only position dependence in @xmath304 comes from the lattice potential , we could readily approach the problem of finding the dynamics of the system by working in the freely falling frame .\\nhowever , it will prove to be useful from a calculational standpoint to transform to a third frame , with a hamiltonian resembling that describing the atom - light interaction from the point of view of dressed states .\\nwe note that although we could have performed a boost directly from the lab frame to the dressed state frame , it is useful to introduce the freely falling frame for pedagogical reasons , since it is the frame in which calculations for atom interferometry are typically performed . in appendix b , we use the transformation between the freely falling frame and the dressed state frame to highlight the parallels between a lattice beam splitter and a typical light pulse beam splitter .\\nwe absorb the initial velocity @xmath31 of the atom in the lab frame ( and hence also in the freely falling frame ) into the dressed state frame , so that velocity @xmath31 in the lab frame corresponds to velocity zero in the dressed state frame .\\nthe hamiltonian in the dressed state frame is :    @xmath32    the unitary transformation @xmath305 that transforms from the dressed state frame to the freely falling frame , so that @xmath306 , corresponds to boost parameters @xmath307 and @xmath308 with a time - dependent phase factor :    @xmath309    the ability to boost to a frame in which the hamiltonian contains no position dependent terms outside of the lattice potential is contingent upon the assumption that the external potential in the lab frame ( not including the lattice potential ) is linear in @xmath26 .\\nhowever , real - world potentials such as the potential corresponding to earth s gravitational field will deviate somewhat from this assumption .\\nany such deviations would manifest as residual position dependent terms in the dressed state hamiltonian , which we collectively refer to as @xmath73 . under the semiclassical approximation ,\\nwe neglect the effects of @xmath73 on the time evolution of the atomic wavepacket .\\nthis approximation is valid when the energy scale of @xmath73 over the spread of the atom s wavefunction ( which is on the order of magnitude of the expectation value of @xmath73 in the atomic wavepacket ) is much smaller than the energy scale of the lattice potential and is small relative to the time scale of the experiment , which is the case for a wide class of experimental parameters .\\nfor example , in the case of a rubidium atom wavepacket with a spatial spread of @xmath310 m in the gravitational field at the earth s surface ( which has a gradient of @xmath311 ) , the energy scale of @xmath73 will be @xmath312 @xmath313 ( @xmath314 hz ) .\\nthis energy scale is smaller than that of a lattice of typical experimental depth ( @xmath283 @xmath315 , where @xmath316 is the recoil energy @xmath317 by a factor of @xmath318 and is small on a time scale of @xmath192 s. the effects of linear gradients and of more general potentials can be accounted for through a straightforward generalization of the results presented in this paper , as discussed in greater depth in @xcite .\\nin sec . ii . , we discretized the hamiltonian using the basis of momentum states @xmath40 for integer @xmath41 .\\nwe now generalize our results to the case of a finite wavepacket .    throughout our analysis\\n, we have worked mainly in the dressed state frame , since this frame is particularly convenient for describing phase evolution in a lattice . for interferometers where we use lattices as waveguides for the atoms\\n, we will want to perform the entire phase shift calculation in this frame .\\nhowever , phase shift calculations for light - pulse atom interferometers are often performed in the freely falling frame .\\nthus , for applications where lattice manipulations are used for beam splitters and mirrors in a light - pulse geometry , it is useful to convert the phase evolution we calculate in the dressed state frame to the freely falling frame .\\nwe thus present the general results derived in this appendix in the freely falling frame .    in sec .\\n, we considered a particular dressed frame in which the initial velocity of the atom is boosted to zero .\\nwe now introduce an unboosted dressed state frame that is related to the freely falling frame by a translation in position space with no boost in momentum space , so that the unitary transformation @xmath319 transforms from the unboosted dressed state frame to the freely falling frame .\\nthe hamiltonian in this frame is :    @xmath320    now , say that before the lattice acceleration , the state that we are accelerating is described by @xmath321 in the freely falling frame .\\nwe can then transform this state vector to the unboosted dressed state frame , describe its evolution to the final time @xmath322 in this frame , and transform back to the freely falling frame .\\nwhere @xmath323 is the time evolution operator the takes us from time @xmath109 to time @xmath324 in the unboosted dressed state frame , we can write :    @xmath325    denoting the initial momentum space wavefunction in the freely falling frame as @xmath326 , we can express eq .\\n( [ eqn : evolution ] ) as :    @xmath327    where we have used the linearity of the operators to bring them inside the integral .\\nwe now consider how each momentum eigenstate @xmath35 evolves in the unboosted dressed state frame .\\nthat is , we must calculate @xmath328 for each @xmath329 . in order to do so\\n, we introduce a class of boosted dressed state frames @xmath330 parameterized by @xmath329 , so that momentum @xmath329 in the unboosted dressed state frame ( which is just the frame @xmath331 ) corresponds to momentum zero in the frame @xmath330 .\\nin essence , where @xmath332 , frame @xmath330 travels with velocity @xmath333 with respect to the unboosted dressed state frame . in frame\\n@xmath330 , the hamiltonian takes the form :    @xmath334    where we note that the unitary transformation that transforms from frame @xmath330 to the unboosted dressed state frame is :    @xmath335}\\\\ ] ]    where @xmath336 is the time evolution operator in frame @xmath330 , we can write :    @xmath337 } \\\\hat{u}_p(t_f ) \\\\hat{t}_{\\\\text{ds}_p}(t_f , t_0 ) \\\\left | 0 \\\\right > \\\\\\\\\\\\end{aligned}\\\\ ] ]    the problem of determining the evolution of the momentum eigenstate @xmath35 in the unboosted dressed state frame thus reduces to evolving the momentum eigenstate @xmath61 in the frame @xmath330 , which we know how to do from the preceding sections .\\nwe assume that the momentum space wavefunction is narrow enough so that the momentum eigenstates we consider are resonant with the lattice ( that is , as we recall from sec .\\niii . , the magnitudes of their velocities with respect to the lattice are less than @xmath48 ) . where the lattice acceleration is chosen so as to transfer a momentum of @xmath338 to the atom , the result from sec .\\niii . tells us that the time evolution in eq .\\n( [ eqn : boostedds ] ) yields :    @xmath339    for phase :    @xmath340    where @xmath341 is the lattice velocity in frame @xmath330 . for the sake of pedagogy , at the moment we neglect the phase arising from the lattice depth , the phase arising from the small periodic variations in the lowest eigenvalue , and the phase arising from non - adiabatic corrections , where we note that we could calculate these contributions if needed . evaluating the integral in eq .\\n( [ eqn : velocityphase ] ) , we obtain :    @xmath342v+ \\\\frac{m}{2 \\\\hbar}v^2(t_f - t_0)\\\\ ] ]    substituting this result into eq .\\n( [ eqn : boostedds ] ) , applying the transformation @xmath343 , and canceling terms in the exponentials , we find that :    @xmath344    we can now evaluate eq .\\n( [ eqn : freelyfallingevolution ] ) , which gives us the final result :    @xmath345    where :    @xmath346    for @xmath347 .\\nobserve that the second term in eq .\\n( [ eqn : pulsephase ] ) is what would typically be called the laser phase in a light - pulse atom interferometer for an @xmath41-photon beam splitter .\\nwe note that as long as the resonance condition is met for the momentum eigenstates we are considering so that they are accelerated , the phase evolved during a lattice acceleration in the @xmath348 dressed state frame , is independent of @xmath329 , as shown in eq .\\n( [ eqn : ds0phase ] ) .\\n( this is true up to non - adiabatic corrections and the small periodic variations in the ground state eigenvalue ) .\\nthis makes dressed state frames particularly convenient for performing calculations involving wavepackets . in contrast , in the freely falling frame , the accumulated phase @xmath349 is dependent on @xmath329 , but this dependence cancels in the final expression for the phase shift between the two arms of an interferometer as long as the distance travelled by the atom while locked into a lattice is the same for both arms .\\nnote that momentum dependent contributions to the total phase shift must arise when we treat the problem purely in terms of dressed states , since the total phase shift is an observable quantity and must therefore be independent of the frame in which it is calculated .\\nthe key point to realize is that if the total distance traveled in the lattice is not the same for both arms , then the two arms will end up in two different dressed state frames .\\nwe now consider the effect on phase evolution of the periodic term @xmath104 in the expression for the ground state eigenvalue of the dressed state hamiltonian given in eq .\\n( [ eqn : evalue ] ) .\\nthis term leads to a momentum dependent correction @xmath350 to the evolved phase @xmath351 described in eq .\\n( [ eqn : velocityphase ] ) , where :    @xmath352    we note that @xmath353 can be calculated using the truncated matrix approximation described in sec .\\nsince @xmath353 is periodic in @xmath354 with period @xmath355 , the contribution @xmath356 of this correction term to the phase difference between the two arms of an interferometer will be highly suppressed if both arms load the atom into the lattice near the center of the zeroth band ( as discussed in sec .\\niii . ) and undergo a nearly integral number of bloch oscillations so that the effects of this periodic variation in the ground state eigenvalue will be largely common to the two arms , as verified by estimation of the integral in eq .\\n( [ eqn : periodicphase ] ) and by numerically solving the schrodinger equation .\\nhere , we derive the resonance condition stated in sec .\\niii . , which says that for a sufficiently slow lattice depth ramp , the atom will be loaded into the ground state of the lattice if the initial velocity of the lattice is within @xmath48 of the initial velocity of the atom .\\nfirst , we examine the eigenvalues of @xmath55 when @xmath357 . for @xmath357 , @xmath55 is diagonal matrix .\\nsince @xmath55 is a matrix defined in terms of the basis of momentum eigenstates @xmath40 for integer\\n@xmath41 , when @xmath55 is diagonal these momentum eigenstates are also the eigenstates of @xmath55 , with corresponding eigenvalues @xmath358 ( which are just the diagonal elements of @xmath55 ) .\\nwe note that the effect of the @xmath359 term in the hamiltonian is to encapsulate the dependence of the eigenvalues and eigenvectors of the hamiltonian on the lattice velocity for any given lattice depth , which can be best conceptually understood from the point of view of the dressed state picture explained in @xcite .\\nthe resonance condition states that if the initial state is @xmath360 , then the initial velocity of the lattice must be within @xmath48 of @xmath361 .\\nthus , the initial value @xmath145 of the dimensionless velocity @xmath79 must be in the range @xmath362 .\\nwe will now show why this condition on @xmath145 implies that the ground state of @xmath55 with @xmath357 is @xmath360 . where we write @xmath363 and let @xmath364 for integer @xmath365 , the eigenvalues of @xmath55 with the lattice depth set to zero are :    @xmath366= 4 e_r \\\\left [ -n_0 ^ 2 + ( \\\\delta n)^2 -n_0 \\\\delta- \\\\delta n \\\\delta \\\\right]\\\\ ] ]    for @xmath367 , @xmath368 will be most negative when @xmath369\\n, so @xmath360 will indeed be the ground state of the hamiltonian when @xmath357 .\\ntherefore , as long as we ramp up the lattice slowly enough and keep the lattice velocity constant while doing so , the adiabatic theorem tells us that the atom will remain in the lattice ground state throughout the ramp up process provided that the ground state never passes through a point of degeneracy , which is indeed the case for fixed @xmath79 and @xmath367 .    the maximum rate at which we can increase the lattice depth while still maintaining the validity of the adiabatic approximation depends on @xmath370 .\\nthis statement follows from the adiabatic condition @xmath60 , where @xmath61 and @xmath62 respectively denote the ground state and the first excited state of the hamiltonian . when the resonance condition holds , note that the first excited state will be @xmath371 .\\nas @xmath370 increases toward 1 , the gap between @xmath372 and @xmath373 ( i.e. the energy gap between the zeroth and first bands ) decreases . to see why this is true\\n, we observe that :    @xmath374    therefore , for the adiabatic approximation to hold , the maximum rate at which we can ramp up the lattice decreases as we increase @xmath370 .\\nthis result is identical to the statement in sec .\\niii . that adiabatic loading is more difficult to achieve near the border of the first brillouin zone .    after ramping up the lattice , we accelerate it until its velocity is within @xmath48 of @xmath375 where @xmath376 is the target momentum eigenstate\\n. the ground state of the hamiltonian when the lattice has finished ramping down will therefore be @xmath376 , and as long as the entire process is adiabatic so that the atom remains in the ground state , the atom will indeed end up in the target state . by analogy to the previous discussion , the maximum rate at which we can adiabatically ramp down the lattice increases as the difference between the final lattice velocity and @xmath375 goes to zero .\\nin order to calculate non - adiabatic corrections at arbitrary order , we define @xmath377 recursively in the natural way based on the notation of sec .\\niv :          where the last term includes @xmath20 sums and @xmath380 .\\nin order for the expansion to be practical from a calculational standpoint , the series must converge quickly enough so that we do not have to find an unreasonable amount of terms .\\nthere are two types of terms that we can often neglect .\\nfirst , where we assume that the system is initially in the ground state , it is often possible to neglect many terms of the form @xmath381 so that we only need to examine a relatively small number of eigenvectors .\\nterms of this form indeed decrease rapidly as @xmath41 increases because the overlap of the @xmath41th eigenstate with the zeroth eigenstate is essentially nonexistent for large enough @xmath41 .\\nsecond , we can often neglect all terms of order greater than some cutoff value\\n. we will be able to do so as long as the time scale over which we are solving the problem is small enough so that integrating a @xmath20th order correction term against a factor of the form @xmath121 yields a @xmath382th order correction term that is much smaller than the correction term of order @xmath20 .\\nnote an important nuance in how we have phrased the above condition  we have mentioned nothing about a hamiltonian that varies slowly in time . the convergence of the perturbative series depends on the time interval of the solution .\\nas long as the hamiltonian does not vary at an infinitely fast rate , we can always work on a small enough time scale so that the series converges rapidly . to solve the problem on time scales for which the series does not converge quickly\\n, we can simply break the problem into multiple parts .\\nthis method provides with us with a means to describe the system for a hamiltonian that changes arbitrarily fast in time .\\nhaving a slowly varying hamiltonian just serves to allow us to solve the problem without dividing it into as many parts ( much of the time we will not have to divide the problem at all , and in sec .\\nv. we derive conditions for when this will be the case ) , thus making the calculation significantly easier .\\nj. m. hogan , d. m. s. johnson , and m. a. kasevich , in _ proceedings of the international school of physics `` enrico fermi '' on atom optics and space physics _ , edited by e. arimondo , w. ertmer , and w. p. schleich ( ios press , amsterdam , 2009 ) , pp .\\n411 - 447 ( arxiv:0806.3261v1 ) .\",\n",
              "  'abstract': ' we provide an analytical description of the dynamics of an atom in an optical lattice using the method of perturbative adiabatic expansion . \\n a precise understanding of the lattice - atom interaction is essential to taking full advantage of the promising applications that optical lattices offer in the field of atom interferometry . \\n one such application is the implementation of large momentum transfer ( lmt ) beam splitters that can potentially provide multiple order of magnitude increases in momentum space separations over current technology . \\n we also propose interferometer geometries where optical lattices are used as waveguides for the atoms throughout the duration of the interferometer sequence . \\n such a technique could simultaneously provide a multiple order of magnitude increase in sensitivity and a multiple order of magnitude decrease in interferometer size for many applications as compared to current state - of - the - art atom interferometers . ',\n",
              "  'section_names': 'introduction\\nthe hamiltonian in different frames\\nphase evolution under the adiabatic approximation\\ncalculating corrections to the adiabatic approximation using the method of perturbative adiabatic expansion\\nan example of perturbative adiabatic expansion: calculating the non-adiabatic correction to the phase shift evolved during a lattice beam splitter\\napplications: atom interferometers using optical lattices as waveguides\\nconclusion\\nacknowledgments\\nboosting between different frames\\ngeneralizing to the case of a finite wavepacket\\nthe resonance condition for an atom to be loaded into the ground state of a lattice\\nperturbative adiabatic expansion at higher orders'}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A partir daqui é desenvolvimento (contém bugs)"
      ],
      "metadata": {
        "id": "CyD_bWnsfTmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importando artigo\n",
        "!pip install tika\n",
        "from tika import parser\n",
        "\n",
        "raw = parser.from_file('/content/abstract.pdf')\n",
        "artigo = raw['content']\n",
        "print(artigo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRbyZi-aPodj",
        "outputId": "6c9d9830-72bc-4efb-c0ae-32c352e76312"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tika in /usr/local/lib/python3.8/dist-packages (1.24)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tika) (57.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->tika) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->tika) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->tika) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->tika) (3.0.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-12-10 13:34:38,755 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to /tmp/tika-server.jar.\n",
            "INFO:tika.tika:Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to /tmp/tika-server.jar.\n",
            "2022-12-10 13:34:39,340 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "INFO:tika.tika:Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "2022-12-10 13:34:39,790 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
            "WARNING:tika.tika:Failed to see startup log message; retrying...\n",
            "2022-12-10 13:34:44,799 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
            "WARNING:tika.tika:Failed to see startup log message; retrying...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Attention Is All You Need\n",
            "\n",
            "Ashish Vaswani∗\n",
            "Google Brain\n",
            "\n",
            "avaswani@google.com\n",
            "\n",
            "Noam Shazeer∗\n",
            "Google Brain\n",
            "\n",
            "noam@google.com\n",
            "\n",
            "Niki Parmar∗\n",
            "Google Research\n",
            "\n",
            "nikip@google.com\n",
            "\n",
            "Jakob Uszkoreit∗\n",
            "Google Research\n",
            "usz@google.com\n",
            "\n",
            "Llion Jones∗\n",
            "Google Research\n",
            "\n",
            "llion@google.com\n",
            "\n",
            "Aidan N. Gomez∗ †\n",
            "University of Toronto\n",
            "\n",
            "aidan@cs.toronto.edu\n",
            "\n",
            "Łukasz Kaiser∗\n",
            "Google Brain\n",
            "\n",
            "lukaszkaiser@google.com\n",
            "\n",
            "Illia Polosukhin∗ ‡\n",
            "illia.polosukhin@gmail.com\n",
            "\n",
            "Abstract\n",
            "\n",
            "The dominant sequence transduction models are based on complex recurrent or\n",
            "convolutional neural networks that include an encoder and a decoder. The best\n",
            "performing models also connect the encoder and decoder through an attention\n",
            "mechanism. We propose a new simple network architecture, the Transformer,\n",
            "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
            "entirely. Experiments on two machine translation tasks show these models to\n",
            "be superior in quality while being more parallelizable and requiring significantly\n",
            "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
            "to-German translation task, improving over the existing best results, including\n",
            "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
            "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
            "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
            "best models from the literature. We show that the Transformer generalizes well to\n",
            "other tasks by applying it successfully to English constituency parsing both with\n",
            "large and limited training data.\n",
            "\n",
            "1 Introduction\n",
            "\n",
            "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
            "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
            "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
            "\n",
            "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
            "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
            "attention and the parameter-free position representation and became the other person involved in nearly every\n",
            "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
            "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
            "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
            "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
            "our research.\n",
            "†Work performed while at Google Brain.\n",
            "‡Work performed while at Google Research.\n",
            "\n",
            "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
            "\n",
            "ar\n",
            "X\n",
            "\n",
            "iv\n",
            ":1\n",
            "\n",
            "70\n",
            "6.\n",
            "\n",
            "03\n",
            "76\n",
            "\n",
            "2v\n",
            "5 \n",
            "\n",
            " [\n",
            "cs\n",
            "\n",
            ".C\n",
            "L\n",
            "\n",
            "] \n",
            " 6\n",
            "\n",
            " D\n",
            "ec\n",
            "\n",
            " 2\n",
            "01\n",
            "\n",
            "7\n",
            "\n",
            "\n",
            "\n",
            "transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\n",
            "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
            "architectures [38, 24, 15].\n",
            "\n",
            "Recurrent models typically factor computation along the symbol positions of the input and output\n",
            "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
            "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
            "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
            "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
            "significant improvements in computational efficiency through factorization tricks [21] and conditional\n",
            "computation [32], while also improving model performance in case of the latter. The fundamental\n",
            "constraint of sequential computation, however, remains.\n",
            "\n",
            "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
            "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
            "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
            "are used in conjunction with a recurrent network.\n",
            "\n",
            "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
            "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
            "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
            "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
            "\n",
            "2 Background\n",
            "\n",
            "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
            "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
            "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
            "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
            "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
            "it more difficult to learn dependencies between distant positions [12]. In the Transformer this is\n",
            "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
            "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
            "described in section 3.2.\n",
            "\n",
            "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
            "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
            "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
            "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
            "\n",
            "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
            "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
            "language modeling tasks [34].\n",
            "\n",
            "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
            "entirely on self-attention to compute representations of its input and output without using sequence-\n",
            "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
            "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
            "\n",
            "3 Model Architecture\n",
            "\n",
            "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
            "Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
            "of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n",
            "sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n",
            "[10], consuming the previously generated symbols as additional input when generating the next.\n",
            "\n",
            "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
            "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
            "respectively.\n",
            "\n",
            "2\n",
            "\n",
            "\n",
            "\n",
            "Figure 1: The Transformer - model architecture.\n",
            "\n",
            "3.1 Encoder and Decoder Stacks\n",
            "\n",
            "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
            "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
            "wise fully connected feed-forward network. We employ a residual connection [11] around each of\n",
            "the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\n",
            "LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\n",
            "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
            "layers, produce outputs of dimension dmodel = 512.\n",
            "\n",
            "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\n",
            "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
            "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
            "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
            "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
            "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
            "predictions for position i can depend only on the known outputs at positions less than i.\n",
            "\n",
            "3.2 Attention\n",
            "\n",
            "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
            "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
            "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
            "query with the corresponding key.\n",
            "\n",
            "3\n",
            "\n",
            "\n",
            "\n",
            "Scaled Dot-Product Attention Multi-Head Attention\n",
            "\n",
            "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
            "attention layers running in parallel.\n",
            "\n",
            "3.2.1 Scaled Dot-Product Attention\n",
            "\n",
            "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
            "queries and keys of dimension dk, and values of dimension dv . We compute the dot products of the\n",
            "query with all keys, divide each by\n",
            "\n",
            "√\n",
            "dk, and apply a softmax function to obtain the weights on the\n",
            "\n",
            "values.\n",
            "\n",
            "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
            "into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\n",
            "the matrix of outputs as:\n",
            "\n",
            "Attention(Q,K, V ) = softmax(\n",
            "QKT\n",
            "√\n",
            "dk\n",
            "\n",
            ")V (1)\n",
            "\n",
            "The two most commonly used attention functions are additive attention [2], and dot-product (multi-\n",
            "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
            "of 1√\n",
            "\n",
            "dk\n",
            ". Additive attention computes the compatibility function using a feed-forward network with\n",
            "\n",
            "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
            "much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
            "matrix multiplication code.\n",
            "\n",
            "While for small values of dk the two mechanisms perform similarly, additive attention outperforms\n",
            "dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\n",
            "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
            "extremely small gradients 4. To counteract this effect, we scale the dot products by 1√\n",
            "\n",
            "dk\n",
            ".\n",
            "\n",
            "3.2.2 Multi-Head Attention\n",
            "\n",
            "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
            "we found it beneficial to linearly project the queries, keys and values h times with different, learned\n",
            "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\n",
            "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
            "output values. These are concatenated and once again projected, resulting in the final values, as\n",
            "depicted in Figure 2.\n",
            "\n",
            "4To illustrate why the dot products get large, assume that the components of q and k are independent random\n",
            "variables with mean 0 and variance 1. Then their dot product, q · k =\n",
            "\n",
            "∑dk\n",
            "i=1\n",
            "\n",
            "qiki, has mean 0 and variance dk.\n",
            "\n",
            "4\n",
            "\n",
            "\n",
            "\n",
            "Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "\n",
            "MultiHead(Q,K, V ) = Concat(head1, ...,headh)W\n",
            "O\n",
            "\n",
            "where headi = Attention(QW\n",
            "Q\n",
            "i ,KW\n",
            "\n",
            "K\n",
            "i , V W\n",
            "\n",
            "V\n",
            "i )\n",
            "\n",
            "Where the projections are parameter matricesWQi ∈ R\n",
            "dmodel×dk ,WKi ∈ R\n",
            "\n",
            "dmodel×dk ,WVi ∈ R\n",
            "dmodel×dv\n",
            "\n",
            "and WO ∈ Rhdv×dmodel .\n",
            "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
            "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n",
            "\n",
            "3.2.3 Applications of Attention in our Model\n",
            "\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "\n",
            "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
            "and the memory keys and values come from the output of the encoder. This allows every\n",
            "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
            "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
            "[38, 2, 9].\n",
            "\n",
            "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
            "and queries come from the same place, in this case, the output of the previous layer in the\n",
            "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
            "encoder.\n",
            "\n",
            "• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
            "all positions in the decoder up to and including that position. We need to prevent leftward\n",
            "information flow in the decoder to preserve the auto-regressive property. We implement this\n",
            "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
            "of the softmax which correspond to illegal connections. See Figure 2.\n",
            "\n",
            "3.3 Position-wise Feed-Forward Networks\n",
            "\n",
            "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
            "connected feed-forward network, which is applied to each position separately and identically. This\n",
            "consists of two linear transformations with a ReLU activation in between.\n",
            "\n",
            "FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\n",
            "\n",
            "While the linear transformations are the same across different positions, they use different parameters\n",
            "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
            "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
            "dff = 2048.\n",
            "\n",
            "3.4 Embeddings and Softmax\n",
            "\n",
            "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
            "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
            "mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
            "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
            "linear transformation, similar to [30]. In the embedding layers, we multiply those weights by\n",
            "\n",
            "√\n",
            "dmodel.\n",
            "\n",
            "3.5 Positional Encoding\n",
            "\n",
            "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
            "order of the sequence, we must inject some information about the relative or absolute position of the\n",
            "\n",
            "5\n",
            "\n",
            "\n",
            "\n",
            "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
            "for different layer types. n is the sequence length, d is the representation dimension, k is the kernel\n",
            "size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
            "\n",
            "Layer Type Complexity per Layer Sequential Maximum Path Length\n",
            "Operations\n",
            "\n",
            "Self-Attention O(n2 · d) O(1) O(1)\n",
            "Recurrent O(n · d2) O(n) O(n)\n",
            "Convolutional O(k · n · d2) O(1) O(logk(n))\n",
            "Self-Attention (restricted) O(r · n · d) O(1) O(n/r)\n",
            "\n",
            "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
            "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
            "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
            "learned and fixed [9].\n",
            "\n",
            "In this work, we use sine and cosine functions of different frequencies:\n",
            "\n",
            "PE(pos,2i) = sin(pos/10000\n",
            "2i/dmodel)\n",
            "\n",
            "PE(pos,2i+1) = cos(pos/10000\n",
            "2i/dmodel)\n",
            "\n",
            "where pos is the position and i is the dimension. That is, each dimension of the positional encoding\n",
            "corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\n",
            "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
            "relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\n",
            "PEpos.\n",
            "\n",
            "We also experimented with using learned positional embeddings [9] instead, and found that the two\n",
            "versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
            "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
            "during training.\n",
            "\n",
            "4 Why Self-Attention\n",
            "\n",
            "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
            "tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
            "(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\n",
            "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
            "consider three desiderata.\n",
            "\n",
            "One is the total computational complexity per layer. Another is the amount of computation that can\n",
            "be parallelized, as measured by the minimum number of sequential operations required.\n",
            "\n",
            "The third is the path length between long-range dependencies in the network. Learning long-range\n",
            "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
            "ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
            "traverse in the network. The shorter these paths between any combination of positions in the input\n",
            "and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\n",
            "the maximum path length between any two input and output positions in networks composed of the\n",
            "different layer types.\n",
            "\n",
            "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
            "executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\n",
            "computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
            "length n is smaller than the representation dimensionality d, which is most often the case with\n",
            "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
            "[38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
            "very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
            "\n",
            "6\n",
            "\n",
            "\n",
            "\n",
            "the input sequence centered around the respective output position. This would increase the maximum\n",
            "path length to O(n/r). We plan to investigate this approach further in future work.\n",
            "\n",
            "A single convolutional layer with kernel width k < n does not connect all pairs of input and output\n",
            "positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\n",
            "or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\n",
            "between any two positions in the network. Convolutional layers are generally more expensive than\n",
            "recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\n",
            "considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\n",
            "convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\n",
            "the approach we take in our model.\n",
            "\n",
            "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\n",
            "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
            "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
            "and semantic structure of the sentences.\n",
            "\n",
            "5 Training\n",
            "\n",
            "This section describes the training regime for our models.\n",
            "\n",
            "5.1 Training Data and Batching\n",
            "\n",
            "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
            "sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\n",
            "target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n",
            "2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\n",
            "vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\n",
            "batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\n",
            "target tokens.\n",
            "\n",
            "5.2 Hardware and Schedule\n",
            "\n",
            "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\n",
            "the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\n",
            "trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\n",
            "bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
            "(3.5 days).\n",
            "\n",
            "5.3 Optimizer\n",
            "\n",
            "We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and � = 10−9. We varied the learning\n",
            "rate over the course of training, according to the formula:\n",
            "\n",
            "lrate = d−0.5model ·min(step_num\n",
            "−0.5, step_num · warmup_steps−1.5) (3)\n",
            "\n",
            "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\n",
            "and decreasing it thereafter proportionally to the inverse square root of the step number. We used\n",
            "warmup_steps = 4000.\n",
            "\n",
            "5.4 Regularization\n",
            "\n",
            "We employ three types of regularization during training:\n",
            "\n",
            "Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\n",
            "sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\n",
            "positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
            "Pdrop = 0.1.\n",
            "\n",
            "7\n",
            "\n",
            "\n",
            "\n",
            "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
            "English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
            "\n",
            "Model\n",
            "BLEU Training Cost (FLOPs)\n",
            "\n",
            "EN-DE EN-FR EN-DE EN-FR\n",
            "ByteNet [18] 23.75\n",
            "Deep-Att + PosUnk [39] 39.2 1.0 · 1020\n",
            "GNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\n",
            "ConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\n",
            "MoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\n",
            "Deep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\n",
            "GNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\n",
            "ConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\n",
            "Transformer (base model) 27.3 38.1 3.3 · 1018\n",
            "Transformer (big) 28.4 41.8 2.3 · 1019\n",
            "\n",
            "Label Smoothing During training, we employed label smoothing of value �ls = 0.1 [36]. This\n",
            "hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
            "\n",
            "6 Results\n",
            "\n",
            "6.1 Machine Translation\n",
            "\n",
            "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
            "in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\n",
            "BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\n",
            "listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\n",
            "surpasses all previously published models and ensembles, at a fraction of the training cost of any of\n",
            "the competitive models.\n",
            "\n",
            "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\n",
            "outperforming all of the previously published single models, at less than 1/4 the training cost of the\n",
            "previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\n",
            "dropout rate Pdrop = 0.1, instead of 0.3.\n",
            "\n",
            "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\n",
            "were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\n",
            "used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\n",
            "were chosen after experimentation on the development set. We set the maximum output length during\n",
            "inference to input length + 50, but terminate early when possible [38].\n",
            "\n",
            "Table 2 summarizes our results and compares our translation quality and training costs to other model\n",
            "architectures from the literature. We estimate the number of floating point operations used to train a\n",
            "model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\n",
            "single-precision floating-point capacity of each GPU 5.\n",
            "\n",
            "6.2 Model Variations\n",
            "\n",
            "To evaluate the importance of different components of the Transformer, we varied our base model\n",
            "in different ways, measuring the change in performance on English-to-German translation on the\n",
            "development set, newstest2013. We used beam search as described in the previous section, but no\n",
            "checkpoint averaging. We present these results in Table 3.\n",
            "\n",
            "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\n",
            "keeping the amount of computation constant, as described in Section 3.2.2. While single-head\n",
            "attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
            "\n",
            "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
            "\n",
            "8\n",
            "\n",
            "\n",
            "\n",
            "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\n",
            "model. All metrics are on the English-to-German translation development set, newstest2013. Listed\n",
            "perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\n",
            "per-word perplexities.\n",
            "\n",
            "N dmodel dff h dk dv Pdrop �ls\n",
            "train PPL BLEU params\n",
            "steps (dev) (dev) ×106\n",
            "\n",
            "base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n",
            "\n",
            "(A)\n",
            "\n",
            "1 512 512 5.29 24.9\n",
            "4 128 128 5.00 25.5\n",
            "\n",
            "16 32 32 4.91 25.8\n",
            "32 16 16 5.01 25.4\n",
            "\n",
            "(B)\n",
            "16 5.16 25.1 58\n",
            "32 5.01 25.4 60\n",
            "\n",
            "(C)\n",
            "\n",
            "2 6.11 23.7 36\n",
            "4 5.19 25.3 50\n",
            "8 4.88 25.5 80\n",
            "\n",
            "256 32 32 5.75 24.5 28\n",
            "1024 128 128 4.66 26.0 168\n",
            "\n",
            "1024 5.12 25.4 53\n",
            "4096 4.75 26.2 90\n",
            "\n",
            "(D)\n",
            "\n",
            "0.0 5.77 24.6\n",
            "0.2 4.95 25.5\n",
            "\n",
            "0.0 4.67 25.3\n",
            "0.2 5.47 25.7\n",
            "\n",
            "(E) positional embedding instead of sinusoids 4.92 25.7\n",
            "big 6 1024 4096 16 0.3 300K 4.33 26.4 213\n",
            "\n",
            "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\n",
            "of WSJ)\n",
            "\n",
            "Parser Training WSJ 23 F1\n",
            "Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\n",
            "\n",
            "Petrov et al. (2006) [29] WSJ only, discriminative 90.4\n",
            "Zhu et al. (2013) [40] WSJ only, discriminative 90.4\n",
            "Dyer et al. (2016) [8] WSJ only, discriminative 91.7\n",
            "Transformer (4 layers) WSJ only, discriminative 91.3\n",
            "Zhu et al. (2013) [40] semi-supervised 91.3\n",
            "\n",
            "Huang & Harper (2009) [14] semi-supervised 91.3\n",
            "McClosky et al. (2006) [26] semi-supervised 92.1\n",
            "\n",
            "Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\n",
            "Transformer (4 layers) semi-supervised 92.7\n",
            "\n",
            "Luong et al. (2015) [23] multi-task 93.0\n",
            "Dyer et al. (2016) [8] generative 93.3\n",
            "\n",
            "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\n",
            "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
            "function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\n",
            "bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\n",
            "sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\n",
            "results to the base model.\n",
            "\n",
            "6.3 English Constituency Parsing\n",
            "\n",
            "To evaluate if the Transformer can generalize to other tasks we performed experiments on English\n",
            "constituency parsing. This task presents specific challenges: the output is subject to strong structural\n",
            "\n",
            "9\n",
            "\n",
            "\n",
            "\n",
            "constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\n",
            "models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
            "\n",
            "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\n",
            "Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\n",
            "using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n",
            "[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\n",
            "for the semi-supervised setting.\n",
            "\n",
            "We performed only a small number of experiments to select the dropout, both attention and residual\n",
            "(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
            "remained unchanged from the English-to-German base translation model. During inference, we\n",
            "increased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\n",
            "for both WSJ only and the semi-supervised setting.\n",
            "\n",
            "Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\n",
            "prisingly well, yielding better results than all previously reported models with the exception of the\n",
            "Recurrent Neural Network Grammar [8].\n",
            "\n",
            "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\n",
            "Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
            "\n",
            "7 Conclusion\n",
            "\n",
            "In this work, we presented the Transformer, the first sequence transduction model based entirely on\n",
            "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
            "multi-headed self-attention.\n",
            "\n",
            "For translation tasks, the Transformer can be trained significantly faster than architectures based\n",
            "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
            "English-to-French translation tasks, we achieve a new state of the art. In the former task our best\n",
            "model outperforms even all previously reported ensembles.\n",
            "\n",
            "We are excited about the future of attention-based models and plan to apply them to other tasks. We\n",
            "plan to extend the Transformer to problems involving input and output modalities other than text and\n",
            "to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\n",
            "such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
            "\n",
            "The code we used to train and evaluate our models is available at https://github.com/\n",
            "tensorflow/tensor2tensor.\n",
            "\n",
            "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
            "comments, corrections and inspiration.\n",
            "\n",
            "References\n",
            "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n",
            "\n",
            "arXiv:1607.06450, 2016.\n",
            "\n",
            "[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
            "learning to align and translate. CoRR, abs/1409.0473, 2014.\n",
            "\n",
            "[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\n",
            "machine translation architectures. CoRR, abs/1703.03906, 2017.\n",
            "\n",
            "[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\n",
            "reading. arXiv preprint arXiv:1601.06733, 2016.\n",
            "\n",
            "[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\n",
            "and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\n",
            "machine translation. CoRR, abs/1406.1078, 2014.\n",
            "\n",
            "[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\n",
            "preprint arXiv:1610.02357, 2016.\n",
            "\n",
            "10\n",
            "\n",
            "https://github.com/tensorflow/tensor2tensor\n",
            "https://github.com/tensorflow/tensor2tensor\n",
            "http://arxiv.org/abs/1607.06450\n",
            "http://arxiv.org/abs/1601.06733\n",
            "http://arxiv.org/abs/1610.02357\n",
            "\n",
            "\n",
            "[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\n",
            "of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n",
            "\n",
            "[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\n",
            "network grammars. In Proc. of NAACL, 2016.\n",
            "\n",
            "[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\n",
            "tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n",
            "\n",
            "[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\n",
            "arXiv:1308.0850, 2013.\n",
            "\n",
            "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\n",
            "age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
            "Recognition, pages 770–778, 2016.\n",
            "\n",
            "[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\n",
            "recurrent nets: the difficulty of learning long-term dependencies, 2001.\n",
            "\n",
            "[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n",
            "9(8):1735–1780, 1997.\n",
            "\n",
            "[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\n",
            "across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\n",
            "Language Processing, pages 832–841. ACL, August 2009.\n",
            "\n",
            "[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\n",
            "the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n",
            "\n",
            "[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\n",
            "Information Processing Systems, (NIPS), 2016.\n",
            "\n",
            "[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\n",
            "on Learning Representations (ICLR), 2016.\n",
            "\n",
            "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\n",
            "ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n",
            "2017.\n",
            "\n",
            "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
            "In International Conference on Learning Representations, 2017.\n",
            "\n",
            "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
            "\n",
            "[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
            "arXiv:1703.10722, 2017.\n",
            "\n",
            "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
            "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
            "arXiv:1703.03130, 2017.\n",
            "\n",
            "[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\n",
            "sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n",
            "\n",
            "[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\n",
            "based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n",
            "\n",
            "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
            "corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n",
            "\n",
            "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\n",
            "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,\n",
            "pages 152–159. ACL, June 2006.\n",
            "\n",
            "11\n",
            "\n",
            "http://arxiv.org/abs/1705.03122\n",
            "http://arxiv.org/abs/1308.0850\n",
            "http://arxiv.org/abs/1602.02410\n",
            "http://arxiv.org/abs/1610.10099\n",
            "http://arxiv.org/abs/1703.10722\n",
            "http://arxiv.org/abs/1703.03130\n",
            "http://arxiv.org/abs/1511.06114\n",
            "http://arxiv.org/abs/1508.04025\n",
            "\n",
            "\n",
            "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
            "model. In Empirical Methods in Natural Language Processing, 2016.\n",
            "\n",
            "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
            "summarization. arXiv preprint arXiv:1705.04304, 2017.\n",
            "\n",
            "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\n",
            "and interpretable tree annotation. In Proceedings of the 21st International Conference on\n",
            "Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\n",
            "2006.\n",
            "\n",
            "[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\n",
            "preprint arXiv:1608.05859, 2016.\n",
            "\n",
            "[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\n",
            "with subword units. arXiv preprint arXiv:1508.07909, 2015.\n",
            "\n",
            "[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
            "and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\n",
            "layer. arXiv preprint arXiv:1701.06538, 2017.\n",
            "\n",
            "[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\n",
            "nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\n",
            "Learning Research, 15(1):1929–1958, 2014.\n",
            "\n",
            "[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\n",
            "networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\n",
            "Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\n",
            "Inc., 2015.\n",
            "\n",
            "[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\n",
            "networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n",
            "\n",
            "[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\n",
            "Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n",
            "\n",
            "[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\n",
            "Advances in Neural Information Processing Systems, 2015.\n",
            "\n",
            "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
            "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\n",
            "translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
            "arXiv:1609.08144, 2016.\n",
            "\n",
            "[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\n",
            "fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n",
            "\n",
            "[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\n",
            "shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n",
            "1: Long Papers), pages 434–443. ACL, August 2013.\n",
            "\n",
            "12\n",
            "\n",
            "http://arxiv.org/abs/1705.04304\n",
            "http://arxiv.org/abs/1608.05859\n",
            "http://arxiv.org/abs/1508.07909\n",
            "http://arxiv.org/abs/1701.06538\n",
            "http://arxiv.org/abs/1609.08144\n",
            "\n",
            "\n",
            "Attention VisualizationsInput-Input Layer5\n",
            "\n",
            "It is in th\n",
            "is\n",
            "\n",
            "sp\n",
            "iri\n",
            "\n",
            "t\n",
            "th\n",
            "\n",
            "at\n",
            "a m\n",
            "\n",
            "aj\n",
            "or\n",
            "\n",
            "ity\n",
            "of A\n",
            "\n",
            "m\n",
            "er\n",
            "\n",
            "ic\n",
            "an\n",
            "\n",
            "go\n",
            "ve\n",
            "\n",
            "rn\n",
            "m\n",
            "\n",
            "en\n",
            "ts\n",
            "\n",
            "ha\n",
            "ve\n",
            "\n",
            "pa\n",
            "ss\n",
            "\n",
            "ed\n",
            "ne\n",
            "\n",
            "w\n",
            "la\n",
            "\n",
            "w\n",
            "s\n",
            "\n",
            "si\n",
            "nc\n",
            "\n",
            "e\n",
            "20\n",
            "\n",
            "09\n",
            "m\n",
            "\n",
            "ak\n",
            "in\n",
            "\n",
            "g\n",
            "th\n",
            "\n",
            "e\n",
            "re\n",
            "\n",
            "gi\n",
            "st\n",
            "\n",
            "ra\n",
            "tio\n",
            "\n",
            "n\n",
            "or vo\n",
            "\n",
            "tin\n",
            "g\n",
            "\n",
            "pr\n",
            "oc\n",
            "\n",
            "es\n",
            "s\n",
            "\n",
            "m\n",
            "or\n",
            "\n",
            "e\n",
            "di\n",
            "\n",
            "ffi\n",
            "cu\n",
            "\n",
            "lt\n",
            ". <E\n",
            "\n",
            "O\n",
            "S\n",
            "\n",
            ">\n",
            "<p\n",
            "\n",
            "ad\n",
            ">\n",
            "\n",
            "<p\n",
            "ad\n",
            "\n",
            ">\n",
            "<p\n",
            "\n",
            "ad\n",
            ">\n",
            "\n",
            "<p\n",
            "ad\n",
            "\n",
            ">\n",
            "<p\n",
            "\n",
            "ad\n",
            ">\n",
            "\n",
            "<p\n",
            "ad\n",
            "\n",
            ">\n",
            "\n",
            "It is in\n",
            "th\n",
            "\n",
            "is\n",
            "sp\n",
            "\n",
            "iri\n",
            "t\n",
            "\n",
            "th\n",
            "at a\n",
            "\n",
            "m\n",
            "aj\n",
            "\n",
            "or\n",
            "ity o\n",
            "\n",
            "f\n",
            "A\n",
            "\n",
            "m\n",
            "er\n",
            "\n",
            "ic\n",
            "an\n",
            "\n",
            "go\n",
            "ve\n",
            "\n",
            "rn\n",
            "m\n",
            "\n",
            "en\n",
            "ts\n",
            "\n",
            "ha\n",
            "ve\n",
            "\n",
            "pa\n",
            "ss\n",
            "\n",
            "ed\n",
            "ne\n",
            "\n",
            "w\n",
            "la\n",
            "\n",
            "w\n",
            "s\n",
            "\n",
            "si\n",
            "nc\n",
            "\n",
            "e\n",
            "20\n",
            "\n",
            "09\n",
            "m\n",
            "\n",
            "ak\n",
            "in\n",
            "\n",
            "g\n",
            "th\n",
            "\n",
            "e\n",
            "re\n",
            "\n",
            "gi\n",
            "st\n",
            "\n",
            "ra\n",
            "tio\n",
            "\n",
            "n or\n",
            "vo\n",
            "\n",
            "tin\n",
            "g\n",
            "\n",
            "pr\n",
            "oc\n",
            "\n",
            "es\n",
            "s\n",
            "\n",
            "m\n",
            "or\n",
            "\n",
            "e\n",
            "di\n",
            "\n",
            "ffi\n",
            "cu\n",
            "\n",
            "lt .\n",
            "<E\n",
            "\n",
            "O\n",
            "S\n",
            "\n",
            ">\n",
            "<p\n",
            "\n",
            "ad\n",
            ">\n",
            "\n",
            "<p\n",
            "ad\n",
            "\n",
            ">\n",
            "<p\n",
            "\n",
            "ad\n",
            ">\n",
            "\n",
            "<p\n",
            "ad\n",
            "\n",
            ">\n",
            "<p\n",
            "\n",
            "ad\n",
            ">\n",
            "\n",
            "<p\n",
            "ad\n",
            "\n",
            ">\n",
            "\n",
            "Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
            "encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\n",
            "the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\n",
            "the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
            "\n",
            "13\n",
            "\n",
            "\n",
            "\n",
            "Input-Input Layer5\n",
            "T\n",
            "\n",
            "he\n",
            "La\n",
            "\n",
            "w\n",
            "w\n",
            "\n",
            "ill\n",
            "ne\n",
            "\n",
            "ve\n",
            "r\n",
            "\n",
            "be pe\n",
            "rf\n",
            "\n",
            "ec\n",
            "t\n",
            "\n",
            ", bu\n",
            "t\n",
            "\n",
            "its ap\n",
            "pl\n",
            "\n",
            "ic\n",
            "at\n",
            "\n",
            "io\n",
            "n\n",
            "\n",
            "sh\n",
            "ou\n",
            "\n",
            "ld\n",
            "be ju\n",
            "\n",
            "st\n",
            "- th\n",
            "\n",
            "is\n",
            "is w\n",
            "\n",
            "ha\n",
            "t\n",
            "\n",
            "w\n",
            "e\n",
            "\n",
            "ar\n",
            "e\n",
            "\n",
            "m\n",
            "is\n",
            "\n",
            "si\n",
            "ng\n",
            "\n",
            ", in m\n",
            "y\n",
            "\n",
            "op\n",
            "in\n",
            "\n",
            "io\n",
            "n\n",
            "\n",
            ". <E\n",
            "O\n",
            "\n",
            "S\n",
            ">\n",
            "\n",
            "<p\n",
            "ad\n",
            "\n",
            ">\n",
            "\n",
            "T\n",
            "he\n",
            "\n",
            "La\n",
            "w\n",
            "\n",
            "w\n",
            "ill\n",
            "\n",
            "ne\n",
            "ve\n",
            "\n",
            "r\n",
            "be\n",
            "\n",
            "pe\n",
            "rf\n",
            "\n",
            "ec\n",
            "t ,\n",
            "\n",
            "bu\n",
            "t\n",
            "\n",
            "its\n",
            "ap\n",
            "\n",
            "pl\n",
            "ic\n",
            "\n",
            "at\n",
            "io\n",
            "\n",
            "n\n",
            "sh\n",
            "\n",
            "ou\n",
            "ld be ju\n",
            "st -\n",
            "\n",
            "th\n",
            "is is\n",
            "\n",
            "w\n",
            "ha\n",
            "\n",
            "t\n",
            "w\n",
            "\n",
            "e\n",
            "ar\n",
            "\n",
            "e\n",
            "m\n",
            "\n",
            "is\n",
            "si\n",
            "\n",
            "ng\n",
            ", in m\n",
            "y\n",
            "\n",
            "op\n",
            "in\n",
            "\n",
            "io\n",
            "n .\n",
            "\n",
            "<E\n",
            "O\n",
            "\n",
            "S\n",
            ">\n",
            "\n",
            "<p\n",
            "ad\n",
            "\n",
            ">\n",
            "\n",
            "Input-Input Layer5\n",
            "\n",
            "T\n",
            "he\n",
            "\n",
            "La\n",
            "w\n",
            "\n",
            "w\n",
            "ill\n",
            "\n",
            "ne\n",
            "ve\n",
            "\n",
            "r\n",
            "be pe\n",
            "\n",
            "rf\n",
            "ec\n",
            "\n",
            "t\n",
            ", bu\n",
            "\n",
            "t\n",
            "its ap\n",
            "\n",
            "pl\n",
            "ic\n",
            "\n",
            "at\n",
            "io\n",
            "\n",
            "n\n",
            "sh\n",
            "\n",
            "ou\n",
            "ld\n",
            "\n",
            "be ju\n",
            "st\n",
            "\n",
            "- th\n",
            "is\n",
            "\n",
            "is w\n",
            "ha\n",
            "\n",
            "t\n",
            "w\n",
            "\n",
            "e\n",
            "ar\n",
            "\n",
            "e\n",
            "m\n",
            "\n",
            "is\n",
            "si\n",
            "\n",
            "ng\n",
            ", in m\n",
            "\n",
            "y\n",
            "op\n",
            "\n",
            "in\n",
            "io\n",
            "\n",
            "n\n",
            ". <E\n",
            "\n",
            "O\n",
            "S\n",
            "\n",
            ">\n",
            "<p\n",
            "\n",
            "ad\n",
            ">\n",
            "\n",
            "T\n",
            "he\n",
            "\n",
            "La\n",
            "w\n",
            "\n",
            "w\n",
            "ill\n",
            "\n",
            "ne\n",
            "ve\n",
            "\n",
            "r\n",
            "be\n",
            "\n",
            "pe\n",
            "rf\n",
            "\n",
            "ec\n",
            "t ,\n",
            "\n",
            "bu\n",
            "t\n",
            "\n",
            "its\n",
            "ap\n",
            "\n",
            "pl\n",
            "ic\n",
            "\n",
            "at\n",
            "io\n",
            "\n",
            "n\n",
            "sh\n",
            "\n",
            "ou\n",
            "ld be ju\n",
            "st -\n",
            "\n",
            "th\n",
            "is is\n",
            "\n",
            "w\n",
            "ha\n",
            "\n",
            "t\n",
            "w\n",
            "\n",
            "e\n",
            "ar\n",
            "\n",
            "e\n",
            "m\n",
            "\n",
            "is\n",
            "si\n",
            "\n",
            "ng\n",
            ", in m\n",
            "y\n",
            "\n",
            "op\n",
            "in\n",
            "\n",
            "io\n",
            "n .\n",
            "\n",
            "<E\n",
            "O\n",
            "\n",
            "S\n",
            ">\n",
            "\n",
            "<p\n",
            "ad\n",
            "\n",
            ">\n",
            "\n",
            "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\n",
            "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
            "and 6. Note that the attentions are very sharp for this word.\n",
            "\n",
            "14\n",
            "\n",
            "\n",
            "\n",
            "Input-Input Layer5\n",
            "T\n",
            "\n",
            "he\n",
            "La\n",
            "\n",
            "w\n",
            "w\n",
            "\n",
            "ill\n",
            "ne\n",
            "\n",
            "ve\n",
            "r\n",
            "\n",
            "be pe\n",
            "rf\n",
            "\n",
            "ec\n",
            "t\n",
            "\n",
            ", bu\n",
            "t\n",
            "\n",
            "its ap\n",
            "pl\n",
            "\n",
            "ic\n",
            "at\n",
            "\n",
            "io\n",
            "n\n",
            "\n",
            "sh\n",
            "ou\n",
            "\n",
            "ld\n",
            "be ju\n",
            "\n",
            "st\n",
            "- th\n",
            "\n",
            "is\n",
            "is w\n",
            "\n",
            "ha\n",
            "t\n",
            "\n",
            "w\n",
            "e\n",
            "\n",
            "ar\n",
            "e\n",
            "\n",
            "m\n",
            "is\n",
            "\n",
            "si\n",
            "ng\n",
            "\n",
            ", in m\n",
            "y\n",
            "\n",
            "op\n",
            "in\n",
            "\n",
            "io\n",
            "n\n",
            "\n",
            ". <E\n",
            "O\n",
            "\n",
            "S\n",
            ">\n",
            "\n",
            "<p\n",
            "ad\n",
            "\n",
            ">\n",
            "\n",
            "T\n",
            "he\n",
            "\n",
            "La\n",
            "w\n",
            "\n",
            "w\n",
            "ill\n",
            "\n",
            "ne\n",
            "ve\n",
            "\n",
            "r\n",
            "be\n",
            "\n",
            "pe\n",
            "rf\n",
            "\n",
            "ec\n",
            "t ,\n",
            "\n",
            "bu\n",
            "t\n",
            "\n",
            "its\n",
            "ap\n",
            "\n",
            "pl\n",
            "ic\n",
            "\n",
            "at\n",
            "io\n",
            "\n",
            "n\n",
            "sh\n",
            "\n",
            "ou\n",
            "ld be ju\n",
            "st -\n",
            "\n",
            "th\n",
            "is is\n",
            "\n",
            "w\n",
            "ha\n",
            "\n",
            "t\n",
            "w\n",
            "\n",
            "e\n",
            "ar\n",
            "\n",
            "e\n",
            "m\n",
            "\n",
            "is\n",
            "si\n",
            "\n",
            "ng\n",
            ", in m\n",
            "y\n",
            "\n",
            "op\n",
            "in\n",
            "\n",
            "io\n",
            "n .\n",
            "\n",
            "<E\n",
            "O\n",
            "\n",
            "S\n",
            ">\n",
            "\n",
            "<p\n",
            "ad\n",
            "\n",
            ">\n",
            "\n",
            "Input-Input Layer5\n",
            "\n",
            "T\n",
            "he\n",
            "\n",
            "La\n",
            "w\n",
            "\n",
            "w\n",
            "ill\n",
            "\n",
            "ne\n",
            "ve\n",
            "\n",
            "r\n",
            "be pe\n",
            "\n",
            "rf\n",
            "ec\n",
            "\n",
            "t\n",
            ", bu\n",
            "\n",
            "t\n",
            "its ap\n",
            "\n",
            "pl\n",
            "ic\n",
            "\n",
            "at\n",
            "io\n",
            "\n",
            "n\n",
            "sh\n",
            "\n",
            "ou\n",
            "ld\n",
            "\n",
            "be ju\n",
            "st\n",
            "\n",
            "- th\n",
            "is\n",
            "\n",
            "is w\n",
            "ha\n",
            "\n",
            "t\n",
            "w\n",
            "\n",
            "e\n",
            "ar\n",
            "\n",
            "e\n",
            "m\n",
            "\n",
            "is\n",
            "si\n",
            "\n",
            "ng\n",
            ", in m\n",
            "\n",
            "y\n",
            "op\n",
            "\n",
            "in\n",
            "io\n",
            "\n",
            "n\n",
            ". <E\n",
            "\n",
            "O\n",
            "S\n",
            "\n",
            ">\n",
            "<p\n",
            "\n",
            "ad\n",
            ">\n",
            "\n",
            "T\n",
            "he\n",
            "\n",
            "La\n",
            "w\n",
            "\n",
            "w\n",
            "ill\n",
            "\n",
            "ne\n",
            "ve\n",
            "\n",
            "r\n",
            "be\n",
            "\n",
            "pe\n",
            "rf\n",
            "\n",
            "ec\n",
            "t ,\n",
            "\n",
            "bu\n",
            "t\n",
            "\n",
            "its\n",
            "ap\n",
            "\n",
            "pl\n",
            "ic\n",
            "\n",
            "at\n",
            "io\n",
            "\n",
            "n\n",
            "sh\n",
            "\n",
            "ou\n",
            "ld be ju\n",
            "st -\n",
            "\n",
            "th\n",
            "is is\n",
            "\n",
            "w\n",
            "ha\n",
            "\n",
            "t\n",
            "w\n",
            "\n",
            "e\n",
            "ar\n",
            "\n",
            "e\n",
            "m\n",
            "\n",
            "is\n",
            "si\n",
            "\n",
            "ng\n",
            ", in m\n",
            "y\n",
            "\n",
            "op\n",
            "in\n",
            "\n",
            "io\n",
            "n .\n",
            "\n",
            "<E\n",
            "O\n",
            "\n",
            "S\n",
            ">\n",
            "\n",
            "<p\n",
            "ad\n",
            "\n",
            ">\n",
            "\n",
            "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\n",
            "sentence. We give two such examples above, from two different heads from the encoder self-attention\n",
            "at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
            "\n",
            "15\n",
            "\n",
            "\n",
            "\t1 Introduction\n",
            "\t2 Background\n",
            "\t3 Model Architecture\n",
            "\t3.1 Encoder and Decoder Stacks\n",
            "\t3.2 Attention\n",
            "\t3.2.1 Scaled Dot-Product Attention\n",
            "\t3.2.2 Multi-Head Attention\n",
            "\t3.2.3 Applications of Attention in our Model\n",
            "\n",
            "\t3.3 Position-wise Feed-Forward Networks\n",
            "\t3.4 Embeddings and Softmax\n",
            "\t3.5 Positional Encoding\n",
            "\n",
            "\t4 Why Self-Attention\n",
            "\t5 Training\n",
            "\t5.1 Training Data and Batching\n",
            "\t5.2 Hardware and Schedule\n",
            "\t5.3 Optimizer\n",
            "\t5.4 Regularization\n",
            "\n",
            "\t6 Results\n",
            "\t6.1 Machine Translation\n",
            "\t6.2 Model Variations\n",
            "\t6.3 English Constituency Parsing\n",
            "\n",
            "\t7 Conclusion\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizando artigo\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "inputs = tokenizer(artigo)\n",
        "inputs"
      ],
      "metadata": {
        "id": "lcj5u851SNQg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ec6e8fcdfaed4f61b1da4bff79af6e31",
            "0c8fa03d1e7f431ea6d624475fdfea84",
            "9320d85e1a084c7b804208dafd829c3c",
            "4747ea38701f45799c0fec2cb0ed000b",
            "9cfb4227ec114759b32d7a7300404c46",
            "60b8dc37f6e4455caa406b9561d7c5ea",
            "adc772fb7cec4606922c5846f0e200b0",
            "d4fee0eb241143c18a8fa68048d5b1a9",
            "114e429ad9a94dbc96423e574fd1924d",
            "11121c60619d47d2acb3b38e041c956e",
            "8a225e8a977043118df5843b23170f85",
            "cd192e31b45a4b5c9c30da23e2524247",
            "95021229a5794dada610c71d6da7fe9c",
            "1275a9a3d9cb4f73a1c1ac5a9a09cade",
            "8bf936941e1840c1b200f9d993261e63",
            "f0062bd08d9f457090d7a793239e6934",
            "f668d24aeb8d40a1a9da210d854bee4d",
            "49930d69049b491d9a8d500fcbab9660",
            "ef48e054ba4e40f3bfa3358090c08f57",
            "76f7a4d43017424b849d2bd3189cd434",
            "0a87e0f96d224ff0ad998eba9822975e",
            "4592e9c2ebc746af90aadce1686115cb",
            "35470a1cbcf7407e83b7774c9ac212ff",
            "c3d5dda2dd354adebe1b22f1c17a32a6",
            "460deb22df5c4f0aac6c08df67dcda82",
            "4fbf6770f27341348a54c8abe9942869",
            "0d8c8e81af1e489f9aa1edeaf64bfc39",
            "856b298fb46140efb4e68dd4d1dd26a6",
            "217f9505891d4f768c1135738ad666a3",
            "874235ed629e425f8a6bec5784c334ed",
            "d1d934bb8c7e4b05bf83d6c2f4ad522e",
            "e7a6fffb966c4d6c8f6ee7990005bb2a",
            "8f3b50d99b32475fbbe3e94e4d556d98",
            "97f34f92c3564bc7aee1e55ca73bbdb7",
            "409208e6a5894d01b300f7852df08038",
            "a6f9895fbdb2439486890f69869dea6d",
            "5119ca035cd1432bb468be834dd19c39",
            "5ddb5942e68642aaa6ad6e642be4c32d",
            "7c348112580d4e808593ca76b245c818",
            "c8ca339bb2c744b8bbc3a0793d5503f0",
            "a1f5e33a7b6542c399833121d1b984c6",
            "b9eed4b986aa45819b25ba5d1a383fd0",
            "d6e6c1eaefc24cbababfbbff2d9ff435",
            "84d5e9c7949b4147b330e660f1e0c357"
          ]
        },
        "outputId": "e63fe450-2ff8-4893-99bc-b8fbb6ff6c2e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec6e8fcdfaed4f61b1da4bff79af6e31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd192e31b45a4b5c9c30da23e2524247"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35470a1cbcf7407e83b7774c9ac212ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97f34f92c3564bc7aee1e55ca73bbdb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (10550 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 3086, 2003, 2035, 2017, 2342, 6683, 4509, 12436, 26760, 7088, 30125, 8224, 4167, 10927, 26760, 7088, 1030, 8224, 1012, 4012, 2053, 3286, 21146, 23940, 2099, 30125, 8224, 4167, 2053, 3286, 1030, 8224, 1012, 4012, 23205, 2072, 19177, 2099, 30125, 8224, 2470, 23205, 11514, 1030, 8224, 1012, 4012, 19108, 2149, 29002, 2890, 4183, 30125, 8224, 2470, 2149, 2480, 1030, 8224, 1012, 4012, 2222, 3258, 3557, 30125, 8224, 2470, 2222, 3258, 1030, 8224, 1012, 4012, 12643, 1050, 1012, 12791, 30125, 1526, 2118, 1997, 4361, 12643, 1030, 20116, 1012, 4361, 1012, 3968, 2226, 1105, 15750, 17112, 15676, 30125, 8224, 4167, 23739, 2480, 11151, 8043, 1030, 8224, 1012, 4012, 5665, 2401, 11037, 6342, 10023, 2378, 30125, 1527, 5665, 2401, 1012, 11037, 6342, 10023, 2378, 1030, 20917, 4014, 1012, 4012, 10061, 1996, 7444, 5537, 9099, 16256, 4275, 2024, 2241, 2006, 3375, 28667, 29264, 2030, 9530, 6767, 7630, 3508, 2389, 15756, 6125, 2008, 2421, 2019, 4372, 16044, 2099, 1998, 1037, 21933, 4063, 1012, 1996, 2190, 4488, 4275, 2036, 7532, 1996, 4372, 16044, 2099, 1998, 21933, 4063, 2083, 2019, 3086, 7337, 1012, 2057, 16599, 1037, 2047, 3722, 2897, 4294, 1010, 1996, 10938, 2121, 1010, 2241, 9578, 2006, 3086, 10595, 1010, 4487, 13102, 6132, 2075, 2007, 28667, 3126, 24413, 1998, 9530, 6767, 7630, 9285, 4498, 1012, 7885, 2006, 2048, 3698, 5449, 8518, 2265, 2122, 4275, 2000, 2022, 6020, 1999, 3737, 2096, 2108, 2062, 5903, 21335, 3468, 1998, 9034, 6022, 2625, 2051, 2000, 3345, 1012, 2256, 2944, 6162, 2015, 2654, 1012, 1018, 1038, 2571, 2226, 2006, 1996, 1059, 20492, 2297, 2394, 1011, 2000, 1011, 2446, 5449, 4708, 1010, 9229, 2058, 1996, 4493, 2190, 3463, 1010, 2164, 21528, 1010, 2011, 2058, 1016, 1038, 2571, 2226, 1012, 2006, 1996, 1059, 20492, 2297, 2394, 1011, 2000, 1011, 2413, 5449, 4708, 1010, 2256, 2944, 21009, 1037, 2047, 2309, 1011, 2944, 2110, 1011, 1997, 1011, 1996, 1011, 2396, 1038, 2571, 2226, 3556, 1997, 4601, 1012, 1022, 2044, 2731, 2005, 1017, 1012, 1019, 2420, 2006, 2809, 14246, 2271, 1010, 1037, 2235, 12884, 1997, 1996, 2731, 5366, 1997, 1996, 2190, 4275, 2013, 1996, 3906, 1012, 2057, 2265, 2008, 1996, 10938, 2121, 2236, 10057, 2092, 2000, 2060, 8518, 2011, 11243, 2009, 5147, 2000, 2394, 5540, 11968, 7741, 2119, 2007, 2312, 1998, 3132, 2731, 2951, 1012, 1015, 4955, 28667, 29264, 15756, 6125, 1010, 2146, 2460, 1011, 2744, 3638, 1031, 2410, 1033, 1998, 4796, 2094, 28667, 29264, 1031, 1021, 1033, 15756, 6125, 1999, 3327, 1010, 2031, 2042, 7933, 2511, 2004, 2110, 1997, 1996, 2396, 8107, 1999, 5537, 11643, 1998, 1598, 2063, 26426, 6691, 1012, 10328, 2344, 2003, 6721, 1012, 19108, 3818, 6419, 29300, 3619, 2007, 2969, 1011, 3086, 1998, 2318, 1996, 3947, 2000, 16157, 2023, 2801, 1012, 6683, 4509, 1010, 2007, 5665, 2401, 1010, 2881, 1998, 7528, 1996, 2034, 10938, 2121, 4275, 1998, 2038, 2042, 10232, 2135, 2920, 1999, 2296, 7814, 1997, 2023, 2147, 1012, 2053, 3286, 3818, 18953, 11089, 1011, 4031, 3086, 1010, 4800, 1011, 2132, 3086, 1998, 1996, 16381, 1011, 2489, 2597, 6630, 1998, 2150, 1996, 2060, 2711, 2920, 1999, 3053, 2296, 6987, 1012, 23205, 2072, 2881, 1010, 7528, 1010, 15757, 1998, 16330, 14518, 2944, 10176, 1999, 2256, 2434, 3642, 15058, 1998, 23435, 2475, 25808, 2953, 1012, 2222, 3258, 2036, 20918, 2007, 3117, 2944, 10176, 1010, 2001, 3625, 2005, 2256, 3988, 3642, 15058, 1010, 1998, 8114, 28937, 1998, 5107, 22318, 1012, 23739, 2480, 1998, 12643, 2985, 14518, 2146, 2420, 12697, 2536, 3033, 1997, 1998, 14972, 23435, 2475, 25808, 2953, 1010, 6419, 2256, 3041, 3642, 15058, 1010, 6551, 9229, 3463, 1998, 5294, 2135, 29494, 2256, 2470, 1012, 1526, 2147, 2864, 2096, 2012, 8224, 4167, 1012, 1527, 2147, 2864, 2096, 2012, 8224, 2470, 1012, 17089, 3034, 2006, 15756, 2592, 6364, 3001, 1006, 9152, 4523, 2418, 1007, 1010, 2146, 3509, 1010, 6187, 1010, 3915, 1012, 12098, 1060, 4921, 1024, 1015, 3963, 1020, 1012, 6021, 6146, 1016, 2615, 1019, 1031, 20116, 1012, 1039, 1048, 1033, 1020, 1040, 14925, 1016, 5890, 1021, 9099, 16256, 3471, 2107, 2004, 2653, 11643, 1998, 3698, 5449, 1031, 3486, 1010, 1016, 1010, 1019, 1033, 1012, 3365, 4073, 2031, 2144, 2506, 2000, 5245, 1996, 7372, 1997, 28667, 29264, 2653, 4275, 1998, 4372, 16044, 2099, 1011, 21933, 4063, 4294, 2015, 1031, 4229, 1010, 2484, 1010, 2321, 1033, 1012, 28667, 29264, 4275, 4050, 5387, 22334, 2247, 1996, 6454, 4460, 1997, 1996, 7953, 1998, 6434, 10071, 1012, 25705, 2075, 1996, 4460, 2000, 4084, 1999, 22334, 2051, 1010, 2027, 9699, 1037, 5537, 1997, 5023, 2163, 1044, 2102, 1010, 2004, 1037, 3853, 1997, 1996, 3025, 5023, 2110, 1044, 2102, 27944, 1998, 1996, 7953, 2005, 2597, 1056, 1012, 2023, 26096, 25582, 3267, 3653, 20464, 22087, 5903, 3989, 2306, 2731, 4973, 1010, 2029, 4150, 4187, 2012, 2936, 5537, 10742, 1010, 2004, 3638, 14679, 5787, 14108, 2075, 2408, 4973, 1012, 3522, 2147, 2038, 4719, 3278, 8377, 1999, 15078, 8122, 2083, 5387, 3989, 12225, 1031, 2538, 1033, 1998, 18462, 22334, 1031, 3590, 1033, 1010, 2096, 2036, 9229, 2944, 2836, 1999, 2553, 1997, 1996, 3732, 1012, 1996, 8050, 27142, 1997, 25582, 22334, 1010, 2174, 1010, 3464, 1012, 3086, 10595, 2031, 2468, 2019, 9897, 2112, 1997, 17075, 5537, 11643, 1998, 9099, 8566, 2278, 1011, 14841, 2239, 4275, 1999, 2536, 8518, 1010, 4352, 11643, 1997, 12530, 15266, 2302, 7634, 2000, 2037, 3292, 1999, 1996, 7953, 2030, 6434, 10071, 1031, 1016, 1010, 2539, 1033, 1012, 1999, 2035, 2021, 1037, 2261, 3572, 1031, 2676, 1033, 1010, 2174, 1010, 2107, 3086, 10595, 2024, 2109, 1999, 9595, 2007, 1037, 28667, 29264, 2897, 1012, 1999, 2023, 2147, 2057, 16599, 1996, 10938, 2121, 1010, 1037, 2944, 4294, 9686, 5403, 9328, 28667, 3126, 24413, 1998, 2612, 18345, 4498, 2006, 2019, 3086, 7337, 2000, 4009, 3795, 12530, 15266, 2090, 7953, 1998, 6434, 1012, 1996, 10938, 2121, 4473, 2005, 6022, 2062, 5903, 3989, 1998, 2064, 3362, 1037, 2047, 2110, 1997, 1996, 2396, 1999, 5449, 3737, 2044, 2108, 4738, 2005, 2004, 2210, 2004, 4376, 2847, 2006, 2809, 1052, 18613, 14246, 2271, 1012, 1016, 4281, 1996, 3125, 1997, 8161, 25582, 22334, 2036, 3596, 1996, 3192, 1997, 1996, 3668, 15756, 14246, 2226, 1031, 2385, 1033, 1010, 24880, 7159, 1031, 2324, 1033, 1998, 9530, 15088, 2475, 2015, 1031, 1023, 1033, 1010, 2035, 1997, 2029, 2224, 9530, 6767, 7630, 3508, 2389, 15756, 6125, 2004, 3937, 2311, 3796, 1010, 9798, 5023, 15066, 1999, 5903, 2005, 2035, 7953, 1998, 6434, 4460, 1012, 1999, 2122, 4275, 1010, 1996, 2193, 1997, 3136, 3223, 2000, 14396, 7755, 2013, 2048, 15275, 7953, 2030, 6434, 4460, 7502, 1999, 1996, 3292, 2090, 4460, 1010, 7399, 2135, 2005, 9530, 15088, 2475, 2015, 1998, 8833, 8486, 2705, 7712, 3973, 2005, 24880, 7159, 1012, 2023, 3084, 2009, 2062, 3697, 2000, 4553, 12530, 15266, 2090, 6802, 4460, 1031, 2260, 1033, 1012, 1999, 1996, 10938, 2121, 2023, 2003, 4359, 2000, 1037, 5377, 2193, 1997, 3136, 1010, 12167, 2012, 1996, 3465, 1997, 4359, 4621, 5813, 2349, 2000, 14985, 3086, 1011, 18215, 4460, 1010, 2019, 3466, 2057, 4675, 18908, 2007, 4800, 1011, 2132, 3086, 2004, 2649, 1999, 2930, 1017, 1012, 1016, 1012, 2969, 1011, 3086, 1010, 2823, 2170, 26721, 1011, 3086, 2003, 2019, 3086, 7337, 8800, 2367, 4460, 1997, 1037, 2309, 5537, 1999, 2344, 2000, 24134, 1037, 6630, 1997, 1996, 5537, 1012, 2969, 1011, 3086, 2038, 2042, 2109, 5147, 1999, 1037, 3528, 1997, 8518, 2164, 3752, 26683, 1010, 10061, 3512, 7680, 7849, 3989, 1010, 25304, 4372, 14162, 3672, 1998, 4083, 4708, 1011, 2981, 6251, 15066, 1031, 1018, 1010, 2676, 1010, 2654, 1010, 2570, 1033, 1012, 2203, 1011, 2000, 1011, 2203, 3638, 6125, 2024, 2241, 2006, 1037, 28667, 29264, 3086, 7337, 2612, 1997, 5537, 1011, 13115, 28667, 3126, 24413, 1998, 2031, 2042, 3491, 2000, 4685, 2092, 2006, 3722, 1011, 2653, 3160, 10739, 1998, 2653, 11643, 8518, 1031, 4090, 1033, 1012, 2000, 1996, 2190, 1997, 2256, 3716, 1010, 2174, 1010, 1996, 10938, 2121, 2003, 1996, 2034, 9099, 16256, 2944, 18345, 4498, 2006, 2969, 1011, 3086, 2000, 24134, 15066, 1997, 2049, 7953, 1998, 6434, 2302, 2478, 5537, 1011, 13115, 29300, 3619, 2030, 9530, 6767, 7630, 3508, 1012, 1999, 1996, 2206, 5433, 1010, 2057, 2097, 6235, 1996, 10938, 2121, 1010, 9587, 29068, 3686, 2969, 1011, 3086, 1998, 6848, 2049, 12637, 2058, 4275, 2107, 2004, 1031, 2459, 1010, 2324, 1033, 1998, 1031, 1023, 1033, 1012, 1017, 2944, 4294, 2087, 6975, 15756, 5537, 9099, 16256, 4275, 2031, 2019, 4372, 16044, 2099, 1011, 21933, 4063, 3252, 1031, 1019, 1010, 1016, 1010, 3486, 1033, 1012, 2182, 1010, 1996, 4372, 16044, 2099, 7341, 2019, 7953, 5537, 1997, 6454, 15066, 1006, 1060, 2487, 1010, 1012, 1012, 1012, 1010, 1060, 2078, 1007, 2000, 1037, 5537, 1997, 7142, 15066, 1062, 1027, 1006, 1062, 2487, 1010, 1012, 1012, 1012, 1010, 1062, 2078, 1007, 1012, 2445, 1062, 1010, 1996, 21933, 4063, 2059, 19421, 2019, 6434, 5537, 1006, 1061, 2487, 1010, 1012, 1012, 1012, 1010, 1061, 2213, 1007, 1997, 9255, 2028, 5783, 2012, 1037, 2051, 1012, 2012, 2169, 3357, 1996, 2944, 2003, 8285, 1011, 19723, 8303, 3512, 1031, 2184, 1033, 1010, 15077, 1996, 3130, 7013, 9255, 2004, 3176, 7953, 2043, 11717, 1996, 2279, 1012, 1996, 10938, 2121, 4076, 2023, 3452, 4294, 2478, 16934, 2969, 1011, 3086, 1998, 2391, 1011, 7968, 1010, 3929, 4198, 9014, 2005, 2119, 1996, 4372, 16044, 2099, 1998, 21933, 4063, 1010, 3491, 1999, 1996, 2187, 1998, 2157, 23672, 1997, 3275, 1015, 1010, 4414, 1012, 1016, 3275, 1015, 1024, 1996, 10938, 2121, 1011, 2944, 4294, 1012, 1017, 1012, 1015, 4372, 16044, 2099, 1998, 21933, 4063, 20829, 4372, 16044, 2099, 1024, 1996, 4372, 16044, 2099, 2003, 3605, 1997, 1037, 9991, 1997, 1050, 1027, 1020, 7235, 9014, 1012, 2169, 6741, 2038, 2048, 4942, 1011, 9014, 1012, 1996, 2034, 2003, 1037, 4800, 1011, 2132, 2969, 1011, 3086, 7337, 1010, 1998, 1996, 2117, 2003, 1037, 3722, 1010, 2597, 1011, 7968, 3929, 4198, 5438, 1011, 2830, 2897, 1012, 2057, 12666, 1037, 21961, 4434, 1031, 2340, 1033, 2105, 2169, 1997, 1996, 2048, 4942, 1011, 9014, 1010, 2628, 2011, 6741, 3671, 3989, 1031, 1015, 1033, 1012, 2008, 2003, 1010, 1996, 6434, 1997, 2169, 4942, 1011, 6741, 2003, 6741, 12131, 2213, 1006, 1060, 1009, 4942, 24314, 1006, 1060, 1007, 1007, 1010, 2073, 4942, 24314, 1006, 1060, 1007, 2003, 1996, 3853, 7528, 2011, 1996, 4942, 1011, 6741, 2993, 1012, 2000, 10956, 2122, 21961, 7264, 1010, 2035, 4942, 1011, 9014, 1999, 1996, 2944, 1010, 2004, 2092, 2004, 1996, 7861, 8270, 4667, 9014, 1010, 3965, 27852, 1997, 9812, 1040, 5302, 9247, 1027, 24406, 1012, 21933, 4063, 1024, 1996, 21933, 4063, 2003, 2036, 3605, 1997, 1037, 9991, 1997, 1050, 1027, 1020, 7235, 9014, 1012, 1999, 2804, 2000, 1996, 2048, 4942, 1011, 9014, 1999, 2169, 4372, 16044, 2099, 6741, 1010, 1996, 21933, 4063, 19274, 2015, 1037, 2353, 4942, 1011, 6741, 1010, 2029, 10438, 4800, 1011, 2132, 3086, 2058, 1996, 6434, 1997, 1996, 4372, 16044, 2099, 9991, 1012, 2714, 2000, 1996, 4372, 16044, 2099, 1010, 2057, 12666, 21961, 7264, 2105, 2169, 1997, 1996, 4942, 1011, 9014, 1010, 2628, 2011, 6741, 3671, 3989, 1012, 2057, 2036, 19933, 1996, 2969, 1011, 3086, 4942, 1011, 6741, 1999, 1996, 21933, 4063, 9991, 2000, 4652, 4460, 2013, 7052, 2000, 4745, 4460, 1012, 2023, 7308, 2075, 1010, 4117, 2007, 2755, 2008, 1996, 6434, 7861, 8270, 4667, 2015, 2024, 16396, 2011, 2028, 2597, 1010, 21312, 2008, 1996, 20932, 2005, 2597, 1045, 2064, 12530, 2069, 2006, 1996, 2124, 27852, 2012, 4460, 2625, 2084, 1045, 1012, 1017, 1012, 1016, 3086, 2019, 3086, 3853, 2064, 2022, 2649, 2004, 12375, 1037, 23032, 1998, 1037, 2275, 1997, 3145, 1011, 3643, 7689, 2000, 2019, 6434, 1010, 2073, 1996, 23032, 1010, 6309, 1010, 5300, 1010, 1998, 6434, 2024, 2035, 19019, 1012, 1996, 6434, 2003, 24806, 2004, 1037, 18215, 7680, 1997, 1996, 5300, 1010, 2073, 1996, 3635, 4137, 2000, 2169, 3643, 2003, 24806, 2011, 1037, 21778, 3853, 1997, 1996, 23032, 2007, 1996, 7978, 3145, 1012, 1017, 18953, 11089, 1011, 4031, 3086, 4800, 1011, 2132, 3086, 3275, 1016, 1024, 1006, 2187, 1007, 18953, 11089, 1011, 4031, 3086, 1012, 1006, 2157, 1007, 4800, 1011, 2132, 3086, 3774, 1997, 2195, 3086, 9014, 2770, 1999, 5903, 1012, 1017, 1012, 1016, 1012, 1015, 18953, 11089, 1011, 4031, 3086, 2057, 2655, 2256, 3327, 3086, 1000, 18953, 11089, 1011, 4031, 3086, 1000, 1006, 3275, 1016, 1007, 1012, 1996, 7953, 3774, 1997, 10861, 5134, 1998, 6309, 1997, 9812, 1040, 2243, 1010, 1998, 5300, 1997, 9812, 1040, 2615, 1012, 2057, 24134, 1996, 11089, 3688, 1997, 1996, 23032, 2007, 2035, 6309, 1010, 11443, 2169, 2011, 1600, 1040, 2243, 1010, 1998, 6611, 1037, 3730, 17848, 3853, 2000, 6855, 1996, 15871, 2006, 1996, 5300, 1012, 1999, 3218, 1010, 2057, 24134, 1996, 3086, 3853, 2006, 1037, 2275, 1997, 10861, 5134, 7453, 1010, 8966, 2362, 2046, 1037, 8185, 1053, 1012, 1996, 6309, 1998, 5300, 2024, 2036, 8966, 2362, 2046, 21520, 1047, 1998, 1058, 1012, 2057, 24134, 1996, 8185, 1997, 27852, 2004, 1024, 3086, 1006, 1053, 1010, 1047, 1010, 1058, 1007, 1027, 3730, 17848, 1006, 1053, 25509, 1600, 1040, 2243, 1007, 1058, 1006, 1015, 1007, 1996, 2048, 2087, 4141, 2109, 3086, 4972, 2024, 29167, 3086, 1031, 1016, 1033, 1010, 1998, 11089, 1011, 4031, 1006, 4800, 1011, 20228, 25184, 1007, 3086, 1012, 11089, 1011, 4031, 3086, 2003, 7235, 2000, 2256, 9896, 1010, 3272, 2005, 1996, 25169, 5387, 1997, 1015, 30127, 1040, 2243, 1012, 29167, 3086, 24134, 2015, 1996, 21778, 3853, 2478, 1037, 5438, 1011, 2830, 2897, 2007, 1037, 2309, 5023, 6741, 1012, 2096, 1996, 2048, 2024, 2714, 1999, 9373, 11619, 1010, 11089, 1011, 4031, 3086, 2003, 2172, 5514, 1998, 2062, 2686, 1011, 8114, 1999, 3218, 1010, 2144, 2009, 2064, 2022, 7528, 2478, 3811, 23569, 27605, 5422, 8185, 24856, 3642, 1012, 2096, 2005, 2235, 5300, 1997, 1040, 2243, 1996, 2048, 10595, 4685, 6660, 1010, 29167, 3086, 2041, 4842, 22694, 11089, 4031, 3086, 2302, 25169, 2005, 3469, 5300, 1997, 1040, 2243, 1031, 1017, 1033, 1012, 2057, 8343, 2008, 2005, 2312, 5300, 1997, 1040, 2243, 1010, 1996, 11089, 3688, 4982, 2312, 1999, 10194, 1010, 6183, 1996, 3730, 17848, 3853, 2046, 4655, 2073, 2009, 2038, 5186, 2235, 17978, 2015, 1018, 1012, 2000, 4675, 18908, 2023, 3466, 1010, 2057, 4094, 1996, 11089, 3688, 2011, 1015, 30127, 1040, 2243, 1012, 1017, 1012, 1016, 1012, 1016, 4800, 1011, 2132, 3086, 2612, 1997, 4488, 1037, 2309, 3086, 3853, 2007, 1040, 5302, 9247, 1011, 8789, 6309, 1010, 5300, 1998, 10861, 5134, 1010, 2057, 2179, 2009, 15189, 2000, 7399, 2135, 2622, 1996, 10861, 5134, 1010, 6309, 1998, 5300, 1044, 2335, 2007, 2367, 1010, 4342, 7399, 21796, 2000, 1040, 2243, 1010, 1040, 2243, 1998, 1040, 2615, 9646, 1010, 4414, 1012, 2006, 2169, 1997, 2122, 11310, 4617, 1997, 10861, 5134, 1010, 6309, 1998, 5300, 2057, 2059, 4685, 1996, 3086, 3853, 1999, 5903, 1010, 21336, 1040, 2615, 1011, 8789, 6434, 5300, 1012, 2122, 2024, 9530, 16280, 23854, 1998, 2320, 2153, 11310, 1010, 4525, 1999, 1996, 2345, 5300, 1010, 2004, 8212, 1999, 3275, 1016, 1012, 1018, 3406, 19141, 2339, 1996, 11089, 3688, 2131, 2312, 1010, 7868, 2008, 1996, 6177, 1997, 1053, 1998, 1047, 2024, 2981, 6721, 10857, 2007, 2812, 1014, 1998, 23284, 1015, 1012, 2059, 2037, 11089, 4031, 1010, 1053, 1087, 1047, 1027, 100, 1045, 1027, 1015, 18816, 3211, 1010, 2038, 2812, 1014, 1998, 23284, 1040, 2243, 1012, 1018, 4800, 1011, 2132, 3086, 4473, 1996, 2944, 2000, 10776, 5463, 2000, 2592, 2013, 2367, 6630, 24807, 10732, 2015, 2012, 2367, 4460, 1012, 2007, 1037, 2309, 3086, 2132, 1010, 14985, 26402, 2015, 2023, 1012, 4800, 4974, 1006, 1053, 1010, 1047, 1010, 1058, 1007, 1027, 9530, 11266, 1006, 2132, 2487, 1010, 1012, 1012, 1012, 1010, 2132, 2232, 1007, 1059, 1051, 2073, 2132, 2072, 1027, 3086, 1006, 1053, 2860, 1053, 1045, 1010, 6448, 1047, 1045, 1010, 1058, 1059, 1058, 1045, 1007, 2073, 1996, 21796, 2024, 16381, 21520, 2860, 14702, 1596, 1054, 1040, 5302, 9247, 26306, 2094, 2243, 1010, 1059, 3211, 1596, 1054, 1040, 5302, 9247, 26306, 2094, 2243, 1010, 1059, 5737, 1596, 1054, 1040, 5302, 9247, 26306, 2094, 2615, 1998, 24185, 1596, 1054, 14945, 2615, 26306, 22117, 10244, 2140, 1012, 1999, 2023, 2147, 2057, 12666, 1044, 1027, 1022, 5903, 3086, 9014, 1010, 2030, 4641, 1012, 2005, 2169, 1997, 2122, 2057, 2224, 1040, 2243, 1027, 1040, 2615, 1027, 1040, 5302, 9247, 1013, 1044, 1027, 4185, 1012, 2349, 2000, 1996, 4359, 9812, 1997, 2169, 2132, 1010, 1996, 2561, 15078, 3465, 2003, 2714, 2000, 2008, 1997, 2309, 1011, 2132, 3086, 2007, 2440, 8789, 3012, 1012, 1017, 1012, 1016, 1012, 1017, 5097, 1997, 3086, 1999, 2256, 2944, 1996, 10938, 2121, 3594, 4800, 1011, 2132, 3086, 1999, 2093, 2367, 3971, 1024, 1528, 1999, 1000, 4372, 16044, 2099, 1011, 21933, 4063, 3086, 1000, 9014, 1010, 1996, 10861, 5134, 2272, 2013, 1996, 3025, 21933, 4063, 6741, 1010, 1998, 1996, 3638, 6309, 1998, 5300, 2272, 2013, 1996, 6434, 1997, 1996, 4372, 16044, 2099, 1012, 2023, 4473, 2296, 2597, 1999, 1996, 21933, 4063, 2000, 5463, 2058, 2035, 4460, 1999, 1996, 7953, 5537, 1012, 2023, 23150, 2015, 1996, 5171, 4372, 16044, 2099, 1011, 21933, 4063, 3086, 10595, 1999, 5537, 1011, 2000, 1011, 5537, 4275, 2107, 2004, 1031, 4229, 1010, 1016, 1010, 1023, 1033, 1012, 1528, 1996, 4372, 16044, 2099, 3397, 2969, 1011, 3086, 9014, 1012, 1999, 1037, 2969, 1011, 3086, 6741, 2035, 1997, 1996, 6309, 1010, 5300, 1998, 10861, 5134, 2272, 2013, 1996, 2168, 2173, 1010, 1999, 2023, 2553, 1010, 1996, 6434, 1997, 1996, 3025, 6741, 1999, 1996, 4372, 16044, 2099, 1012, 2169, 2597, 1999, 1996, 4372, 16044, 2099, 2064, 5463, 2000, 2035, 4460, 1999, 1996, 3025, 6741, 1997, 1996, 4372, 16044, 2099, 1012, 1528, 6660, 1010, 2969, 1011, 3086, 9014, 1999, 1996, 21933, 4063, 3499, 2169, 2597, 1999, 1996, 21933, 4063, 2000, 5463, 2000, 2035, 4460, 1999, 1996, 21933, 4063, 2039, 2000, 1998, 2164, 2008, 2597, 1012, 2057, 2342, 2000, 4652, 2187, 7652, 2592, 4834, 1999, 1996, 21933, 4063, 2000, 7969, 1996, 8285, 1011, 19723, 8303, 3512, 3200, 1012, 2057, 10408, 2023, 2503, 1997, 18953, 11089, 1011, 4031, 3086, 2011, 7308, 2075, 2041, 1006, 4292, 2000, 1597, 30128, 1007, 2035, 5300, 1999, 1996, 7953, 1997, 1996, 3730, 17848, 2029, 17254, 2000, 6206, 7264, 1012, 2156, 3275, 1016, 1012, 1017, 1012, 1017, 2597, 1011, 7968, 5438, 1011, 2830, 6125, 1999, 2804, 2000, 3086, 4942, 1011, 9014, 1010, 2169, 1997, 1996, 9014, 1999, 2256, 4372, 16044, 2099, 1998, 21933, 4063, 3397, 1037, 3929, 4198, 5438, 1011, 2830, 2897, 1010, 2029, 2003, 4162, 2000, 2169, 2597, 10329, 1998, 7235, 2135, 1012, 2023, 3774, 1997, 2048, 7399, 21865, 2007, 1037, 2128, 7630, 13791, 1999, 2090, 1012, 21461, 2078, 1006, 1060, 1007, 1027, 4098, 1006, 1014, 1010, 1060, 2860, 2487, 1009, 29491, 1007, 1059, 2475, 1009, 1038, 2475, 1006, 1016, 1007, 2096, 1996, 7399, 21865, 2024, 1996, 2168, 2408, 2367, 4460, 1010, 2027, 2224, 2367, 11709, 2013, 6741, 2000, 6741, 1012, 2178, 2126, 1997, 7851, 2023, 2003, 2004, 2048, 9530, 6767, 7630, 9285, 2007, 16293, 2946, 1015, 1012, 1996, 8789, 3012, 1997, 7953, 1998, 6434, 2003, 1040, 5302, 9247, 1027, 24406, 1010, 1998, 1996, 5110, 1011, 6741, 2038, 8789, 3012, 1040, 4246, 1027, 19627, 2620, 1012, 1017, 1012, 1018, 7861, 8270, 4667, 2015, 1998, 3730, 17848, 6660, 2000, 2060, 5537, 9099, 16256, 4275, 1010, 2057, 2224, 4342, 7861, 8270, 4667, 2015, 2000, 10463, 1996, 7953, 19204, 2015, 1998, 6434, 19204, 2015, 2000, 19019, 1997, 9812, 1040, 5302, 9247, 1012, 2057, 2036, 2224, 1996, 5156, 4342, 7399, 9099, 29278, 1011, 13523, 3258, 1998, 3730, 17848, 3853, 2000, 10463, 1996, 21933, 4063, 6434, 2000, 10173, 2279, 1011, 19204, 4013, 3676, 14680, 1012, 1999, 2256, 2944, 1010, 2057, 3745, 1996, 2168, 3635, 8185, 2090, 1996, 2048, 7861, 8270, 4667, 9014, 1998, 1996, 3653, 1011, 3730, 17848, 7399, 8651, 1010, 2714, 2000, 1031, 2382, 1033, 1012, 1999, 1996, 7861, 8270, 4667, 9014, 1010, 2057, 4800, 22086, 2216, 15871, 2011, 1600, 1040, 5302, 9247, 1012, 1017, 1012, 1019, 2597, 2389, 17181, 2144, 2256, 2944, 3397, 2053, 28667, 3126, 24413, 1998, 2053, 9530, 6767, 7630, 3508, 1010, 1999, 2344, 2005, 1996, 2944, 2000, 2191, 2224, 1997, 1996, 2344, 1997, 1996, 5537, 1010, 2057, 2442, 1999, 20614, 2070, 2592, 2055, 1996, 5816, 2030, 7619, 2597, 1997, 1996, 1019, 2795, 1015, 1024, 4555, 4130, 10742, 1010, 2566, 1011, 6741, 11619, 1998, 6263, 2193, 1997, 25582, 3136, 2005, 2367, 6741, 4127, 1012, 1050, 2003, 1996, 5537, 3091, 1010, 1040, 2003, 1996, 6630, 9812, 1010, 1047, 2003, 1996, 16293, 2946, 1997, 9530, 6767, 7630, 9285, 1998, 1054, 1996, 2946, 1997, 1996, 5101, 1999, 7775, 2969, 1011, 3086, 1012, 6741, 2828, 11619, 2566, 6741, 25582, 4555, 4130, 3091, 3136, 2969, 1011, 3086, 1051, 1006, 1050, 2475, 1087, 1040, 1007, 1051, 1006, 1015, 1007, 1051, 1006, 1015, 1007, 28667, 29264, 1051, 1006, 1050, 1087, 1040, 2475, 1007, 1051, 1006, 1050, 1007, 1051, 1006, 1050, 1007, 9530, 6767, 7630, 3508, 2389, 1051, 1006, 1047, 1087, 1050, 1087, 1040, 2475, 1007, 1051, 1006, 1015, 1007, 1051, 1006, 8833, 2243, 1006, 1050, 1007, 1007, 2969, 1011, 3086, 1006, 7775, 1007, 1051, 1006, 1054, 1087, 1050, 1087, 1040, 1007, 1051, 1006, 1015, 1007, 1051, 1006, 1050, 1013, 1054, 1007, 19204, 2015, 1999, 1996, 5537, 1012, 2000, 2023, 2203, 1010, 2057, 5587, 1000, 2597, 2389, 17181, 2015, 1000, 2000, 1996, 7953, 7861, 8270, 4667, 2015, 2012, 1996, 24196, 1997, 1996, 4372, 16044, 2099, 1998, 21933, 4063, 20829, 1012, 1996, 2597, 2389, 17181, 2015, 2031, 1996, 2168, 9812, 1040, 5302, 9247, 2004, 1996, 7861, 8270, 4667, 2015, 1010, 2061, 2008, 1996, 2048, 2064, 2022, 7680, 7583, 1012, 2045, 2024, 2116, 9804, 1997, 2597, 2389, 17181, 2015, 1010, 4342, 1998, 4964, 1031, 1023, 1033, 1012, 1999, 2023, 2147, 1010, 2057, 2224, 8254, 2063, 1998, 2522, 11493, 2063, 4972, 1997, 2367, 13139, 1024, 21877, 1006, 13433, 2015, 1010, 1016, 2072, 1007, 1027, 8254, 1006, 13433, 2015, 1013, 6694, 2692, 1016, 2072, 1013, 1040, 5302, 9247, 1007, 21877, 1006, 13433, 2015, 1010, 1016, 2072, 1009, 1015, 1007, 1027, 2522, 2015, 1006, 13433, 2015, 1013, 6694, 2692, 1016, 2072, 1013, 1040, 5302, 9247, 1007, 2073, 13433, 2015, 2003, 1996, 2597, 1998, 1045, 2003, 1996, 9812, 1012, 2008, 2003, 1010, 2169, 9812, 1997, 1996, 2597, 2389, 17181, 14788, 2000, 1037, 8254, 26658, 3593, 1012, 1996, 29263, 2433, 1037, 14965, 14967, 2013, 1016, 29731, 2000, 6694, 2692, 1087, 1016, 29731, 1012, 2057, 4900, 2023, 3853, 2138, 2057, 1044, 22571, 14573, 2229, 3550, 2009, 2052, 3499, 1996, 2944, 2000, 4089, 4553, 2000, 5463, 2011, 5816, 4460, 1010, 2144, 2005, 2151, 4964, 16396, 1047, 1010, 27233, 2891, 1009, 1047, 2064, 2022, 3421, 2004, 1037, 7399, 3853, 1997, 27233, 2891, 1012, 2057, 2036, 20918, 2007, 2478, 4342, 2597, 2389, 7861, 8270, 4667, 2015, 1031, 1023, 1033, 2612, 1010, 1998, 2179, 2008, 1996, 2048, 4617, 2550, 3053, 7235, 3463, 1006, 2156, 2795, 1017, 5216, 1006, 1041, 1007, 1007, 1012, 2057, 4900, 1996, 8254, 26658, 16975, 2544, 2138, 2009, 2089, 3499, 1996, 2944, 2000, 4469, 18155, 3686, 2000, 5537, 10742, 2936, 2084, 1996, 3924, 8567, 2076, 2731, 1012, 1018, 2339, 2969, 1011, 3086, 1999, 2023, 2930, 2057, 12826, 2536, 5919, 1997, 2969, 1011, 3086, 9014, 2000, 1996, 28667, 29264, 1998, 9530, 6767, 7630, 1011, 14841, 16026, 9014, 4141, 2109, 2005, 12375, 2028, 8023, 1011, 3091, 5537, 1997, 6454, 15066, 1006, 1060, 2487, 1010, 1012, 1012, 1012, 1010, 1060, 2078, 1007, 2000, 2178, 5537, 1997, 5020, 3091, 1006, 1062, 2487, 1010, 1012, 1012, 1012, 1010, 1062, 2078, 1007, 1010, 2007, 8418, 1010, 1062, 2072, 1596, 16428, 1010, 2107, 2004, 1037, 5023, 6741, 1999, 1037, 5171, 5537, 9099, 16256, 4372, 16044, 2099, 2030, 21933, 4063, 1012, 9587, 29068, 5844, 2256, 2224, 1997, 2969, 1011, 3086, 2057, 5136, 2093, 4078, 18688, 6790, 1012, 2028, 2003, 1996, 2561, 15078, 11619, 2566, 6741, 1012, 2178, 2003, 1996, 3815, 1997, 22334, 2008, 2064, 2022, 5903, 3550, 1010, 2004, 7594, 2011, 1996, 6263, 2193, 1997, 25582, 3136, 3223, 1012, 1996, 2353, 2003, 1996, 4130, 3091, 2090, 2146, 1011, 2846, 12530, 15266, 1999, 1996, 2897, 1012, 4083, 2146, 1011, 2846, 12530, 15266, 2003, 1037, 3145, 4119, 1999, 2116, 5537, 9099, 16256, 8518, 1012, 2028, 3145, 5387, 12473, 1996, 3754, 2000, 4553, 2107, 12530, 15266, 2003, 1996, 3091, 1997, 1996, 10425, 2830, 1998, 8848, 7755, 2031, 2000, 20811, 1999, 1996, 2897, 1012, 1996, 7820, 2122, 10425, 2090, 2151, 5257, 1997, 4460, 1999, 1996, 7953, 1998, 6434, 10071, 1010, 1996, 6082, 2009, 2003, 2000, 4553, 2146, 1011, 2846, 12530, 15266, 1031, 2260, 1033, 1012, 6516, 2057, 2036, 12826, 1996, 4555, 4130, 3091, 2090, 2151, 2048, 7953, 1998, 6434, 4460, 1999, 6125, 3605, 1997, 1996, 2367, 6741, 4127, 1012, 2004, 3264, 1999, 2795, 1015, 1010, 1037, 2969, 1011, 3086, 6741, 8539, 2035, 4460, 2007, 1037, 5377, 2193, 1997, 25582, 2135, 6472, 3136, 1010, 6168, 1037, 28667, 29264, 6741, 5942, 1051, 1006, 1050, 1007, 25582, 3136, 1012, 1999, 3408, 1997, 15078, 11619, 1010, 2969, 1011, 3086, 9014, 2024, 5514, 2084, 28667, 29264, 9014, 2043, 1996, 5537, 3091, 1050, 2003, 3760, 2084, 1996, 6630, 8789, 3012, 1040, 1010, 2029, 2003, 2087, 2411, 1996, 2553, 2007, 6251, 15066, 2109, 2011, 2110, 1011, 1997, 1011, 1996, 1011, 2396, 4275, 1999, 3698, 11913, 1010, 2107, 2004, 2773, 1011, 3538, 1031, 4229, 1033, 1998, 24880, 1011, 3940, 1031, 2861, 1033, 15066, 1012, 2000, 5335, 15078, 2836, 2005, 8518, 5994, 2200, 2146, 10071, 1010, 2969, 1011, 3086, 2071, 2022, 7775, 2000, 6195, 2069, 1037, 5101, 1997, 2946, 1054, 1999, 1020, 1996, 7953, 5537, 8857, 2105, 1996, 7972, 6434, 2597, 1012, 2023, 2052, 3623, 1996, 4555, 4130, 3091, 2000, 1051, 1006, 1050, 1013, 1054, 1007, 1012, 2057, 2933, 2000, 8556, 2023, 3921, 2582, 1999, 2925, 2147, 1012, 1037, 2309, 9530, 6767, 7630, 3508, 2389, 6741, 2007, 16293, 9381, 1047, 1026, 1050, 2515, 2025, 7532, 2035, 7689, 1997, 7953, 1998, 6434, 4460, 1012, 2725, 2061, 5942, 1037, 9991, 1997, 1051, 1006, 1050, 1013, 1047, 1007, 9530, 6767, 7630, 3508, 2389, 9014, 1999, 1996, 2553, 1997, 25177, 16293, 2015, 1010, 2030, 1051, 1006, 8833, 2243, 1006, 1050, 1007, 1007, 1999, 1996, 2553, 1997, 29454, 4383, 9530, 6767, 7630, 9285, 1031, 2324, 1033, 1010, 4852, 1996, 3091, 1997, 1996, 6493, 10425, 2090, 2151, 2048, 4460, 1999, 1996, 2897, 1012, 9530, 6767, 7630, 3508, 2389, 9014, 2024, 3227, 2062, 6450, 2084, 28667, 29264, 9014, 1010, 2011, 1037, 5387, 1997, 1047, 1012, 19802, 25236, 9530, 6767, 7630, 9285, 1031, 1020, 1033, 1010, 2174, 1010, 9885, 1996, 11619, 9839, 1010, 2000, 1051, 1006, 1047, 1087, 1050, 1087, 1040, 1009, 1050, 1087, 1040, 2475, 1007, 1012, 2130, 2007, 1047, 1027, 1050, 1010, 2174, 1010, 1996, 11619, 1997, 1037, 19802, 25236, 9530, 6767, 7630, 3508, 2003, 5020, 2000, 1996, 5257, 1997, 1037, 2969, 1011, 3086, 6741, 1998, 1037, 2391, 1011, 7968, 5438, 1011, 2830, 6741, 1010, 1996, 3921, 2057, 2202, 1999, 2256, 2944, 1012, 2004, 2217, 5770, 1010, 2969, 1011, 3086, 2071, 10750, 2062, 17841, 3085, 4275, 1012, 2057, 22459, 3086, 20611, 2013, 2256, 4275, 1998, 2556, 1998, 6848, 4973, 1999, 1996, 22524, 1012, 2025, 2069, 2079, 3265, 3086, 4641, 4415, 4553, 2000, 4685, 2367, 8518, 1010, 2116, 3711, 2000, 8327, 5248, 3141, 2000, 1996, 19962, 2696, 13306, 1998, 21641, 3252, 1997, 1996, 11746, 1012, 1019, 2731, 2023, 2930, 5577, 1996, 2731, 6939, 2005, 2256, 4275, 1012, 1019, 1012, 1015, 2731, 2951, 1998, 14108, 2075, 2057, 4738, 2006, 1996, 3115, 1059, 20492, 2297, 2394, 1011, 2446, 2951, 13462, 5398, 1997, 2055, 1018, 1012, 1019, 2454, 6251, 7689, 1012, 11746, 2020, 12359, 2478, 24880, 1011, 3940, 17181, 1031, 1017, 1033, 1010, 2029, 2038, 1037, 4207, 3120, 1011, 4539, 16188, 1997, 2055, 16444, 8889, 19204, 2015, 1012, 2005, 2394, 1011, 2413, 1010, 2057, 2109, 1996, 6022, 3469, 1059, 20492, 2297, 2394, 1011, 2413, 2951, 13462, 5398, 1997, 4029, 2213, 11746, 1998, 3975, 19204, 2015, 2046, 1037, 13710, 8889, 2773, 1011, 3538, 16188, 1031, 4229, 1033, 1012, 6251, 7689, 2020, 14108, 2098, 2362, 2011, 15796, 5537, 3091, 1012, 2169, 2731, 14108, 4838, 1037, 2275, 1997, 6251, 7689, 4820, 3155, 25108, 2692, 3120, 19204, 2015, 1998, 25108, 2692, 4539, 19204, 2015, 1012, 1019, 1012, 1016, 8051, 1998, 6134, 2057, 4738, 2256, 4275, 2006, 2028, 3698, 2007, 1022, 1050, 17258, 2401, 1052, 18613, 14246, 2271, 1012, 2005, 2256, 2918, 4275, 2478, 1996, 23760, 28689, 22828, 2015, 2649, 2802, 1996, 3259, 1010, 2169, 2731, 3357, 2165, 2055, 1014, 1012, 1018, 3823, 1012, 2057, 4738, 1996, 2918, 4275, 2005, 1037, 2561, 1997, 2531, 1010, 2199, 4084, 2030, 2260, 2847, 1012, 2005, 2256, 2502, 4275, 1010, 1006, 2649, 2006, 1996, 3953, 2240, 1997, 2795, 1017, 1007, 1010, 3357, 2051, 2001, 1015, 1012, 1014, 3823, 1012, 1996, 2502, 4275, 2020, 4738, 2005, 3998, 1010, 2199, 4084, 1006, 1017, 1012, 1019, 2420, 1007, 1012, 1019, 1012, 1017, 23569, 27605, 6290, 2057, 2109, 1996, 4205, 23569, 27605, 6290, 1031, 2322, 1033, 2007, 1156, 2487, 1027, 1014, 1012, 1023, 1010, 1156, 2475, 1027, 1014, 1012, 5818, 1998, 1027, 2184, 22543, 2683, 1012, 2057, 9426, 1996, 4083, 3446, 2058, 1996, 2607, 1997, 2731, 1010, 2429, 2000, 1996, 5675, 1024, 1048, 11657, 1027, 1040, 22543, 2692, 1012, 1019, 5302, 9247, 1087, 8117, 1006, 3357, 1035, 16371, 2213, 1597, 2692, 1012, 1019, 1010, 3357, 1035, 16371, 2213, 1087, 4010, 6279, 1035, 4084, 27944, 1012, 1019, 1007, 1006, 1017, 1007, 2023, 14788, 2000, 4852, 1996, 4083, 3446, 7399, 2135, 2005, 1996, 2034, 4010, 6279, 1035, 4084, 2731, 4084, 1010, 1998, 16922, 2009, 6920, 14267, 2135, 2000, 1996, 19262, 2675, 7117, 1997, 1996, 3357, 2193, 1012, 2057, 2109, 4010, 6279, 1035, 4084, 1027, 20143, 1012, 1019, 1012, 1018, 3180, 3989, 2057, 12666, 2093, 4127, 1997, 3180, 3989, 2076, 2731, 1024, 21961, 4530, 5833, 2057, 6611, 4530, 5833, 1031, 3943, 1033, 2000, 1996, 6434, 1997, 2169, 4942, 1011, 6741, 1010, 2077, 2009, 2003, 2794, 2000, 1996, 4942, 1011, 6741, 7953, 1998, 3671, 3550, 1012, 1999, 2804, 1010, 2057, 6611, 4530, 5833, 2000, 1996, 20571, 1997, 1996, 7861, 8270, 4667, 2015, 1998, 1996, 2597, 2389, 17181, 2015, 1999, 2119, 1996, 4372, 16044, 2099, 1998, 21933, 4063, 20829, 1012, 2005, 1996, 2918, 2944, 1010, 2057, 2224, 1037, 3446, 1997, 22851, 18981, 1027, 1014, 1012, 1015, 1012, 1021, 2795, 1016, 1024, 1996, 10938, 2121, 6162, 2015, 2488, 1038, 2571, 2226, 7644, 2084, 3025, 2110, 1011, 1997, 1011, 1996, 1011, 2396, 4275, 2006, 1996, 2394, 1011, 2000, 1011, 2446, 1998, 2394, 1011, 2000, 1011, 2413, 2739, 22199, 11387, 16932, 5852, 2012, 1037, 12884, 1997, 1996, 2731, 3465, 1012, 2944, 1038, 2571, 2226, 2731, 3465, 1006, 28583, 2015, 1007, 4372, 1011, 2139, 4372, 1011, 10424, 4372, 1011, 2139, 4372, 1011, 10424, 24880, 7159, 1031, 2324, 1033, 2603, 1012, 4293, 2784, 1011, 2012, 2102, 1009, 13433, 19729, 2243, 1031, 4464, 1033, 4464, 1012, 1016, 1015, 1012, 1014, 1087, 9402, 2692, 1043, 2078, 20492, 1009, 1054, 2140, 1031, 4229, 1033, 2484, 1012, 1020, 4464, 1012, 6227, 1016, 1012, 1017, 1087, 7886, 2683, 1015, 1012, 1018, 1087, 9402, 2692, 9530, 15088, 2475, 2015, 1031, 1023, 1033, 2423, 1012, 2385, 2871, 1012, 4805, 1023, 1012, 1020, 1087, 7886, 2620, 1015, 1012, 1019, 1087, 9402, 2692, 22078, 1031, 3590, 1033, 2656, 1012, 6021, 2871, 1012, 5179, 1016, 1012, 1014, 1087, 7886, 2683, 1015, 1012, 1016, 1087, 9402, 2692, 2784, 1011, 2012, 2102, 1009, 13433, 19729, 2243, 7241, 1031, 4464, 1033, 2871, 1012, 1018, 1022, 1012, 1014, 1087, 9402, 2692, 1043, 2078, 20492, 1009, 1054, 2140, 7241, 1031, 4229, 1033, 2656, 1012, 2382, 4601, 1012, 2385, 1015, 1012, 1022, 1087, 9402, 2692, 1015, 1012, 1015, 1087, 9402, 2487, 9530, 15088, 2475, 2015, 7241, 1031, 1023, 1033, 2656, 1012, 4029, 4601, 1012, 2756, 1021, 1012, 1021, 1087, 7886, 2683, 1015, 1012, 1016, 1087, 9402, 2487, 10938, 2121, 1006, 2918, 2944, 1007, 2676, 1012, 1017, 4229, 1012, 1015, 1017, 1012, 1017, 1087, 7886, 2620, 10938, 2121, 1006, 2502, 1007, 2654, 1012, 1018, 4601, 1012, 1022, 1016, 1012, 1017, 1087, 7886, 2683, 3830, 27045, 2076, 2731, 1010, 2057, 4846, 3830, 27045, 1997, 3643, 1048, 2015, 1027, 1014, 1012, 1015, 1031, 4029, 1033, 1012, 2023, 13403, 2566, 19386, 3012, 1010, 2004, 1996, 2944, 10229, 2000, 2022, 2062, 12422, 1010, 2021, 24840, 10640, 1998, 1038, 2571, 2226, 3556, 1012, 1020, 3463, 1020, 1012, 1015, 3698, 5449, 2006, 1996, 1059, 20492, 2297, 2394, 1011, 2000, 1011, 2446, 5449, 4708, 1010, 1996, 2502, 10938, 2121, 2944, 1006, 10938, 2121, 1006, 2502, 1007, 1999, 2795, 1016, 1007, 2041, 4842, 22694, 1996, 2190, 3130, 2988, 4275, 1006, 2164, 21528, 1007, 2011, 2062, 2084, 1016, 1012, 1014, 1038, 2571, 2226, 1010, 7411, 1037, 2047, 2110, 1011, 1997, 1011, 1996, 1011, 2396, 1038, 2571, 2226, 3556, 1997, 2654, 1012, 1018, 1012, 1996, 9563, 1997, 2023, 2944, 2003, 3205, 1999, 1996, 3953, 2240, 1997, 2795, 1017, 1012, 2731, 2165, 1017, 1012, 1019, 2420, 2006, 1022, 1052, 18613, 14246, 2271, 1012, 2130, 2256, 2918, 2944, 7505, 15194, 2229, 2035, 3130, 2405, 4275, 1998, 21528, 1010, 2012, 1037, 12884, 1997, 1996, 2731, 3465, 1997, 2151, 1997, 1996, 6975, 4275, 1012, 2006, 1996, 1059, 20492, 2297, 2394, 1011, 2000, 1011, 2413, 5449, 4708, 1010, 2256, 2502, 2944, 6162, 2015, 1037, 1038, 2571, 2226, 3556, 1997, 4601, 1012, 1014, 1010, 2041, 4842, 14192, 2075, 2035, 1997, 1996, 3130, 2405, 2309, 4275, 1010, 2012, 2625, 2084, 1015, 1013, 1018, 1996, 2731, 3465, 1997, 1996, 3025, 2110, 1011, 1997, 1011, 1996, 1011, 2396, 2944, 1012, 1996, 10938, 2121, 1006, 2502, 1007, 2944, 4738, 2005, 2394, 1011, 2000, 1011, 2413, 2109, 4530, 5833, 3446, 22851, 18981, 1027, 1014, 1012, 1015, 1010, 2612, 1997, 1014, 1012, 1017, 1012, 2005, 1996, 2918, 4275, 1010, 2057, 2109, 1037, 2309, 2944, 4663, 2011, 14985, 1996, 2197, 1019, 26520, 2015, 1010, 2029, 2020, 2517, 2012, 2184, 1011, 3371, 14025, 1012, 2005, 1996, 2502, 4275, 1010, 2057, 11398, 1996, 2197, 2322, 26520, 2015, 1012, 2057, 2109, 7504, 3945, 2007, 1037, 7504, 2946, 1997, 1018, 1998, 3091, 6531, 1155, 1027, 1014, 1012, 1020, 1031, 4229, 1033, 1012, 2122, 23760, 28689, 22828, 2015, 2020, 4217, 2044, 21470, 2006, 1996, 2458, 2275, 1012, 2057, 2275, 1996, 4555, 6434, 3091, 2076, 28937, 2000, 7953, 3091, 1009, 2753, 1010, 2021, 20320, 2220, 2043, 2825, 1031, 4229, 1033, 1012, 2795, 1016, 7680, 7849, 10057, 2256, 3463, 1998, 22963, 2256, 5449, 3737, 1998, 2731, 5366, 2000, 2060, 2944, 4294, 2015, 2013, 1996, 3906, 1012, 2057, 10197, 1996, 2193, 1997, 8274, 2391, 3136, 2109, 2000, 3345, 1037, 2944, 2011, 4800, 22086, 2075, 1996, 2731, 2051, 1010, 1996, 2193, 1997, 14246, 2271, 2109, 1010, 1998, 2019, 10197, 1997, 1996, 8760, 2309, 1011, 11718, 8274, 1011, 2391, 3977, 1997, 2169, 14246, 2226, 1019, 1012, 1020, 1012, 1016, 2944, 8358, 2000, 16157, 1996, 5197, 1997, 2367, 6177, 1997, 1996, 10938, 2121, 1010, 2057, 9426, 2256, 2918, 2944, 1999, 2367, 3971, 1010, 9854, 1996, 2689, 1999, 2836, 2006, 2394, 1011, 2000, 1011, 2446, 5449, 2006, 1996, 2458, 2275, 1010, 2739, 22199, 11387, 17134, 1012, 2057, 2109, 7504, 3945, 2004, 2649, 1999, 1996, 3025, 2930, 1010, 2021, 2053, 26520, 14985, 1012, 2057, 2556, 2122, 3463, 1999, 2795, 1017, 1012, 1999, 2795, 1017, 10281, 1006, 1037, 1007, 1010, 2057, 8137, 1996, 2193, 1997, 3086, 4641, 1998, 1996, 3086, 3145, 1998, 3643, 9646, 1010, 4363, 1996, 3815, 1997, 22334, 5377, 1010, 2004, 2649, 1999, 2930, 1017, 1012, 1016, 1012, 1016, 1012, 2096, 2309, 1011, 2132, 3086, 2003, 1014, 1012, 1023, 1038, 2571, 2226, 4788, 2084, 1996, 2190, 4292, 1010, 3737, 2036, 9010, 2125, 2007, 2205, 2116, 4641, 1012, 1019, 8545, 2109, 5300, 1997, 1016, 1012, 1022, 1010, 1017, 1012, 1021, 1010, 1020, 1012, 1014, 1998, 1023, 1012, 1019, 1056, 10258, 11923, 2005, 1047, 17914, 1010, 1047, 12740, 1010, 1049, 12740, 1998, 1052, 18613, 1010, 4414, 1012, 1022, 2795, 1017, 1024, 8358, 2006, 1996, 10938, 2121, 4294, 1012, 4895, 9863, 2098, 5300, 2024, 7235, 2000, 2216, 1997, 1996, 2918, 2944, 1012, 2035, 12046, 2015, 2024, 2006, 1996, 2394, 1011, 2000, 1011, 2446, 5449, 2458, 2275, 1010, 2739, 22199, 11387, 17134, 1012, 3205, 2566, 19386, 6447, 2024, 2566, 1011, 2773, 11198, 1010, 2429, 2000, 2256, 24880, 1011, 3940, 17181, 1010, 1998, 2323, 2025, 2022, 4102, 2000, 2566, 1011, 2773, 2566, 19386, 6447, 1012, 1050, 1040, 5302, 9247, 1040, 4246, 1044, 1040, 2243, 1040, 2615, 22851, 18981, 1048, 2015, 3345, 4903, 2140, 1038, 2571, 2226, 11498, 5244, 4084, 1006, 16475, 1007, 1006, 16475, 1007, 1095, 10790, 2575, 2918, 1020, 24406, 19627, 2620, 1022, 4185, 4185, 1014, 1012, 1015, 1014, 1012, 1015, 2531, 2243, 1018, 1012, 6227, 2423, 1012, 1022, 3515, 1006, 1037, 1007, 1015, 24406, 24406, 1019, 1012, 2756, 2484, 1012, 1023, 1018, 11899, 11899, 1019, 1012, 4002, 2423, 1012, 1019, 2385, 3590, 3590, 1018, 1012, 6205, 2423, 1012, 1022, 3590, 2385, 2385, 1019, 1012, 5890, 2423, 1012, 1018, 1006, 1038, 1007, 2385, 1019, 1012, 2385, 2423, 1012, 1015, 5388, 3590, 1019, 1012, 5890, 2423, 1012, 1018, 3438, 1006, 1039, 1007, 1016, 1020, 1012, 2340, 2603, 1012, 1021, 4029, 1018, 1019, 1012, 2539, 2423, 1012, 1017, 2753, 1022, 1018, 1012, 6070, 2423, 1012, 1019, 3770, 17273, 3590, 3590, 1019, 1012, 4293, 2484, 1012, 1019, 2654, 9402, 2549, 11899, 11899, 1018, 1012, 5764, 2656, 1012, 1014, 16923, 9402, 2549, 1019, 1012, 2260, 2423, 1012, 1018, 5187, 2871, 2683, 2575, 1018, 1012, 4293, 2656, 1012, 1016, 3938, 1006, 1040, 1007, 1014, 1012, 1014, 1019, 1012, 6255, 2484, 1012, 1020, 1014, 1012, 1016, 1018, 1012, 5345, 2423, 1012, 1019, 1014, 1012, 1014, 1018, 1012, 6163, 2423, 1012, 1017, 1014, 1012, 1016, 1019, 1012, 4700, 2423, 1012, 1021, 1006, 1041, 1007, 2597, 2389, 7861, 8270, 4667, 2612, 1997, 8254, 26658, 9821, 1018, 1012, 6227, 2423, 1012, 1021, 2502, 1020, 9402, 2549, 2871, 2683, 2575, 2385, 1014, 1012, 1017, 3998, 2243, 1018, 1012, 3943, 2656, 1012, 1018, 19883, 2795, 1018, 1024, 1996, 10938, 2121, 2236, 10057, 2092, 2000, 2394, 5540, 11968, 7741, 1006, 3463, 2024, 2006, 2930, 2603, 1997, 1059, 2015, 3501, 1007, 11968, 8043, 2731, 1059, 2015, 3501, 2603, 20069, 19354, 21095, 2015, 1004, 15676, 3449, 2632, 1012, 1006, 2297, 1007, 1031, 4261, 1033, 1059, 2015, 3501, 2069, 1010, 5860, 20026, 3981, 6024, 6070, 1012, 1017, 9004, 12298, 3802, 2632, 1012, 1006, 2294, 1007, 1031, 2756, 1033, 1059, 2015, 3501, 2069, 1010, 5860, 20026, 3981, 6024, 3938, 1012, 1018, 15503, 3802, 2632, 1012, 1006, 2286, 1007, 1031, 2871, 1033, 1059, 2015, 3501, 2069, 1010, 5860, 20026, 3981, 6024, 3938, 1012, 1018, 23494, 3802, 2632, 1012, 1006, 2355, 1007, 1031, 1022, 1033, 1059, 2015, 3501, 2069, 1010, 5860, 20026, 3981, 6024, 6205, 1012, 1021, 10938, 2121, 1006, 1018, 9014, 1007, 1059, 2015, 3501, 2069, 1010, 5860, 20026, 3981, 6024, 6205, 1012, 1017, 15503, 3802, 2632, 1012, 1006, 2286, 1007, 1031, 2871, 1033, 4100, 1011, 13588, 6205, 1012, 1017, 15469, 1004, 8500, 1006, 2268, 1007, 1031, 2403, 1033, 4100, 1011, 13588, 6205, 1012, 1017, 23680, 10483, 4801, 3802, 2632, 1012, 1006, 2294, 1007, 1031, 2656, 1033, 4100, 1011, 13588, 6227, 1012, 1015, 19354, 21095, 2015, 1004, 15676, 3449, 2632, 1012, 1006, 2297, 1007, 1031, 4261, 1033, 4100, 1011, 13588, 6227, 1012, 1015, 10938, 2121, 1006, 1018, 9014, 1007, 4100, 1011, 13588, 6227, 1012, 1021, 11320, 5063, 3802, 2632, 1012, 1006, 2325, 1007, 1031, 2603, 1033, 4800, 1011, 4708, 6109, 1012, 1014, 23494, 3802, 2632, 1012, 1006, 2355, 1007, 1031, 1022, 1033, 11416, 6024, 6109, 1012, 1017, 1999, 2795, 1017, 10281, 1006, 1038, 1007, 1010, 2057, 11949, 2008, 8161, 1996, 3086, 3145, 2946, 1040, 2243, 13403, 2944, 3737, 1012, 2023, 6083, 2008, 12515, 21778, 2003, 2025, 3733, 1998, 2008, 1037, 2062, 12138, 21778, 3853, 2084, 11089, 4031, 2089, 2022, 15189, 1012, 2057, 2582, 11949, 1999, 10281, 1006, 1039, 1007, 1998, 1006, 1040, 1007, 2008, 1010, 2004, 3517, 1010, 7046, 4275, 2024, 2488, 1010, 1998, 4530, 5833, 2003, 2200, 14044, 1999, 9992, 2058, 1011, 11414, 1012, 1999, 5216, 1006, 1041, 1007, 2057, 5672, 2256, 8254, 26658, 16975, 2597, 2389, 17181, 2007, 4342, 2597, 2389, 7861, 8270, 4667, 2015, 1031, 1023, 1033, 1010, 1998, 11949, 3053, 7235, 3463, 2000, 1996, 2918, 2944, 1012, 1020, 1012, 1017, 2394, 5540, 11968, 7741, 2000, 16157, 2065, 1996, 10938, 2121, 2064, 2236, 4697, 2000, 2060, 8518, 2057, 2864, 7885, 2006, 2394, 5540, 11968, 7741, 1012, 2023, 4708, 7534, 3563, 7860, 1024, 1996, 6434, 2003, 3395, 2000, 2844, 8332, 1023, 14679, 1998, 2003, 6022, 2936, 2084, 1996, 7953, 1012, 7297, 1010, 29300, 2078, 5537, 1011, 2000, 1011, 5537, 4275, 2031, 2025, 2042, 2583, 2000, 18759, 2110, 1011, 1997, 1011, 1996, 1011, 2396, 3463, 1999, 2235, 1011, 2951, 25228, 1031, 4261, 1033, 1012, 2057, 4738, 1037, 1018, 1011, 6741, 10938, 2121, 2007, 1040, 5302, 9247, 1027, 9402, 2549, 2006, 1996, 2813, 2395, 3485, 1006, 1059, 2015, 3501, 1007, 4664, 1997, 1996, 9502, 3392, 9299, 1031, 2423, 1033, 1010, 2055, 2871, 2243, 2731, 11746, 1012, 2057, 2036, 4738, 2009, 1999, 1037, 4100, 1011, 13588, 4292, 1010, 2478, 1996, 3469, 2152, 1011, 7023, 1998, 2022, 8024, 3051, 19362, 8043, 13058, 6525, 2013, 2007, 3155, 2459, 2213, 11746, 1031, 4261, 1033, 1012, 2057, 2109, 1037, 16188, 1997, 2385, 2243, 19204, 2015, 2005, 1996, 1059, 2015, 3501, 2069, 4292, 1998, 1037, 16188, 1997, 3590, 2243, 19204, 2015, 2005, 1996, 4100, 1011, 13588, 4292, 1012, 2057, 2864, 2069, 1037, 2235, 2193, 1997, 7885, 2000, 7276, 1996, 4530, 5833, 1010, 2119, 3086, 1998, 21961, 1006, 2930, 1019, 1012, 1018, 1007, 1010, 4083, 6165, 1998, 7504, 2946, 2006, 1996, 2930, 2570, 2458, 2275, 1010, 2035, 2060, 11709, 2815, 15704, 2013, 1996, 2394, 1011, 2000, 1011, 2446, 2918, 5449, 2944, 1012, 2076, 28937, 1010, 2057, 3445, 1996, 4555, 6434, 3091, 2000, 7953, 3091, 1009, 3998, 1012, 2057, 2109, 1037, 7504, 2946, 1997, 2538, 1998, 1155, 1027, 1014, 1012, 1017, 2005, 2119, 1059, 2015, 3501, 2069, 1998, 1996, 4100, 1011, 13588, 4292, 1012, 2256, 3463, 1999, 2795, 1018, 2265, 2008, 2750, 1996, 3768, 1997, 4708, 1011, 3563, 17372, 2256, 2944, 10438, 7505, 1011, 26927, 7741, 2135, 2092, 1010, 21336, 2488, 3463, 2084, 2035, 3130, 2988, 4275, 2007, 1996, 6453, 1997, 1996, 28667, 29264, 15756, 2897, 8035, 1031, 1022, 1033, 1012, 1999, 5688, 2000, 29300, 2078, 5537, 1011, 2000, 1011, 5537, 4275, 1031, 4261, 1033, 1010, 1996, 10938, 2121, 2041, 4842, 22694, 1996, 8256, 1011, 11968, 8043, 1031, 2756, 1033, 2130, 2043, 2731, 2069, 2006, 1996, 1059, 2015, 3501, 2731, 2275, 1997, 2871, 2243, 11746, 1012, 1021, 7091, 1999, 2023, 2147, 1010, 2057, 3591, 1996, 10938, 2121, 1010, 1996, 2034, 5537, 9099, 16256, 2944, 2241, 4498, 2006, 3086, 1010, 6419, 1996, 28667, 29264, 9014, 2087, 4141, 2109, 1999, 4372, 16044, 2099, 1011, 21933, 4063, 4294, 2015, 2007, 4800, 1011, 3753, 2969, 1011, 3086, 1012, 2005, 5449, 8518, 1010, 1996, 10938, 2121, 2064, 2022, 4738, 6022, 5514, 2084, 4294, 2015, 2241, 2006, 28667, 29264, 2030, 9530, 6767, 7630, 3508, 2389, 9014, 1012, 2006, 2119, 1059, 20492, 2297, 2394, 1011, 2000, 1011, 2446, 1998, 1059, 20492, 2297, 2394, 1011, 2000, 1011, 2413, 5449, 8518, 1010, 2057, 6162, 1037, 2047, 2110, 1997, 1996, 2396, 1012, 1999, 1996, 2280, 4708, 2256, 2190, 2944, 2041, 4842, 22694, 2130, 2035, 3130, 2988, 21528, 1012, 2057, 2024, 7568, 2055, 1996, 2925, 1997, 3086, 1011, 2241, 4275, 1998, 2933, 2000, 6611, 2068, 2000, 2060, 8518, 1012, 2057, 2933, 2000, 7949, 1996, 10938, 2121, 2000, 3471, 5994, 7953, 1998, 6434, 16913, 11475, 7368, 2060, 2084, 3793, 1998, 2000, 8556, 2334, 1010, 7775, 3086, 10595, 2000, 18228, 5047, 2312, 20407, 1998, 27852, 2107, 2004, 4871, 1010, 5746, 1998, 2678, 1012, 2437, 4245, 2625, 25582, 2003, 2178, 2470, 3289, 1997, 14635, 1012, 1996, 3642, 2057, 2109, 2000, 3345, 1998, 16157, 2256, 4275, 2003, 2800, 2012, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 2475, 25808, 2953, 1012, 13399, 8163, 2057, 2024, 8794, 2000, 6583, 2140, 10556, 29358, 13578, 10087, 2099, 1998, 15963, 2175, 25974, 2015, 2005, 2037, 5909, 3993, 7928, 1010, 20983, 1998, 7780, 1012, 7604, 1031, 1015, 1033, 5261, 26947, 8670, 1010, 6175, 4575, 11382, 7352, 1010, 1998, 11023, 1041, 9374, 2239, 1012, 6741, 3671, 3989, 1012, 12098, 9048, 2615, 17463, 6657, 2102, 12098, 9048, 2615, 1024, 8148, 2581, 1012, 5757, 19961, 2692, 1010, 2355, 1012, 1031, 1016, 1033, 1040, 2480, 22930, 2854, 8670, 14945, 5162, 2226, 1010, 18712, 5575, 10536, 4609, 16480, 1010, 1998, 10930, 14235, 2050, 3841, 11411, 1012, 15756, 3698, 5449, 2011, 10776, 4083, 2000, 25705, 1998, 17637, 1012, 2522, 12171, 1010, 14689, 1013, 8574, 2683, 1012, 5840, 2581, 2509, 1010, 2297, 1012, 1031, 1017, 1033, 14465, 28101, 2480, 1010, 4698, 2751, 2666, 1010, 19538, 1011, 2084, 2290, 11320, 5063, 1010, 1998, 22035, 2278, 1058, 1012, 3393, 1012, 5294, 8993, 1997, 15756, 3698, 5449, 4294, 2015, 1012, 2522, 12171, 1010, 14689, 1013, 28366, 1012, 6021, 21057, 2575, 1010, 2418, 1012, 1031, 1018, 1033, 29214, 11837, 2290, 15898, 1010, 5622, 11947, 1010, 1998, 14719, 8411, 5001, 6790, 1012, 2146, 2460, 1011, 2744, 3638, 1011, 6125, 2005, 3698, 3752, 1012, 12098, 9048, 2615, 17463, 6657, 2102, 12098, 9048, 2615, 1024, 8148, 2487, 1012, 5757, 2581, 22394, 1010, 2355, 1012, 1031, 1019, 1033, 18712, 5575, 10536, 4609, 16480, 1010, 12075, 3158, 21442, 23144, 5092, 2121, 1010, 6187, 23296, 2906, 19739, 23314, 28362, 1010, 10768, 15222, 8945, 16377, 6072, 1010, 7570, 28875, 2099, 8040, 2232, 12449, 2243, 1010, 1998, 10930, 14235, 2050, 3841, 11411, 1012, 4083, 7655, 15066, 2478, 29300, 2078, 4372, 16044, 2099, 1011, 21933, 4063, 2005, 7778, 3698, 5449, 1012, 2522, 12171, 1010, 14689, 1013, 8574, 2575, 1012, 10550, 2620, 1010, 2297, 1012, 1031, 1020, 1033, 8173, 16480, 22592, 1012, 1060, 24422, 1024, 2784, 4083, 2007, 5995, 14244, 19802, 25236, 9530, 6767, 7630, 9285, 1012, 12098, 9048, 2615, 17463, 6657, 2102, 12098, 9048, 2615, 1024, 25800, 1012, 6185, 19481, 2581, 1010, 2355, 1012, 2184, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 2475, 25808, 2953, 16770, 1024, 1013, 1013, 21025, 2705, 12083, 1012, 4012, 1013, 23435, 12314, 1013, 23435, 2475, 25808, 2953, 8299, 1024, 1013, 1013, 12098, 9048, 2615, 1012, 8917, 1013, 14689, 1013, 8148, 2581, 1012, 5757, 19961, 2692, 8299, 1024, 1013, 1013, 12098, 9048, 2615, 1012, 8917, 1013, 14689, 1013, 8148, 2487, 1012, 5757, 2581, 22394, 8299, 1024, 1013, 1013, 12098, 9048, 2615, 1012, 8917, 1013, 14689, 1013, 25800, 1012, 6185, 19481, 2581, 1031, 1021, 1033, 12022, 29337, 3070, 15972, 1010, 6187, 23296, 2906, 19739, 23314, 28362, 1010, 18712, 5575, 10536, 4609, 16480, 1010, 1998, 10930, 14235, 2050, 3841, 11411, 1012, 17537, 9312, 1997, 4796, 2094, 28667, 29264, 15756, 6125, 2006, 5537, 11643, 1012, 2522, 12171, 1010, 14689, 1013, 15471, 2475, 1012, 26271, 2629, 1010, 2297, 1012, 1031, 1022, 1033, 3782, 23494, 1010, 4748, 4048, 12734, 2050, 28919, 27108, 2080, 1010, 8374, 3608, 20367, 2891, 1010, 1998, 7240, 1037, 1012, 3044, 1012, 28667, 29264, 15756, 2897, 8035, 2015, 1012, 1999, 4013, 2278, 1012, 1997, 6583, 6305, 2140, 1010, 2355, 1012, 1031, 1023, 1033, 10680, 16216, 26378, 3070, 1010, 2745, 8740, 3669, 1010, 2585, 12604, 11239, 2099, 1010, 11064, 8038, 8609, 2015, 1010, 1998, 13619, 2078, 1050, 1012, 29102, 1012, 9530, 6767, 7630, 1011, 14841, 16026, 5537, 2000, 5537, 4083, 1012, 12098, 9048, 2615, 17463, 6657, 2102, 12098, 9048, 2615, 1024, 29326, 1012, 6021, 12521, 2475, 2615, 2475, 1010, 2418, 1012, 1031, 2184, 1033, 4074, 9729, 1012, 11717, 10071, 2007, 28667, 29264, 15756, 6125, 1012, 12098, 9048, 2615, 17463, 6657, 2102, 12098, 9048, 2615, 1024, 7558, 2620, 1012, 5511, 12376, 1010, 2286, 1012, 1031, 2340, 1033, 11928, 6562, 2002, 1010, 27735, 10513, 9327, 1010, 21146, 2080, 19784, 14916, 1010, 1998, 29214, 3103, 1012, 2784, 21961, 4083, 2005, 10047, 1011, 2287, 5038, 1012, 1999, 8931, 1997, 1996, 15368, 3034, 2006, 3274, 4432, 1998, 5418, 5038, 1010, 5530, 29065, 1516, 6255, 2620, 1010, 2355, 1012, 1031, 2260, 1033, 19802, 2361, 21929, 28362, 21646, 1010, 10930, 14235, 2050, 3841, 11411, 1010, 14174, 25312, 9363, 3490, 1010, 1998, 23171, 8040, 26837, 16425, 21436, 1012, 17978, 4834, 1999, 28667, 29264, 16996, 1024, 1996, 7669, 1997, 4083, 2146, 1011, 2744, 12530, 15266, 1010, 2541, 1012, 1031, 2410, 1033, 19802, 2361, 21929, 28362, 21646, 1998, 23171, 8040, 26837, 16425, 21436, 1012, 2146, 2460, 1011, 2744, 3638, 1012, 15756, 22334, 1010, 1023, 1006, 1022, 1007, 1024, 26063, 1516, 15051, 1010, 2722, 1012, 1031, 2403, 1033, 1062, 19991, 14702, 5654, 15469, 1998, 2984, 8500, 1012, 2969, 1011, 2731, 7473, 2546, 2290, 8035, 2015, 2007, 2397, 3372, 5754, 17287, 9285, 2408, 4155, 1012, 1999, 8931, 1997, 1996, 2268, 3034, 2006, 17537, 4725, 1999, 3019, 2653, 6364, 1010, 5530, 6640, 2475, 1516, 6391, 2487, 1012, 9353, 2140, 1010, 2257, 2268, 1012, 1031, 2321, 1033, 7148, 2389, 23258, 5004, 18682, 1010, 2030, 20282, 19354, 21095, 2015, 1010, 3505, 24253, 1010, 2053, 3286, 21146, 23940, 2099, 1010, 1998, 18999, 20552, 8814, 1012, 11131, 1996, 6537, 1997, 2653, 11643, 1012, 12098, 9048, 2615, 17463, 6657, 2102, 12098, 9048, 2615, 1024, 8148, 2475, 1012, 6185, 23632, 2692, 1010, 2355, 1012, 1031, 2385, 1033, 1105, 15750, 17112, 15676, 1998, 3520, 2100, 3841, 11411, 1012, 2064, 3161, 3638, 5672, 3086, 1029, 1999, 9849, 1999, 15756, 2592, 6364, 3001, 1010, 1006, 9152, 4523, 1007, 1010, 2355, 1012, 1031, 2459, 1033, 1105, 15750, 17112, 15676, 1998, 6335, 3148, 10514, 29064, 22507, 1012, 15756, 14246, 2271, 4553, 13792, 1012, 1999, 2248, 3034, 2006, 4083, 15066, 1006, 24582, 20974, 1007, 1010, 2355, 1012, 1031, 2324, 1033, 6583, 2140, 10556, 29358, 13578, 10087, 2099, 1010, 27333, 2063, 9686, 5051, 14854, 2102, 1010, 8129, 4079, 7054, 1010, 7158, 3158, 7939, 1051, 8551, 1010, 4074, 9729, 1010, 1998, 12849, 1011, 4097, 10556, 19722, 2243, 10841, 8649, 7630, 1012, 15756, 3698, 5449, 1999, 7399, 2051, 1012, 12098, 9048, 2615, 17463, 6657, 2102, 12098, 9048, 2615, 1024, 25800, 1012, 2531, 2683, 2683, 2615, 2475, 1010, 2418, 1012, 1031, 2539, 1033, 24863, 5035, 1010, 5529, 23906, 1010, 11320, 5063, 7570, 5654, 1010, 1998, 3656, 1049, 1012, 5481, 1012, 14336, 3086, 6125, 1012, 1999, 2248, 3034, 2006, 4083, 15066, 1010, 2418, 1012, 1031, 2322, 1033, 2351, 27350, 2332, 2863, 1998, 5261, 8670, 1012, 4205, 1024, 1037, 4118, 2005, 2358, 11663, 20875, 20600, 1012, 1999, 24582, 20974, 1010, 2325, 1012, 1031, 2538, 1033, 15589, 5705, 6137, 13970, 24925, 6777, 1998, 11235, 18353, 9695, 1012, 5387, 3989, 12225, 2005, 1048, 3367, 2213, 6125, 1012, 12098, 9048, 2615, 17463, 6657, 2102, 12098, 9048, 2615, 1024, 28366, 1012, 10550, 19317, 1010, 2418, 1012, 1031, 2570, 1033, 14367, 4819, 11409, 1010, 8117, 19845, 19004, 1010, 23080, 2053, 9077, 7895, 9998, 11053, 1010, 9587, 9805, 1010, 17620, 27735, 1010, 16208, 14367, 1010, 1998, 10930, 14235, 2050, 3841, 11411, 1012, 1037, 14336, 2969, 1011, 2012, 6528, 6024, 6251, 7861, 8270, 4667, 1012, 12098, 9048, 2615, 17463, 6657, 2102, 12098, 9048, 2615, 1024, 28366, 1012, 6021, 17134, 2692, 1010, 2418, 1012, 1031, 2603, 1033, 19538, 1011, 2084, 2290, 11320, 5063, 1010, 22035, 2278, 1058, 1012, 3393, 1010, 6335, 3148, 10514, 29064, 22507, 1010, 2030, 20282, 19354, 21095, 2015, 1010, 1998, 23739, 2480, 15676, 1012, 4800, 1011, 4708, 5537, 2000, 5537, 4083, 1012, 12098, 9048, 2615, 17463, 6657, 2102, 12098, 9048, 2615, 1024, 16528, 2487, 1012, 5757, 14526, 2549, 1010, 2325, 1012, 1031, 2484, 1033, 19538, 1011, 2084, 2290, 11320, 5063, 1010, 7632, 13765, 6887, 3286, 1010, 1998, 5696, 1040, 11956, 1012, 4621, 8107, 2000, 3086, 1011, 2241, 15756, 3698, 5449, 1012, 12098, 9048, 2615, 17463, 6657, 2102, 12098, 9048, 2615, 1024, 5018, 2620, 1012, 5840, 2692, 17788, 1010, 2325, 1012, 1031, 2423, 1033, 6395, 1052, 6647, 1010, 2984, 5754, 7871, 19839, 2666, 21355, 1010, 1998, 14807, 11685, 22612, 1012, 2311, 1037, 2312, 5754, 17287, 3064, 13931, 1997, 2394, 1024, 1996, 9502, 3392, 9299, 1012, 15078, 15397, 1010, 2539, 1006, 1016, 1007, 1024, 22997, 1516, 14210, 1010, 2857, 1012, 1031, 2656, 1033, 2585, 23680, 10483, 4801, 1010, 8207, 25869, 6200, 2243, 1010, 1998, 2928, 3779, 1012, 4621, 2969, 1011, 2731, 2005, 11968, 7741, 1012, 1999, 8931, 1997, 1996, 2529, 2653, 2974, 3034, 1997, 1996, 6583, 6305, 2140, 1010, 2364, 3034, 1010, 5530, 15017, 1516, 18914, 1012, 9353, 2140, 1010, 2238, 2294, 1012, 2340, 8299, 1024, 1013, 1013, 12098, 9048, 2615, 1012, 8917, 1013, 14689, 1013, 29326, 1012, 6021, 12521, 2475, 8299, 1024, 1013, 1013, 12098, 9048, 2615, 1012, 8917, 1013, 14689, 1013, 7558, 2620, 1012, 5511, 12376, 8299, 1024, 1013, 1013, 12098, 9048, 2615, 1012, 8917, 1013, 14689, 1013, 8148, 2475, 1012, 6185, 23632, 2692, 8299, 1024, 1013, 1013, 12098, 9048, 2615, 1012, 8917, 1013, 14689, 1013, 25800, 1012, 2531, 2683, 2683, 8299, 1024, 1013, 1013, 12098, 9048, 2615, 1012, 8917, 1013, 14689, 1013, 28366, 1012, 10550, 19317, 8299, 1024, 1013, 1013, 12098, 9048, 2615, 1012, 8917, 1013, 14689, 1013, 28366, 1012, 6021, 17134, 2692, 8299, 1024, 1013, 1013, 12098, 9048, 2615, 1012, 8917, 1013, 14689, 1013, 16528, 2487, 1012, 5757, 14526, 2549, 8299, 1024, 1013, 1013, 12098, 9048, 2615, 1012, 8917, 1013, 14689, 1013, 5018, 2620, 1012, 5840, 2692, 17788, 1031, 2676, 1033, 2019, 18569, 11968, 28209, 1010, 7436, 26997, 15687, 1010, 16510, 2319, 8405, 8695, 1010, 1998, 19108, 2149, 29002, 2890, 4183, 1012, 1037, 21933, 8737, 8820, 3468, 3086, 2944, 1012, 1999, 17537, 4725, 1999, 3019, 2653, 6364, 1010, 2355, 1012, 1031, 2654, 1033, 12836, 2378, 2703, 2271, 1010, 29080, 6562, 8418, 5063, 1010, 1998, 2957, 27084, 5886, 1012, 1037, 2784, 11013, 2944, 2005, 10061, 3512, 7680, 7849, 3989, 1012, 12098, 9048, 2615, 17463, 6657, 2102, 12098, 9048, 2615, 1024, 29326, 1012, 5840, 14142, 2549, 1010, 2418, 1012, 1031, 2756, 1033, 22889, 11431, 9004, 12298, 1010, 6506, 12712, 1010, 12836, 2378, 16215, 18410, 5602, 1010, 1998, 4907, 12555, 1012, 4083, 8321, 1010, 9233, 1010, 1998, 17841, 3085, 3392, 5754, 17287, 3508, 1012, 1999, 8931, 1997, 1996, 7398, 2248, 3034, 2006, 15078, 15397, 1998, 26409, 3296, 3116, 1997, 1996, 9353, 2140, 1010, 5530, 4724, 2509, 1516, 17422, 1012, 9353, 2140, 1010, 2251, 2294, 1012, 1031, 2382, 1033, 1997, 4313, 2811, 1998, 5622, 2953, 4702, 1012, 2478, 1996, 6434, 7861, 8270, 4667, 2000, 5335, 2653, 4275, 1012, 12098, 9048, 2615, 17463, 6657, 2102, 12098, 9048, 2615, 1024, 8148, 2620, 1012, 5709, 27531, 2683, 1010, 2355, 1012, 1031, 2861, 1033, 7043, 12411, 16118, 7033, 1010, 6287, 2018, 3527, 2860, 1010, 1998, 10481, 16421, 1012, 15756, 3698, 5449, 1997, 4678, 2616, 2007, 4942, 18351, 3197, 1012, 12098, 9048, 2615, 17463, 6657, 2102, 12098, 9048, 2615, 1024, 5018, 2620, 1012, 5718, 21057, 2683, 1010, 2325, 1012, 1031, 3590, 1033, 2053, 3286, 21146, 23940, 2099, 1010, 17207, 22786, 14719, 15006, 12377, 2072, 1010, 1047, 28534, 17112, 3406, 2546, 5003, 12871, 15378, 1010, 5557, 4482, 1010, 22035, 2278, 3393, 1010, 11023, 9374, 2239, 1010, 1998, 5076, 4670, 1012, 25506, 2135, 2312, 15756, 6125, 1024, 1996, 24961, 1011, 4796, 2094, 8150, 1011, 1997, 1011, 8519, 6741, 1012, 12098, 9048, 2615, 17463, 6657, 2102, 12098, 9048, 2615, 1024, 26059, 1012, 5757, 22275, 2620, 1010, 2418, 1012, 1031, 3943, 1033, 9152, 24788, 5185, 12044, 2696, 3567, 1010, 11023, 1041, 9374, 2239, 1010, 4074, 1047, 21885, 5369, 15904, 1010, 6335, 3148, 10514, 29064, 22507, 1010, 1998, 22949, 5802, 16183, 27573, 4904, 4305, 1011, 13292, 1012, 4530, 5833, 1024, 1037, 3722, 2126, 2000, 4652, 15756, 6125, 2013, 2058, 8873, 13027, 1012, 3485, 1997, 3698, 4083, 2470, 1010, 2321, 1006, 1015, 1007, 1024, 4612, 1516, 3845, 1010, 2297, 1012, 1031, 4090, 1033, 18952, 27698, 12186, 2099, 10514, 10023, 3676, 6790, 2099, 1010, 4300, 1055, 2480, 10278, 1010, 4463, 12755, 1010, 1998, 6487, 21023, 1012, 2203, 1011, 2000, 1011, 2203, 3638, 6125, 1012, 1999, 1039, 1012, 22242, 1010, 1050, 1012, 1040, 1012, 5623, 1010, 1040, 1012, 1040, 1012, 3389, 1010, 1049, 1012, 10514, 5856, 11613, 1010, 1998, 1054, 1012, 11721, 26573, 2102, 1010, 10195, 1010, 9849, 1999, 15756, 2592, 6364, 3001, 2654, 1010, 5530, 24194, 2692, 1516, 24194, 2620, 1012, 19649, 9228, 1010, 4297, 1012, 1010, 2325, 1012, 1031, 3486, 1033, 6335, 3148, 10514, 29064, 22507, 1010, 2030, 20282, 19354, 21095, 2015, 1010, 1998, 22035, 2278, 1058, 2615, 3393, 1012, 5537, 2000, 5537, 4083, 2007, 15756, 6125, 1012, 1999, 9849, 1999, 15756, 2592, 6364, 3001, 1010, 5530, 17196, 2549, 1516, 23532, 2475, 1010, 2297, 1012, 1031, 4029, 1033, 3017, 1055, 4371, 5999, 2100, 1010, 6320, 3158, 6806, 12722, 2063, 1010, 22703, 22834, 16020, 1010, 6285, 8988, 2239, 14021, 7770, 2015, 1010, 1998, 1062, 5638, 29076, 7974, 24185, 22895, 2050, 1012, 2128, 15222, 8950, 2075, 1996, 12149, 4294, 2005, 3274, 4432, 1012, 2522, 12171, 1010, 14689, 1013, 16528, 2475, 1012, 4002, 26976, 2581, 1010, 2325, 1012, 1031, 4261, 1033, 19354, 21095, 2015, 1004, 15676, 1010, 12849, 2080, 1010, 9004, 12298, 1010, 10514, 29064, 22507, 1010, 1998, 9374, 2239, 1012, 8035, 2004, 1037, 3097, 2653, 1012, 1999, 9849, 1999, 15756, 2592, 6364, 3001, 1010, 2325, 1012, 1031, 4229, 1033, 18999, 20552, 8814, 1010, 3505, 24253, 1010, 1062, 4048, 24383, 8802, 1010, 22035, 2278, 1058, 3393, 1010, 12050, 4496, 7140, 5831, 1010, 13865, 24532, 7869, 2100, 1010, 20446, 1047, 15564, 4609, 1010, 11237, 12966, 1010, 19781, 17377, 1010, 16536, 24532, 7869, 2100, 1010, 3802, 2632, 1012, 8224, 1521, 1055, 15756, 3698, 5449, 2291, 1024, 7987, 3593, 4726, 1996, 6578, 2090, 2529, 1998, 3698, 5449, 1012, 12098, 9048, 2615, 17463, 6657, 2102, 12098, 9048, 2615, 1024, 28058, 1012, 5511, 16932, 2549, 1010, 2355, 1012, 1031, 4464, 1033, 10147, 2063, 14367, 1010, 20879, 12966, 1010, 15990, 19696, 3070, 7418, 1010, 26473, 5622, 1010, 1998, 11417, 15990, 1012, 2784, 28667, 29264, 4275, 2007, 3435, 1011, 2830, 7264, 2005, 15756, 3698, 5449, 1012, 2522, 12171, 1010, 14689, 1013, 8148, 2575, 1012, 5840, 16147, 2683, 1010, 2355, 1012, 1031, 2871, 1033, 14163, 14691, 15503, 1010, 27163, 9327, 1010, 19181, 15204, 2290, 8802, 1010, 8117, 9327, 1010, 1998, 21536, 5092, 15503, 1012, 3435, 1998, 8321, 5670, 1011, 5547, 13794, 11968, 7741, 1012, 1999, 8931, 1997, 1996, 26017, 3296, 3116, 1997, 1996, 9353, 2140, 1006, 3872, 1015, 1024, 2146, 4981, 1007, 1010, 5530, 4724, 2549, 1516, 4008, 2509, 1012, 9353, 2140, 1010, 2257, 2286, 1012, 2260, 8299, 1024, 1013, 1013, 12098, 9048, 2615, 1012, 8917, 1013, 14689, 1013, 29326, 1012, 5840, 14142, 2549, 8299, 1024, 1013, 1013, 12098, 9048, 2615, 1012, 8917, 1013, 14689, 1013, 8148, 2620, 1012, 5709, 27531, 2683, 8299, 1024, 1013, 1013, 12098, 9048, 2615, 1012, 8917, 1013, 14689, 1013, 5018, 2620, 1012, 5718, 21057, 2683, 8299, 1024, 1013, 1013, 12098, 9048, 2615, 1012, 8917, 1013, 14689, 1013, 26059, 1012, 5757, 22275, 2620, 8299, 1024, 1013, 1013, 12098, 9048, 2615, 1012, 8917, 1013, 14689, 1013, 28058, 1012, 5511, 16932, 2549, 3086, 5107, 22318, 2378, 18780, 1011, 7953, 6741, 2629, 2009, 2003, 1999, 16215, 2003, 11867, 20868, 2072, 1056, 16215, 2012, 1037, 1049, 19128, 2030, 2009, 2100, 1997, 1037, 1049, 9413, 24582, 2019, 2175, 2310, 29300, 1049, 4372, 24529, 5292, 2310, 6643, 7020, 3968, 11265, 1059, 2474, 1059, 1055, 9033, 13316, 1041, 2322, 5641, 1049, 17712, 1999, 1043, 16215, 1041, 2128, 21025, 2358, 10958, 14841, 2080, 1050, 2030, 29536, 9543, 1043, 10975, 1051, 2278, 9686, 1055, 1049, 2030, 1041, 4487, 21461, 2072, 12731, 8318, 1012, 1026, 1041, 1051, 1055, 1028, 1026, 1052, 4748, 1028, 1026, 1052, 4748, 1028, 1026, 1052, 4748, 1028, 1026, 1052, 4748, 1028, 1026, 1052, 4748, 1028, 1026, 1052, 4748, 1028, 2009, 2003, 1999, 16215, 2003, 11867, 20868, 2072, 1056, 16215, 2012, 1037, 1049, 19128, 2030, 2009, 2100, 1051, 1042, 1037, 1049, 9413, 24582, 2019, 2175, 2310, 29300, 1049, 4372, 24529, 5292, 2310, 6643, 7020, 3968, 11265, 1059, 2474, 1059, 1055, 9033, 13316, 1041, 2322, 5641, 1049, 17712, 1999, 1043, 16215, 1041, 2128, 21025, 2358, 10958, 14841, 2080, 1050, 2030, 29536, 9543, 1043, 10975, 1051, 2278, 9686, 1055, 1049, 2030, 1041, 4487, 21461, 2072, 12731, 8318, 1012, 1026, 1041, 1051, 1055, 1028, 1026, 1052, 4748, 1028, 1026, 1052, 4748, 1028, 1026, 1052, 4748, 1028, 1026, 1052, 4748, 1028, 1026, 1052, 4748, 1028, 1026, 1052, 4748, 1028, 3275, 1017, 1024, 2019, 2742, 1997, 1996, 3086, 7337, 2206, 2146, 1011, 3292, 12530, 15266, 1999, 1996, 4372, 16044, 2099, 2969, 1011, 3086, 1999, 6741, 1019, 1997, 1020, 1012, 2116, 1997, 1996, 3086, 4641, 5463, 2000, 1037, 6802, 24394, 1997, 1996, 12034, 1520, 2437, 1521, 1010, 7678, 1996, 7655, 1520, 2437, 1012, 1012, 1012, 2062, 3697, 1521, 1012, 3086, 2015, 2182, 3491, 2069, 2005, 1996, 2773, 1520, 2437, 1521, 1012, 2367, 6087, 5050, 2367, 4641, 1012, 2190, 7021, 1999, 3609, 1012, 2410, 7953, 1011, 7953, 6741, 2629, 1056, 2002, 2474, 1059, 1059, 5665, 11265, 2310, 1054, 2022, 21877, 21792, 14925, 1056, 1010, 20934, 1056, 2049, 9706, 20228, 24582, 2012, 22834, 1050, 14021, 15068, 25510, 2022, 18414, 2358, 1011, 16215, 2003, 2003, 1059, 5292, 1056, 1059, 1041, 12098, 1041, 1049, 2003, 9033, 12835, 1010, 1999, 1049, 1061, 6728, 1999, 22834, 1050, 1012, 1026, 1041, 1051, 1055, 1028, 1026, 1052, 4748, 1028, 1056, 2002, 2474, 1059, 1059, 5665, 11265, 2310, 1054, 2022, 21877, 21792, 14925, 1056, 1010, 20934, 1056, 2049, 9706, 20228, 24582, 2012, 22834, 1050, 14021, 15068, 25510, 2022, 18414, 2358, 1011, 16215, 2003, 2003, 1059, 5292, 1056, 1059, 1041, 12098, 1041, 1049, 2003, 9033, 12835, 1010, 1999, 1049, 1061, 6728, 1999, 22834, 1050, 1012, 1026, 1041, 1051, 1055, 1028, 1026, 1052, 4748, 1028, 7953, 1011, 7953, 6741, 2629, 1056, 2002, 2474, 1059, 1059, 5665, 11265, 2310, 1054, 2022, 21877, 21792, 14925, 1056, 1010, 20934, 1056, 2049, 9706, 20228, 24582, 2012, 22834, 1050, 14021, 15068, 25510, 2022, 18414, 2358, 1011, 16215, 2003, 2003, 1059, 5292, 1056, 1059, 1041, 12098, 1041, 1049, 2003, 9033, 12835, 1010, 1999, 1049, 1061, 6728, 1999, 22834, 1050, 1012, 1026, 1041, 1051, 1055, 1028, 1026, 1052, 4748, 1028, 1056, 2002, 2474, 1059, 1059, 5665, 11265, 2310, 1054, 2022, 21877, 21792, 14925, 1056, 1010, 20934, 1056, 2049, 9706, 20228, 24582, 2012, 22834, 1050, 14021, 15068, 25510, 2022, 18414, 2358, 1011, 16215, 2003, 2003, 1059, 5292, 1056, 1059, 1041, 12098, 1041, 1049, 2003, 9033, 12835, 1010, 1999, 1049, 1061, 6728, 1999, 22834, 1050, 1012, 1026, 1041, 1051, 1055, 1028, 1026, 1052, 4748, 1028, 3275, 1018, 1024, 2048, 3086, 4641, 1010, 2036, 1999, 6741, 1019, 1997, 1020, 1010, 4593, 2920, 1999, 9617, 8458, 6525, 5813, 1012, 2327, 1024, 2440, 3086, 2015, 2005, 2132, 1019, 1012, 3953, 1024, 7275, 3086, 2015, 2013, 2074, 1996, 2773, 1520, 2049, 1521, 2005, 3086, 4641, 1019, 1998, 1020, 1012, 3602, 2008, 1996, 3086, 2015, 2024, 2200, 4629, 2005, 2023, 2773, 1012, 2403, 7953, 1011, 7953, 6741, 2629, 1056, 2002, 2474, 1059, 1059, 5665, 11265, 2310, 1054, 2022, 21877, 21792, 14925, 1056, 1010, 20934, 1056, 2049, 9706, 20228, 24582, 2012, 22834, 1050, 14021, 15068, 25510, 2022, 18414, 2358, 1011, 16215, 2003, 2003, 1059, 5292, 1056, 1059, 1041, 12098, 1041, 1049, 2003, 9033, 12835, 1010, 1999, 1049, 1061, 6728, 1999, 22834, 1050, 1012, 1026, 1041, 1051, 1055, 1028, 1026, 1052, 4748, 1028, 1056, 2002, 2474, 1059, 1059, 5665, 11265, 2310, 1054, 2022, 21877, 21792, 14925, 1056, 1010, 20934, 1056, 2049, 9706, 20228, 24582, 2012, 22834, 1050, 14021, 15068, 25510, 2022, 18414, 2358, 1011, 16215, 2003, 2003, 1059, 5292, 1056, 1059, 1041, 12098, 1041, 1049, 2003, 9033, 12835, 1010, 1999, 1049, 1061, 6728, 1999, 22834, 1050, 1012, 1026, 1041, 1051, 1055, 1028, 1026, 1052, 4748, 1028, 7953, 1011, 7953, 6741, 2629, 1056, 2002, 2474, 1059, 1059, 5665, 11265, 2310, 1054, 2022, 21877, 21792, 14925, 1056, 1010, 20934, 1056, 2049, 9706, 20228, 24582, 2012, 22834, 1050, 14021, 15068, 25510, 2022, 18414, 2358, 1011, 16215, 2003, 2003, 1059, 5292, 1056, 1059, 1041, 12098, 1041, 1049, 2003, 9033, 12835, 1010, 1999, 1049, 1061, 6728, 1999, 22834, 1050, 1012, 1026, 1041, 1051, 1055, 1028, 1026, 1052, 4748, 1028, 1056, 2002, 2474, 1059, 1059, 5665, 11265, 2310, 1054, 2022, 21877, 21792, 14925, 1056, 1010, 20934, 1056, 2049, 9706, 20228, 24582, 2012, 22834, 1050, 14021, 15068, 25510, 2022, 18414, 2358, 1011, 16215, 2003, 2003, 1059, 5292, 1056, 1059, 1041, 12098, 1041, 1049, 2003, 9033, 12835, 1010, 1999, 1049, 1061, 6728, 1999, 22834, 1050, 1012, 1026, 1041, 1051, 1055, 1028, 1026, 1052, 4748, 1028, 3275, 1019, 1024, 2116, 1997, 1996, 3086, 4641, 8327, 9164, 2008, 3849, 3141, 2000, 1996, 3252, 1997, 1996, 6251, 1012, 2057, 2507, 2048, 2107, 4973, 2682, 1010, 2013, 2048, 2367, 4641, 2013, 1996, 4372, 16044, 2099, 2969, 1011, 3086, 2012, 6741, 1019, 1997, 1020, 1012, 1996, 4641, 4415, 4342, 2000, 4685, 2367, 8518, 1012, 2321, 1015, 4955, 1016, 4281, 1017, 2944, 4294, 1017, 1012, 1015, 4372, 16044, 2099, 1998, 21933, 4063, 20829, 1017, 1012, 1016, 3086, 1017, 1012, 1016, 1012, 1015, 18953, 11089, 1011, 4031, 3086, 1017, 1012, 1016, 1012, 1016, 4800, 1011, 2132, 3086, 1017, 1012, 1016, 1012, 1017, 5097, 1997, 3086, 1999, 2256, 2944, 1017, 1012, 1017, 2597, 1011, 7968, 5438, 1011, 2830, 6125, 1017, 1012, 1018, 7861, 8270, 4667, 2015, 1998, 3730, 17848, 1017, 1012, 1019, 2597, 2389, 17181, 1018, 2339, 2969, 1011, 3086, 1019, 2731, 1019, 1012, 1015, 2731, 2951, 1998, 14108, 2075, 1019, 1012, 1016, 8051, 1998, 6134, 1019, 1012, 1017, 23569, 27605, 6290, 1019, 1012, 1018, 3180, 3989, 1020, 3463, 1020, 1012, 1015, 3698, 5449, 1020, 1012, 1016, 2944, 8358, 1020, 1012, 1017, 2394, 5540, 11968, 7741, 1021, 7091, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens(inputs.input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyzrtsYARI7E",
        "outputId": "ffebbd71-5d5f-4248-ebdc-f110e17cfdde"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'attention',\n",
              " 'is',\n",
              " 'all',\n",
              " 'you',\n",
              " 'need',\n",
              " 'ash',\n",
              " '##ish',\n",
              " 'va',\n",
              " '##sw',\n",
              " '##ani',\n",
              " '##∗',\n",
              " 'google',\n",
              " 'brain',\n",
              " 'ava',\n",
              " '##sw',\n",
              " '##ani',\n",
              " '@',\n",
              " 'google',\n",
              " '.',\n",
              " 'com',\n",
              " 'no',\n",
              " '##am',\n",
              " 'sha',\n",
              " '##zee',\n",
              " '##r',\n",
              " '##∗',\n",
              " 'google',\n",
              " 'brain',\n",
              " 'no',\n",
              " '##am',\n",
              " '@',\n",
              " 'google',\n",
              " '.',\n",
              " 'com',\n",
              " 'nik',\n",
              " '##i',\n",
              " 'parma',\n",
              " '##r',\n",
              " '##∗',\n",
              " 'google',\n",
              " 'research',\n",
              " 'nik',\n",
              " '##ip',\n",
              " '@',\n",
              " 'google',\n",
              " '.',\n",
              " 'com',\n",
              " 'jakob',\n",
              " 'us',\n",
              " '##zko',\n",
              " '##re',\n",
              " '##it',\n",
              " '##∗',\n",
              " 'google',\n",
              " 'research',\n",
              " 'us',\n",
              " '##z',\n",
              " '@',\n",
              " 'google',\n",
              " '.',\n",
              " 'com',\n",
              " 'll',\n",
              " '##ion',\n",
              " 'jones',\n",
              " '##∗',\n",
              " 'google',\n",
              " 'research',\n",
              " 'll',\n",
              " '##ion',\n",
              " '@',\n",
              " 'google',\n",
              " '.',\n",
              " 'com',\n",
              " 'aidan',\n",
              " 'n',\n",
              " '.',\n",
              " 'gomez',\n",
              " '##∗',\n",
              " '†',\n",
              " 'university',\n",
              " 'of',\n",
              " 'toronto',\n",
              " 'aidan',\n",
              " '@',\n",
              " 'cs',\n",
              " '.',\n",
              " 'toronto',\n",
              " '.',\n",
              " 'ed',\n",
              " '##u',\n",
              " 'ł',\n",
              " '##uka',\n",
              " '##sz',\n",
              " 'kaiser',\n",
              " '##∗',\n",
              " 'google',\n",
              " 'brain',\n",
              " 'lukas',\n",
              " '##z',\n",
              " '##kai',\n",
              " '##ser',\n",
              " '@',\n",
              " 'google',\n",
              " '.',\n",
              " 'com',\n",
              " 'ill',\n",
              " '##ia',\n",
              " 'polo',\n",
              " '##su',\n",
              " '##kh',\n",
              " '##in',\n",
              " '##∗',\n",
              " '‡',\n",
              " 'ill',\n",
              " '##ia',\n",
              " '.',\n",
              " 'polo',\n",
              " '##su',\n",
              " '##kh',\n",
              " '##in',\n",
              " '@',\n",
              " 'gma',\n",
              " '##il',\n",
              " '.',\n",
              " 'com',\n",
              " 'abstract',\n",
              " 'the',\n",
              " 'dominant',\n",
              " 'sequence',\n",
              " 'trans',\n",
              " '##duction',\n",
              " 'models',\n",
              " 'are',\n",
              " 'based',\n",
              " 'on',\n",
              " 'complex',\n",
              " 'rec',\n",
              " '##urrent',\n",
              " 'or',\n",
              " 'con',\n",
              " '##vo',\n",
              " '##lu',\n",
              " '##tion',\n",
              " '##al',\n",
              " 'neural',\n",
              " 'networks',\n",
              " 'that',\n",
              " 'include',\n",
              " 'an',\n",
              " 'en',\n",
              " '##code',\n",
              " '##r',\n",
              " 'and',\n",
              " 'a',\n",
              " 'deco',\n",
              " '##der',\n",
              " '.',\n",
              " 'the',\n",
              " 'best',\n",
              " 'performing',\n",
              " 'models',\n",
              " 'also',\n",
              " 'connect',\n",
              " 'the',\n",
              " 'en',\n",
              " '##code',\n",
              " '##r',\n",
              " 'and',\n",
              " 'deco',\n",
              " '##der',\n",
              " 'through',\n",
              " 'an',\n",
              " 'attention',\n",
              " 'mechanism',\n",
              " '.',\n",
              " 'we',\n",
              " 'propose',\n",
              " 'a',\n",
              " 'new',\n",
              " 'simple',\n",
              " 'network',\n",
              " 'architecture',\n",
              " ',',\n",
              " 'the',\n",
              " 'transform',\n",
              " '##er',\n",
              " ',',\n",
              " 'based',\n",
              " 'solely',\n",
              " 'on',\n",
              " 'attention',\n",
              " 'mechanisms',\n",
              " ',',\n",
              " 'di',\n",
              " '##sp',\n",
              " '##ens',\n",
              " '##ing',\n",
              " 'with',\n",
              " 'rec',\n",
              " '##ur',\n",
              " '##rence',\n",
              " 'and',\n",
              " 'con',\n",
              " '##vo',\n",
              " '##lu',\n",
              " '##tions',\n",
              " 'entirely',\n",
              " '.',\n",
              " 'experiments',\n",
              " 'on',\n",
              " 'two',\n",
              " 'machine',\n",
              " 'translation',\n",
              " 'tasks',\n",
              " 'show',\n",
              " 'these',\n",
              " 'models',\n",
              " 'to',\n",
              " 'be',\n",
              " 'superior',\n",
              " 'in',\n",
              " 'quality',\n",
              " 'while',\n",
              " 'being',\n",
              " 'more',\n",
              " 'parallel',\n",
              " '##iza',\n",
              " '##ble',\n",
              " 'and',\n",
              " 'requiring',\n",
              " 'significantly',\n",
              " 'less',\n",
              " 'time',\n",
              " 'to',\n",
              " 'train',\n",
              " '.',\n",
              " 'our',\n",
              " 'model',\n",
              " 'achieve',\n",
              " '##s',\n",
              " '28',\n",
              " '.',\n",
              " '4',\n",
              " 'b',\n",
              " '##le',\n",
              " '##u',\n",
              " 'on',\n",
              " 'the',\n",
              " 'w',\n",
              " '##mt',\n",
              " '2014',\n",
              " 'english',\n",
              " '-',\n",
              " 'to',\n",
              " '-',\n",
              " 'german',\n",
              " 'translation',\n",
              " 'task',\n",
              " ',',\n",
              " 'improving',\n",
              " 'over',\n",
              " 'the',\n",
              " 'existing',\n",
              " 'best',\n",
              " 'results',\n",
              " ',',\n",
              " 'including',\n",
              " 'ensembles',\n",
              " ',',\n",
              " 'by',\n",
              " 'over',\n",
              " '2',\n",
              " 'b',\n",
              " '##le',\n",
              " '##u',\n",
              " '.',\n",
              " 'on',\n",
              " 'the',\n",
              " 'w',\n",
              " '##mt',\n",
              " '2014',\n",
              " 'english',\n",
              " '-',\n",
              " 'to',\n",
              " '-',\n",
              " 'french',\n",
              " 'translation',\n",
              " 'task',\n",
              " ',',\n",
              " 'our',\n",
              " 'model',\n",
              " 'establishes',\n",
              " 'a',\n",
              " 'new',\n",
              " 'single',\n",
              " '-',\n",
              " 'model',\n",
              " 'state',\n",
              " '-',\n",
              " 'of',\n",
              " '-',\n",
              " 'the',\n",
              " '-',\n",
              " 'art',\n",
              " 'b',\n",
              " '##le',\n",
              " '##u',\n",
              " 'score',\n",
              " 'of',\n",
              " '41',\n",
              " '.',\n",
              " '8',\n",
              " 'after',\n",
              " 'training',\n",
              " 'for',\n",
              " '3',\n",
              " '.',\n",
              " '5',\n",
              " 'days',\n",
              " 'on',\n",
              " 'eight',\n",
              " 'gp',\n",
              " '##us',\n",
              " ',',\n",
              " 'a',\n",
              " 'small',\n",
              " 'fraction',\n",
              " 'of',\n",
              " 'the',\n",
              " 'training',\n",
              " 'costs',\n",
              " 'of',\n",
              " 'the',\n",
              " 'best',\n",
              " 'models',\n",
              " 'from',\n",
              " 'the',\n",
              " 'literature',\n",
              " '.',\n",
              " 'we',\n",
              " 'show',\n",
              " 'that',\n",
              " 'the',\n",
              " 'transform',\n",
              " '##er',\n",
              " 'general',\n",
              " '##izes',\n",
              " 'well',\n",
              " 'to',\n",
              " 'other',\n",
              " 'tasks',\n",
              " 'by',\n",
              " 'applying',\n",
              " 'it',\n",
              " 'successfully',\n",
              " 'to',\n",
              " 'english',\n",
              " 'constituency',\n",
              " 'par',\n",
              " '##sing',\n",
              " 'both',\n",
              " 'with',\n",
              " 'large',\n",
              " 'and',\n",
              " 'limited',\n",
              " 'training',\n",
              " 'data',\n",
              " '.',\n",
              " '1',\n",
              " 'introduction',\n",
              " 'rec',\n",
              " '##urrent',\n",
              " 'neural',\n",
              " 'networks',\n",
              " ',',\n",
              " 'long',\n",
              " 'short',\n",
              " '-',\n",
              " 'term',\n",
              " 'memory',\n",
              " '[',\n",
              " '13',\n",
              " ']',\n",
              " 'and',\n",
              " 'gate',\n",
              " '##d',\n",
              " 'rec',\n",
              " '##urrent',\n",
              " '[',\n",
              " '7',\n",
              " ']',\n",
              " 'neural',\n",
              " 'networks',\n",
              " 'in',\n",
              " 'particular',\n",
              " ',',\n",
              " 'have',\n",
              " 'been',\n",
              " 'firmly',\n",
              " 'established',\n",
              " 'as',\n",
              " 'state',\n",
              " 'of',\n",
              " 'the',\n",
              " 'art',\n",
              " 'approaches',\n",
              " 'in',\n",
              " 'sequence',\n",
              " 'modeling',\n",
              " 'and',\n",
              " '∗',\n",
              " '##e',\n",
              " '##qual',\n",
              " 'contribution',\n",
              " '.',\n",
              " 'listing',\n",
              " 'order',\n",
              " 'is',\n",
              " 'random',\n",
              " '.',\n",
              " 'jakob',\n",
              " 'proposed',\n",
              " 'replacing',\n",
              " 'rn',\n",
              " '##ns',\n",
              " 'with',\n",
              " 'self',\n",
              " '-',\n",
              " 'attention',\n",
              " 'and',\n",
              " 'started',\n",
              " 'the',\n",
              " 'effort',\n",
              " 'to',\n",
              " 'evaluate',\n",
              " 'this',\n",
              " 'idea',\n",
              " '.',\n",
              " 'ash',\n",
              " '##ish',\n",
              " ',',\n",
              " 'with',\n",
              " 'ill',\n",
              " '##ia',\n",
              " ',',\n",
              " 'designed',\n",
              " 'and',\n",
              " 'implemented',\n",
              " 'the',\n",
              " 'first',\n",
              " 'transform',\n",
              " '##er',\n",
              " 'models',\n",
              " 'and',\n",
              " 'has',\n",
              " 'been',\n",
              " 'crucial',\n",
              " '##ly',\n",
              " 'involved',\n",
              " 'in',\n",
              " 'every',\n",
              " 'aspect',\n",
              " 'of',\n",
              " 'this',\n",
              " 'work',\n",
              " '.',\n",
              " 'no',\n",
              " '##am',\n",
              " 'proposed',\n",
              " 'scaled',\n",
              " 'dot',\n",
              " '-',\n",
              " 'product',\n",
              " 'attention',\n",
              " ',',\n",
              " 'multi',\n",
              " '-',\n",
              " 'head',\n",
              " 'attention',\n",
              " 'and',\n",
              " 'the',\n",
              " 'parameter',\n",
              " '-',\n",
              " 'free',\n",
              " 'position',\n",
              " 'representation',\n",
              " 'and',\n",
              " 'became',\n",
              " 'the',\n",
              " 'other',\n",
              " 'person',\n",
              " 'involved',\n",
              " 'in',\n",
              " 'nearly',\n",
              " 'every',\n",
              " 'detail',\n",
              " '.',\n",
              " 'nik',\n",
              " '##i',\n",
              " 'designed',\n",
              " ',',\n",
              " 'implemented',\n",
              " ',',\n",
              " 'tuned',\n",
              " 'and',\n",
              " 'evaluated',\n",
              " 'countless',\n",
              " 'model',\n",
              " 'variants',\n",
              " 'in',\n",
              " 'our',\n",
              " 'original',\n",
              " 'code',\n",
              " '##base',\n",
              " 'and',\n",
              " 'tensor',\n",
              " '##2',\n",
              " '##tens',\n",
              " '##or',\n",
              " '.',\n",
              " 'll',\n",
              " '##ion',\n",
              " 'also',\n",
              " 'experimented',\n",
              " 'with',\n",
              " 'novel',\n",
              " 'model',\n",
              " 'variants',\n",
              " ',',\n",
              " 'was',\n",
              " 'responsible',\n",
              " 'for',\n",
              " 'our',\n",
              " 'initial',\n",
              " 'code',\n",
              " '##base',\n",
              " ',',\n",
              " 'and',\n",
              " 'efficient',\n",
              " 'inference',\n",
              " 'and',\n",
              " 'visual',\n",
              " '##izations',\n",
              " '.',\n",
              " 'lukas',\n",
              " '##z',\n",
              " 'and',\n",
              " 'aidan',\n",
              " 'spent',\n",
              " 'countless',\n",
              " 'long',\n",
              " 'days',\n",
              " 'designing',\n",
              " 'various',\n",
              " 'parts',\n",
              " 'of',\n",
              " 'and',\n",
              " 'implementing',\n",
              " 'tensor',\n",
              " '##2',\n",
              " '##tens',\n",
              " '##or',\n",
              " ',',\n",
              " 'replacing',\n",
              " 'our',\n",
              " 'earlier',\n",
              " 'code',\n",
              " '##base',\n",
              " ',',\n",
              " 'greatly',\n",
              " 'improving',\n",
              " 'results',\n",
              " 'and',\n",
              " 'massive',\n",
              " '##ly',\n",
              " 'accelerating',\n",
              " 'our',\n",
              " 'research',\n",
              " '.',\n",
              " '†',\n",
              " 'work',\n",
              " 'performed',\n",
              " 'while',\n",
              " 'at',\n",
              " 'google',\n",
              " 'brain',\n",
              " '.',\n",
              " '‡',\n",
              " 'work',\n",
              " 'performed',\n",
              " 'while',\n",
              " 'at',\n",
              " 'google',\n",
              " 'research',\n",
              " '.',\n",
              " '31st',\n",
              " 'conference',\n",
              " 'on',\n",
              " 'neural',\n",
              " 'information',\n",
              " 'processing',\n",
              " 'systems',\n",
              " '(',\n",
              " 'ni',\n",
              " '##ps',\n",
              " '2017',\n",
              " ')',\n",
              " ',',\n",
              " 'long',\n",
              " 'beach',\n",
              " ',',\n",
              " 'ca',\n",
              " ',',\n",
              " 'usa',\n",
              " '.',\n",
              " 'ar',\n",
              " 'x',\n",
              " 'iv',\n",
              " ':',\n",
              " '1',\n",
              " '70',\n",
              " '6',\n",
              " '.',\n",
              " '03',\n",
              " '76',\n",
              " '2',\n",
              " '##v',\n",
              " '5',\n",
              " '[',\n",
              " 'cs',\n",
              " '.',\n",
              " 'c',\n",
              " 'l',\n",
              " ']',\n",
              " '6',\n",
              " 'd',\n",
              " 'ec',\n",
              " '2',\n",
              " '01',\n",
              " '7',\n",
              " 'trans',\n",
              " '##duction',\n",
              " 'problems',\n",
              " 'such',\n",
              " 'as',\n",
              " 'language',\n",
              " 'modeling',\n",
              " 'and',\n",
              " 'machine',\n",
              " 'translation',\n",
              " '[',\n",
              " '35',\n",
              " ',',\n",
              " '2',\n",
              " ',',\n",
              " '5',\n",
              " ']',\n",
              " '.',\n",
              " 'numerous',\n",
              " 'efforts',\n",
              " 'have',\n",
              " 'since',\n",
              " 'continued',\n",
              " 'to',\n",
              " 'push',\n",
              " 'the',\n",
              " 'boundaries',\n",
              " 'of',\n",
              " 'rec',\n",
              " '##urrent',\n",
              " 'language',\n",
              " 'models',\n",
              " 'and',\n",
              " 'en',\n",
              " '##code',\n",
              " '##r',\n",
              " '-',\n",
              " 'deco',\n",
              " '##der',\n",
              " 'architecture',\n",
              " '##s',\n",
              " '[',\n",
              " '38',\n",
              " ',',\n",
              " '24',\n",
              " ',',\n",
              " '15',\n",
              " ']',\n",
              " '.',\n",
              " 'rec',\n",
              " '##urrent',\n",
              " 'models',\n",
              " 'typically',\n",
              " 'factor',\n",
              " 'computation',\n",
              " 'along',\n",
              " 'the',\n",
              " 'symbol',\n",
              " 'positions',\n",
              " 'of',\n",
              " 'the',\n",
              " 'input',\n",
              " 'and',\n",
              " 'output',\n",
              " 'sequences',\n",
              " '.',\n",
              " 'align',\n",
              " '##ing',\n",
              " 'the',\n",
              " 'positions',\n",
              " 'to',\n",
              " 'steps',\n",
              " 'in',\n",
              " 'computation',\n",
              " 'time',\n",
              " ',',\n",
              " 'they',\n",
              " 'generate',\n",
              " 'a',\n",
              " 'sequence',\n",
              " 'of',\n",
              " 'hidden',\n",
              " 'states',\n",
              " 'h',\n",
              " '##t',\n",
              " ',',\n",
              " 'as',\n",
              " 'a',\n",
              " 'function',\n",
              " 'of',\n",
              " 'the',\n",
              " 'previous',\n",
              " 'hidden',\n",
              " 'state',\n",
              " 'h',\n",
              " '##t',\n",
              " '##−1',\n",
              " 'and',\n",
              " 'the',\n",
              " 'input',\n",
              " 'for',\n",
              " 'position',\n",
              " 't',\n",
              " '.',\n",
              " 'this',\n",
              " 'inherently',\n",
              " 'sequential',\n",
              " 'nature',\n",
              " 'pre',\n",
              " '##cl',\n",
              " '##udes',\n",
              " 'parallel',\n",
              " '##ization',\n",
              " 'within',\n",
              " 'training',\n",
              " 'examples',\n",
              " ',',\n",
              " 'which',\n",
              " 'becomes',\n",
              " 'critical',\n",
              " 'at',\n",
              " 'longer',\n",
              " 'sequence',\n",
              " 'lengths',\n",
              " ',',\n",
              " 'as',\n",
              " 'memory',\n",
              " 'constraints',\n",
              " 'limit',\n",
              " 'batch',\n",
              " '##ing',\n",
              " 'across',\n",
              " 'examples',\n",
              " '.',\n",
              " 'recent',\n",
              " 'work',\n",
              " 'has',\n",
              " 'achieved',\n",
              " 'significant',\n",
              " 'improvements',\n",
              " 'in',\n",
              " 'computational',\n",
              " 'efficiency',\n",
              " 'through',\n",
              " 'factor',\n",
              " '##ization',\n",
              " 'tricks',\n",
              " '[',\n",
              " '21',\n",
              " ']',\n",
              " 'and',\n",
              " 'conditional',\n",
              " 'computation',\n",
              " '[',\n",
              " '32',\n",
              " ']',\n",
              " ',',\n",
              " 'while',\n",
              " 'also',\n",
              " 'improving',\n",
              " 'model',\n",
              " 'performance',\n",
              " 'in',\n",
              " 'case',\n",
              " 'of',\n",
              " 'the',\n",
              " 'latter',\n",
              " '.',\n",
              " 'the',\n",
              " 'fundamental',\n",
              " 'constraint',\n",
              " 'of',\n",
              " 'sequential',\n",
              " 'computation',\n",
              " ',',\n",
              " 'however',\n",
              " ',',\n",
              " 'remains',\n",
              " '.',\n",
              " 'attention',\n",
              " 'mechanisms',\n",
              " 'have',\n",
              " 'become',\n",
              " 'an',\n",
              " 'integral',\n",
              " 'part',\n",
              " 'of',\n",
              " 'compelling',\n",
              " 'sequence',\n",
              " 'modeling',\n",
              " 'and',\n",
              " 'trans',\n",
              " '##du',\n",
              " '##c',\n",
              " '-',\n",
              " 'ti',\n",
              " '##on',\n",
              " 'models',\n",
              " 'in',\n",
              " 'various',\n",
              " 'tasks',\n",
              " ',',\n",
              " 'allowing',\n",
              " 'modeling',\n",
              " 'of',\n",
              " 'depend',\n",
              " '##encies',\n",
              " 'without',\n",
              " 'regard',\n",
              " 'to',\n",
              " 'their',\n",
              " 'distance',\n",
              " 'in',\n",
              " 'the',\n",
              " 'input',\n",
              " 'or',\n",
              " 'output',\n",
              " 'sequences',\n",
              " '[',\n",
              " '2',\n",
              " ',',\n",
              " '19',\n",
              " ']',\n",
              " '.',\n",
              " 'in',\n",
              " 'all',\n",
              " 'but',\n",
              " 'a',\n",
              " 'few',\n",
              " 'cases',\n",
              " '[',\n",
              " '27',\n",
              " ']',\n",
              " ',',\n",
              " 'however',\n",
              " ',',\n",
              " 'such',\n",
              " 'attention',\n",
              " 'mechanisms',\n",
              " 'are',\n",
              " 'used',\n",
              " 'in',\n",
              " 'conjunction',\n",
              " 'with',\n",
              " 'a',\n",
              " 'rec',\n",
              " '##urrent',\n",
              " 'network',\n",
              " '.',\n",
              " 'in',\n",
              " 'this',\n",
              " 'work',\n",
              " 'we',\n",
              " 'propose',\n",
              " 'the',\n",
              " 'transform',\n",
              " '##er',\n",
              " ',',\n",
              " 'a',\n",
              " 'model',\n",
              " 'architecture',\n",
              " 'es',\n",
              " '##che',\n",
              " '##wing',\n",
              " 'rec',\n",
              " '##ur',\n",
              " '##rence',\n",
              " 'and',\n",
              " 'instead',\n",
              " 'relying',\n",
              " 'entirely',\n",
              " 'on',\n",
              " 'an',\n",
              " 'attention',\n",
              " 'mechanism',\n",
              " 'to',\n",
              " 'draw',\n",
              " 'global',\n",
              " 'depend',\n",
              " '##encies',\n",
              " 'between',\n",
              " 'input',\n",
              " 'and',\n",
              " 'output',\n",
              " '.',\n",
              " 'the',\n",
              " 'transform',\n",
              " '##er',\n",
              " 'allows',\n",
              " 'for',\n",
              " 'significantly',\n",
              " 'more',\n",
              " 'parallel',\n",
              " '##ization',\n",
              " 'and',\n",
              " 'can',\n",
              " 'reach',\n",
              " 'a',\n",
              " 'new',\n",
              " 'state',\n",
              " 'of',\n",
              " 'the',\n",
              " 'art',\n",
              " 'in',\n",
              " 'translation',\n",
              " 'quality',\n",
              " 'after',\n",
              " 'being',\n",
              " 'trained',\n",
              " 'for',\n",
              " 'as',\n",
              " 'little',\n",
              " 'as',\n",
              " 'twelve',\n",
              " 'hours',\n",
              " 'on',\n",
              " 'eight',\n",
              " 'p',\n",
              " '##100',\n",
              " 'gp',\n",
              " '##us',\n",
              " '.',\n",
              " '2',\n",
              " 'background',\n",
              " 'the',\n",
              " 'goal',\n",
              " 'of',\n",
              " 'reducing',\n",
              " 'sequential',\n",
              " 'computation',\n",
              " 'also',\n",
              " 'forms',\n",
              " 'the',\n",
              " 'foundation',\n",
              " 'of',\n",
              " 'the',\n",
              " 'extended',\n",
              " 'neural',\n",
              " 'gp',\n",
              " '##u',\n",
              " '[',\n",
              " '16',\n",
              " ']',\n",
              " ',',\n",
              " 'byte',\n",
              " '##net',\n",
              " '[',\n",
              " '18',\n",
              " ']',\n",
              " 'and',\n",
              " 'con',\n",
              " '##vs',\n",
              " '##2',\n",
              " '##s',\n",
              " '[',\n",
              " '9',\n",
              " ']',\n",
              " ',',\n",
              " 'all',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 512\n",
        "max_target_length = 30\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(\n",
        "        examples[\"review_body\"],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        examples[\"review_title\"], max_length=max_target_length, truncation=True\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "4N7GB2dfTgUh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizando dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# model_checkpoint = \"bert-base-uncased\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "trained_tokenized_dataset = tokenizer(str(small_train_dataset))\n",
        "eval_tokenized_dataset = tokenizer(str(small_eval_dataset))"
      ],
      "metadata": {
        "id": "CpD5SSA-PDZt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parte inicial do treinamento do modelo\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"test_trainer\", \n",
        "    evaluation_strategy=\"epoch\", \n",
        "    per_device_train_batch_size = 2)"
      ],
      "metadata": {
        "id": "KWLXaRvsSZiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52cd9f4f-e48d-4aa6-95fc-53059e9abb80"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Métricas de 'summarization'\n",
        "# O rogue é baseado no cálculo das pontuações de precisão e recuperação para a sobreposição\n",
        "\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "!pip install -q rouge_score\n",
        "\n",
        "rouge_score = evaluate.load(\"rouge\")"
      ],
      "metadata": {
        "id": "DXBo_TbdRmHr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "9175e7f54c79461195a4f760b892cb15",
            "bc20f8d61b5f430cbea422a995a1a869",
            "35fb998895fa4f579e277d1bb42ed42b",
            "02d2d46a31f54ff186b5971fa15637c1",
            "a40c2a134a874951ac33a2b3b2603ce4",
            "21d4138ff2084a2e97d2ad442525b936",
            "3e339bc4cea140958769a4b79e1dd6b3",
            "b2fa0b8ded8c44c48ca53be0163e77b8",
            "1c3882e3091141049af4b981bddbf577",
            "3df9d43eb1034d99bca956a677a2bf90",
            "94c80ff7ccc24d88b7f6a981660746e0"
          ]
        },
        "outputId": "83c26c5d-c5aa-4b47-97ee-472255a28994"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9175e7f54c79461195a4f760b892cb15"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence_transformers"
      ],
      "metadata": {
        "id": "hxzRlLlyXUfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac11ddb8-c425-4eed-f55d-6396b6e5b076"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 85 kB 2.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 31.2 MB/s \n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos pegar o abstract que a gente gerou do artigo, a partir disso vamos \n",
        "# comparar com o abstract original do artigo original que geramos o abstract\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Single list of sentences\n",
        "sentences = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'I love pasta',\n",
        "             'The new movie is awesome',\n",
        "             'The cat plays in the garden',\n",
        "             'A woman watches TV',\n",
        "             'The new movie is so great',\n",
        "             'Do you like pizza?']\n",
        "\n",
        "# Compute embeddings\n",
        "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine-similarities for each sentence with each other sentence\n",
        "cosine_scores = util.cos_sim(embeddings, embeddings)\n",
        "\n",
        "# Find the pairs with the highest cosine similarity scores\n",
        "pairs = []\n",
        "for i in range(len(cosine_scores)-1):\n",
        "    for j in range(i+1, len(cosine_scores)):\n",
        "        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
        "\n",
        "# Sort scores in decreasing order\n",
        "pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "for pair in pairs[0:10]:\n",
        "    i, j = pair['index']\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], pair['score']))  "
      ],
      "metadata": {
        "id": "Gg0cNPw8XC2y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639,
          "referenced_widgets": [
            "e3b39088b855495697719b60cd5932be",
            "477ca5715e114a529324f90d8b53aea6",
            "68887fea87634f669ac03b736a4890e0",
            "5b168f8a98cb4201ad51ba1f3f0dc2f6",
            "ffce278c85134a5a87c3aefbc19c47fe",
            "729eedd51b874000a23c6699fb95302c",
            "165559d5b4b54ff4ab9b7d18950fbe16",
            "cf161a17bf124c08a3489455ed36ce2d",
            "d75dff51b1664727bcf16c8d72ae5c07",
            "b323232212a3473698522c716c6c69d5",
            "90afebb387394feebce1db7761348437",
            "b1ddddb1c9e44d5eae262d1678992b63",
            "2d1bba22cc0349baaa368b1c95fc6a94",
            "4abfc3420e794f6583acb580799395ba",
            "9b04b7cf15874cbebb665e9fa6282427",
            "b423f76a4b94478fa7ba5b8d4b28c237",
            "9dc8621df02a4a2ca14886fb853e03d6",
            "990b5bedb1124f5ca68b4688c036d159",
            "154f49fbc17249ec9df4e4043b8ddf6d",
            "1f29a42f088c439f96bec59785260059",
            "70c445e64e294659bbd077396c035f29",
            "8e8dff9244a74f3daf3008a8e0292e41",
            "097c0d80265f4acdb6023d621cdc7165",
            "1e80be009bfa46109def7234b3584f07",
            "052fb2ee69594b288f234230aebc4388",
            "c8f0f79b3ba34eb98b84187e85fc4837",
            "74a50e2051824814b57cbf1a7bfc92d2",
            "0f406c61259d4b2fb480c07a0ec739f2",
            "55636254297c4a2f8b9b03c48a8a93aa",
            "d8f6da120ef446b484f8ec75ec6c5bf5",
            "00676cbf6b874466835af1a1cd27c5ee",
            "e01bfc8b77854dbea9b896c37c708736",
            "a00b6f7cd94e447caaf800c30147ff69",
            "5435dc1f959544399efaee664b4e532d",
            "a7f81be7d49c474e983b2fd42ed55c6d",
            "6014b2ed9c2c4ef1a2df0511c549c6e5",
            "53abde030e304170aae513022ce3707d",
            "63f89def94a045d2ac532a4c7cff1f4d",
            "16ac5eecd7704876bfddab562adeb1c2",
            "9fd05416de37427596c4f0332153d677",
            "9952d1540c6b485a80e0c6a1fafd6da2",
            "d2ff481b7677489c8e4523970b2534dd",
            "b956b2e50203415caf5dec7dd8b8cdd4",
            "1a3c5dd0c8a44c928ac400c57d1f1cec",
            "0486b2425b3f41c588b790b6af84c48b",
            "8f4891e4994349bbadda9b6953fa2df7",
            "06eeb90bd7fb45e182d7d9b77ff26fe5",
            "b5e0b141dc5448a1b2d90c66bcec8a99",
            "fe8eb6886692417682bd057f689ed650",
            "73b2f44221524429a07173de18bcc387",
            "51a5fe9c8cc44a03be6798d2a3be7bf4",
            "a62a4dc4b6a442429cec06100bf4541d",
            "e2f2f38282734a2b81472e497e33a460",
            "17a2179d3dfc4b80b12b214d7a5c186c",
            "46dd1bd908834116b4a51aaffe2eb4df",
            "156e4d57987e4306a711e1c36c2f5245",
            "95c0cc9b45f74eb3896abd4ea85e2332",
            "078d7ee17843459aa81a3666c6e75a52",
            "a9f014ce156c413b9a41fa442554b18e",
            "a601afd5a43a42ddaa3476b2fc5c1053",
            "00aa7f845c6941aba9b77d83670aa896",
            "3a9069771c4547ed83d5fee7f1767f0a",
            "de58fd8c31654622a1c53020beacfb7a",
            "b315ad78bdc04a3e86d629c945d51b9c",
            "78fc0d280c6e47a489e610e6243ee581",
            "063f1b7f57ff4ffca19d2afe3540bdfe",
            "3e669728a23c4e09a6fb72617563ec0a",
            "dc0a5949f7d04718a2adb34f69b9fc36",
            "273e27a9e6d64f15a6773fcd09f6efd4",
            "e9cf70d8e35744e2986ad53b3a7e31cd",
            "2331c8e5341849e095e9fb74ea1bfef8",
            "a56ad411da424deaa2857822ee2e5e3b",
            "8518e6a30404449d96a026b03db485bd",
            "1763979ee11848468c6a662a183a2b2b",
            "00424fd15c54482593e1d0d9602d1bff",
            "ee2f3e5c5cbd42019f9d64e8abcb6d1e",
            "e99c491cc98b41e5b6ad886fe4103155",
            "4f4691bdfa454952b11286b83e2ab816",
            "5d2b6d42461e4b7395e74333d1143398",
            "0d2571fcc8b4448b925654b3473e468a",
            "5dae72632f3f424199018362d344f165",
            "16c89f0a018f4d7597a20ba94c1861d4",
            "3e64b14bc0754685afcb3c16986bf6a5",
            "086dca3623ca4bd89437fd187edb9f86",
            "42313643e3244ee4ab0bb572125bbc59",
            "4e340db9954c49e19ff8063c7351aca1",
            "ba1bc434a9f748b9bd87613e98759154",
            "fa4fdc42558d46d58b6bf0898a043ea2",
            "e20c058afa494c839c6f65d710f3002e",
            "dda249404b974bc28fa001eb52d776ba",
            "32fc402ed275482eafd55e23b9dd4ef4",
            "c1f45feed261456e883f839f1eb91964",
            "34b510620e374dbdaa18ab4c35591b62",
            "391cdb7fb6204040b3b8d6a0a8ab2616",
            "0ee86f4487044b2a9cac48349ac5f91a",
            "3af04412b6514e19b4fe7a5f7850a864",
            "b85f25ab013b4339abac821e8e41f6c8",
            "f5d2d3a9528f4bf69ab0b259691aff79",
            "1c1ab734dbd44ba29305eb4935f2ef18",
            "6c729e7334124054b82ef9995a921477",
            "f55bdc2483124a638d98b809bab18f72",
            "925cd7ffb47c49cabc5c9b3ffbe25e2c",
            "919de4ca377941849e4290f83f32f42c",
            "3a22f0c78363413689d981965050e2c7",
            "28aa2e87a8ec4719893f0f3daf7001ce",
            "f3a04f087169474d8a8a47412352de6a",
            "fa2da8e1a1ec4953bf381731a80d7911",
            "8e7d1654397a47fe945a6093a72a54df",
            "a71f01b101c646d9a6e098c0f9901c70",
            "dc9d27e958614367ac0cec18b2c6a892",
            "8d0cee7a999c4b3bb5851f4041ed7308",
            "dc339996e23641869e4fe17811b39bd7",
            "3c0396688eb547628751743e0cd3c56c",
            "c9b53666a0894328ae654e523c4571b5",
            "3b84c98cb74f4f24a27ad5f71436f407",
            "2188fa8174ed425e83191136944f9636",
            "0dff709d22e2439fa251c0b111336a18",
            "5de913d8d7424d168466140349f0663c",
            "6483a6453d104e2d8cd48012aaaf3453",
            "88fa915a4a794666909a73a74124febd",
            "a8e0146d7d3c476fa97d4f36abbdeac6",
            "8e59b070f01748f5937fd7c884409821",
            "0a3d1a7bcf4d4bcd95078129d8aee36b",
            "a466b36404444748b80e53469816363e",
            "05b406f5f03c4154b5f1872417147a78",
            "f1d6e46abc394eadb70719cb31a9b732",
            "b71ccf59acaa46988d588840e02cac26",
            "f7c2b75e5acb4b399a6073d8a972ce7d",
            "a979e22176214165afe1b5f6b5df7e8c",
            "5581d9069dd04d4d92183f9fded638a9",
            "63b7f9141edd4870be4b8fd165878b86",
            "a551c73057604508a229b037712fc078",
            "7088b66e4830427c89d2188d6058d97f",
            "4a612ed2eb24458eb84a1b86d0616b23",
            "a61fa34a269b4c91be92418671245299",
            "9c3806d674bf45f2a3691e054a7984ea",
            "15f47ed7d8134fc5aa17defe993cd250",
            "3c9df6f69bca41978a1b6ee606b8d836",
            "0283a845dc5144a587d2f3ecc0e5ffec",
            "e16a39bda51841c2bcc433a79facdaf2",
            "7142c335c89e49bc9f65965c151bc0c5",
            "1b02e97db4d64a09883996ff71ccff6f",
            "5178445f2e194ecab497831f86043adc",
            "b15878849bce44c8bacb689c171e499b",
            "ffa7ff1765f74120827261de29861dff",
            "0cbce1f18f0e4a65ba25ce58ebc89b2c",
            "8fa7e7261f5c48dfbd868f5f38371f65",
            "2b9504f8b62a4424b582ab9fcedcd111",
            "bb19d1c99b164d80b5dc214cbe04e6e5",
            "d911c8507d4946159217408ead154b39",
            "f97db775564041e9bef014b1e11375d2",
            "4312fa978b304d43bd98bf9e0521578d",
            "c48104256ba14372b85306a79966edb9",
            "39190c4c13d14c48a4b3a53848753e2a"
          ]
        },
        "outputId": "39df28b6-c1c7-4198-84fe-5794129a520e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3b39088b855495697719b60cd5932be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1ddddb1c9e44d5eae262d1678992b63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "097c0d80265f4acdb6023d621cdc7165"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5435dc1f959544399efaee664b4e532d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0486b2425b3f41c588b790b6af84c48b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "156e4d57987e4306a711e1c36c2f5245"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e669728a23c4e09a6fb72617563ec0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f4691bdfa454952b11286b83e2ab816"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e20c058afa494c839c6f65d710f3002e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c729e7334124054b82ef9995a921477"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d0cee7a999c4b3bb5851f4041ed7308"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e59b070f01748f5937fd7c884409821"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7088b66e4830427c89d2188d6058d97f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b15878849bce44c8bacb689c171e499b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.8939\n",
            "The cat sits outside \t\t The cat plays in the garden \t\t Score: 0.6788\n",
            "I love pasta \t\t Do you like pizza? \t\t Score: 0.5096\n",
            "I love pasta \t\t The new movie is so great \t\t Score: 0.2560\n",
            "I love pasta \t\t The new movie is awesome \t\t Score: 0.2440\n",
            "A man is playing guitar \t\t The cat plays in the garden \t\t Score: 0.2105\n",
            "The new movie is awesome \t\t Do you like pizza? \t\t Score: 0.1969\n",
            "The new movie is so great \t\t Do you like pizza? \t\t Score: 0.1692\n",
            "The cat sits outside \t\t A woman watches TV \t\t Score: 0.1310\n",
            "The cat plays in the garden \t\t Do you like pizza? \t\t Score: 0.0900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Para o modelo, precisamos decodificar as saídas e os rótulos em texto antes de \n",
        "# podermos calcular as pontuações/métricas do Rouge\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Decode generated summaries into text\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    # Decode reference summaries into text\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # ROUGE expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    result = rouge_score.compute(\n",
        "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
        "    )\n",
        "    \n",
        "# Extract the median scores\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ],
      "metadata": {
        "id": "7t5aPy-rUHAU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "b9Pl6thGOp8W"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "EEc95YiEVEjS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"test_trainer\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=4,\n",
        "    predict_with_generate=True,\n",
        "    save_strategy=\"no\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KwKipzhTQfU",
        "outputId": "21ac2426-9eb9-4317-a1cf-df24bfa8576c"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Saving model checkpoint to test_trainer\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in test_trainer/tokenizer_config.json\n",
            "Special tokens file saved in test_trainer/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=trained_tokenized_dataset,\n",
        "    eval_dataset=eval_tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "SSfJf1QhRyo4"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "trainer.save_model()\n",
        "trainer.train(True)"
      ],
      "metadata": {
        "id": "dBFIR5nKS9ty",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "930923fd-1e86-4ca3-b4c6-72b44a7322e0"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to test_trainer\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "tokenizer config file saved in test_trainer/tokenizer_config.json\n",
            "Special tokens file saved in test_trainer/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-492dd930f318>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1511\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_last_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresume_from_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1513\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No valid checkpoint found in output directory ({args.output_dir})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresume_from_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No valid checkpoint found in output directory (test_trainer)"
          ]
        }
      ]
    }
  ]
}